<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>GitHub Trending - February 15, 2026</title>
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <header>
        <h1>GitHub Trending Digest - February 15, 2026</h1>
        <nav>
            <a href="../">&larr; GitHub Calendar</a>
            <a href="../hn/">Hacker News</a>
        </nav>
    </header>
    <main>
        <div class="repo-controls">
            <button id="collapse-seen-btn" type="button">Collapse Seen Repos</button>
            <button id="expand-all-btn" type="button">Expand All</button>
        </div>
        <article>
            <div class="repos">

            <section class="repo" data-seen-before="1">
                <div class="repo-header-row">
                    <h3>1. <a href="https://github.com/tambo-ai/tambo" target="_blank" rel="noopener noreferrer">tambo-ai/tambo</a> <span class="seen-badge">Seen before</span></h3>
                    <button type="button" class="repo-toggle" aria-expanded="true">Hide details</button>
                </div>
                <div class="repo-body">
                    <p class="description">Generative UI SDK for React</p>
                    <p class="meta">
                        <span class="language">TypeScript</span> |
                        <span class="stars">9,776 stars</span>
                        | <span class="today">127 stars today</span>
                    </p>
                    <p class="history">First seen: February 13, 2026 | Consecutive daily streak: 1 day</p>
                    <div class="ai-summary">
                        <h4>Analysis</h4>
                        <p>Tambo is a React toolkit for building generative UIs where an LLM-driven agent decides which registered components to render and streams props to them in real time. Developers register components with Zod schemas that become LLM tool definitions; the included agent runs the conversation loop, selects components, and streams updates with built-in cancellation, error recovery, and reconnection. The project ships as a fullstack solution (Tambo Cloud or self-hosted via Docker) with provider configuration, hooks (useTambo, useTamboThreadInput), local browser-executable tools, and MCP integrations, and it supports multiple LLM providers or agent frameworks.</p>
<p>Tambo is useful for frontend teams and product engineers who want adaptive, conversational interfaces—examples include AI-driven analytics dashboards, interactive charts, chat interfaces that spawn components, shopping carts, and task boards—because it automates component selection and incremental prop streaming. It accelerates prototyping and production by providing persistent, stateful interactable components, schema-driven safety for LLM calls, and flexible hosting options for enterprise needs. Its traction reflects growing demand for richer, real-time AI frontends and the convenience of removing manual tool-to-component wiring while supporting multiple LLM ecosystems.</p>
                    </div>
                </div>
            </section>

            <section class="repo" data-seen-before="1">
                <div class="repo-header-row">
                    <h3>2. <a href="https://github.com/SynkraAI/aios-core" target="_blank" rel="noopener noreferrer">SynkraAI/aios-core</a> <span class="seen-badge">Seen before</span></h3>
                    <button type="button" class="repo-toggle" aria-expanded="true">Hide details</button>
                </div>
                <div class="repo-body">
                    <p class="description">Synkra AIOS: AI-Orchestrated System for Full Stack Development - Core Framework v4.0</p>
                    <p class="meta">
                        <span class="language">JavaScript</span> |
                        <span class="stars">638 stars</span>
                        | <span class="today">209 stars today</span>
                    </p>
                    <p class="history">First seen: February 14, 2026 | Consecutive daily streak: 2 days</p>
                    <div class="ai-summary">
                        <h4>Analysis</h4>
                        <p>Synkra AIOS is a CLI-first framework that orchestrates specialized AI agents to automate full‑stack development workflows, providing a core runtime and tooling (v4.0) for planning, development and observability. Its main features include dedicated agent roles (analyst, PM, architect, scrum master, dev, QA), a two‑phase process that first generates PRDs and architecture then produces hyper‑detailed story files for implementation, lifecycle hooks and IDE integrations, plus observability (SSE dashboards, logs, metrics). Technically it is a Node.js-based toolchain distributed via npx (aios-core CLI), relies on engineered prompts and human‑in‑the‑loop refinements, file-backed artifacts and hook adapters to integrate with multiple CLIs/IDEs, and provides install/upgrade flows that preserve project customizations.</p>
<p>The project benefits engineering teams, product managers and dev leads who need consistent, context-rich specification‑to‑code handoffs and stronger automation of repetitive tasks, reducing context loss between planning and implementation. It also suits solo developers and non‑technical domains (creative writing, business strategy, education) that can leverage domain‑specific agents to scale expertise quickly. Synkra is gaining attention because it combines agent orchestration, reproducible CLI‑first workflows and IDE hook parity to improve developer productivity, governance and observability in AI‑assisted software delivery.</p>
                    </div>
                </div>
            </section>

            <section class="repo" data-seen-before="1">
                <div class="repo-header-row">
                    <h3>3. <a href="https://github.com/rowboatlabs/rowboat" target="_blank" rel="noopener noreferrer">rowboatlabs/rowboat</a> <span class="seen-badge">Seen before</span></h3>
                    <button type="button" class="repo-toggle" aria-expanded="true">Hide details</button>
                </div>
                <div class="repo-body">
                    <p class="description">Open-source AI coworker, with memory</p>
                    <p class="meta">
                        <span class="language">TypeScript</span> |
                        <span class="stars">6,234 stars</span>
                        | <span class="today">217 stars today</span>
                    </p>
                    <p class="history">First seen: February 13, 2026 | Consecutive daily streak: 1 day</p>
                    <div class="ai-summary">
                        <h4>Analysis</h4>
                        <p>Rowboat is an open-source, local-first AI coworker that builds and maintains a long-lived knowledge graph from your email, meeting notes, and other work artifacts to help you summarize, draft, plan, and produce deliverables. It stores memory as an Obsidian-compatible vault of plain Markdown with backlinks so context is transparent, editable, and kept on your machine. Rowboat connects to services like Gmail, Calendar, Drive, meeting transcription tools, and can record voice notes, draft emails, generate PDFs and slides, and run background agents to automate recurring tasks. Technically it supports local and hosted LLMs (Ollama, LM Studio, or API-based providers) and exposes a Model Context Protocol (MCP) to integrate search, databases, and other tools.</p>
<p>The project benefits knowledge workers, product managers, executives, and small teams who need contextual continuity, private data storage, and automated task assistance. By compounding memory rather than reconstructing context each time, Rowboat reduces repetitive work, speeds meeting prep, improves email drafting, and helps capture decisions and action items reliably. Its local-first design, Obsidian compatibility, background agents, and support for local models make it attractive to privacy-conscious users, while MCP-driven extensibility aligns with the current trend toward customizable, interoperable AI assistants.</p>
                    </div>
                </div>
            </section>

            <section class="repo" data-seen-before="0">
                <div class="repo-header-row">
                    <h3>4. <a href="https://github.com/minio/minio" target="_blank" rel="noopener noreferrer">minio/minio</a> </h3>
                    <button type="button" class="repo-toggle" aria-expanded="true">Hide details</button>
                </div>
                <div class="repo-body">
                    <p class="description">MinIO is a high-performance, S3 compatible object store, open sourced under GNU AGPLv3 license.</p>
                    <p class="meta">
                        <span class="language">Go</span> |
                        <span class="stars">60,291 stars</span>
                        | <span class="today">29 stars today</span>
                    </p>
                    <p class="history">First seen: February 15, 2026 | Consecutive daily streak: 1 day</p>
                    <div class="ai-summary">
                        <h4>Analysis</h4>
                        <p>MinIO is a high-performance, S3‑compatible object storage server (licensed under AGPLv3) designed for AI/ML, analytics, and other data‑intensive workloads. It offers S3 API compatibility, fast object operations, an embedded web Console, the mc CLI, language SDKs, and deployment options including building from source, Docker images, and Kubernetes via Helm or an operator; it also employs erasure coding and distributed deployment patterns to scale and protect data. Implemented in Go, MinIO runs as a standalone or distributed server process that exposes S3 semantics for seamless integration with existing S3 tools and data pipelines.</p>
<p>This project is useful for teams needing a lightweight, high‑throughput, self‑hosted S3 endpoint for AI/ML pipelines, analytics, backups, edge storage, or private cloud deployments where control, performance, and cost predictability matter; platform operators and developers benefit from broad tooling and straightforward orchestration. That said, the repository is marked as no longer maintained, so while the architecture and feature set explain its popularity for modern data workflows, prospective users should evaluate maintained forks or commercial alternatives (e.g., AIStor) before adopting it for production.</p>
                    </div>
                </div>
            </section>

            <section class="repo" data-seen-before="1">
                <div class="repo-header-row">
                    <h3>5. <a href="https://github.com/ChromeDevTools/chrome-devtools-mcp" target="_blank" rel="noopener noreferrer">ChromeDevTools/chrome-devtools-mcp</a> <span class="seen-badge">Seen before</span></h3>
                    <button type="button" class="repo-toggle" aria-expanded="true">Hide details</button>
                </div>
                <div class="repo-body">
                    <p class="description">Chrome DevTools for coding agents</p>
                    <p class="meta">
                        <span class="language">TypeScript</span> |
                        <span class="stars">25,153 stars</span>
                        | <span class="today">331 stars today</span>
                    </p>
                    <p class="history">First seen: February 12, 2026 | Consecutive daily streak: 4 days</p>
                    <div class="ai-summary">
                        <h4>Analysis</h4>
                        <p>Chrome DevTools MCP is an MCP (Model‑Context‑Protocol) server that lets AI coding agents (e.g., Gemini, Claude, Cursor, Copilot) control and inspect a live Chrome browser by exposing Chrome DevTools functionality. Its main features include recording and analyzing performance traces, network request inspection, screenshots, console messages with source‑mapped stack traces, and reliable automation driven by puppeteer with automatic waits. Technically it runs as a local Node.js process (distributed via npx), attaches to or launches a Chrome instance over the remote debugging protocol, and exposes DevTools APIs and optional “skills” to MCP clients; it can also augment lab traces with CrUX field data and collects usage statistics by default (opt‑out available).</p>
<p>This project is valuable for engineers and platform builders who want to integrate deep browser automation, debugging, and performance analysis into LLM-driven workflows—use cases include automated bug reproduction, performance audits, QA test automation, and assistive developer tooling. Teams building AI assistants, SRE/performance teams, QA engineers, and tool integrators benefit because MCP enables standardized, programmatic access to the full power of DevTools from agents, improving reliability and observability of automated tasks. Its relevance is rising as more developer tooling adopts agent-based workflows and MCP-style integrations to let models perform actionable, context-aware operations in real browsers.</p>
                    </div>
                </div>
            </section>

            <section class="repo" data-seen-before="0">
                <div class="repo-header-row">
                    <h3>6. <a href="https://github.com/alibaba/zvec" target="_blank" rel="noopener noreferrer">alibaba/zvec</a> </h3>
                    <button type="button" class="repo-toggle" aria-expanded="true">Hide details</button>
                </div>
                <div class="repo-body">
                    <p class="description">A lightweight, lightning-fast, in-process vector database</p>
                    <p class="meta">
                        <span class="language">C++</span> |
                        <span class="stars">1,416 stars</span>
                        | <span class="today">172 stars today</span>
                    </p>
                    <p class="history">First seen: February 15, 2026 | Consecutive daily streak: 1 day</p>
                    <div class="ai-summary">
                        <h4>Analysis</h4>
                        <p>Zvec is an open-source, in-process vector database that embeds Alibaba’s Proxima vector search engine to provide low-latency, production-grade similarity search directly inside applications. Its main features include millisecond searches over billions of vectors, native support for dense and sparse embeddings, multi-vector queries, hybrid search combining semantic similarity with structured filters, and simple collection/vector schema APIs. Technically it runs as a library (no separate server) with Python and Node.js clients, letting you create/open collections, insert documents with vectors, and execute VectorQuery-based searches; it supports Linux and macOS on x86_64 and ARM64 and can be installed via pip/npm or built from source. The README emphasizes speed, minimal setup, and scalability driven by the underlying Proxima engine.</p>
<p>Zvec is valuable for teams and developers building semantic search, recommendation systems, retrieval-augmented generation (RAG) for LLMs, personalization, and edge or embedded applications that require low-latency similarity lookups without deploying external services. ML engineers, product teams, and startups benefit from its simplicity and in-process model because it reduces operational overhead while supporting hybrid queries that combine semantic relevance with structured filters for production use cases. Its focus on performance, multi-vector and dense+sparse support, and easy integration make it well-suited to the current surge in embedding-based workflows and generative AI, which is driving adoption of lightweight, high-performance vector stores.</p>
                    </div>
                </div>
            </section>

            <section class="repo" data-seen-before="0">
                <div class="repo-header-row">
                    <h3>7. <a href="https://github.com/ruvnet/wifi-densepose" target="_blank" rel="noopener noreferrer">ruvnet/wifi-densepose</a> </h3>
                    <button type="button" class="repo-toggle" aria-expanded="true">Hide details</button>
                </div>
                <div class="repo-body">
                    <p class="description">Production-ready implementation of InvisPose - a revolutionary WiFi-based dense human pose estimation system that enables real-time full-body tracking through walls using commodity mesh routers</p>
                    <p class="meta">
                        <span class="language">Python</span> |
                        <span class="stars">6,230 stars</span>
                        | <span class="today">98 stars today</span>
                    </p>
                    <p class="history">First seen: February 15, 2026 | Consecutive daily streak: 1 day</p>
                    <div class="ai-summary">
                        <h4>Analysis</h4>
                        <p>WiFi-DensePose is a production-ready system that estimates dense human pose and tracks multiple people in real time using WiFi Channel State Information (CSI) instead of cameras. It fuses CSI preprocessing, phase sanitization, and feature extraction with a DensePose neural head and multi-object tracking, and exposes REST and WebSocket APIs for live streaming and control. The project includes a high-performance Rust port with validated microbenchmarks and full test coverage, and supports deployment via pip, Docker, or building from source for low-latency, resource-efficient operation.</p>
<p>This technology delivers privacy-preserving, through-wall sensing suited to healthcare (fall and vital-sign monitoring), fitness, smart homes, security, and disaster response (the WiFi‑Mat module for survivor detection and triage). Enterprise features—authentication, rate limiting, monitoring—and WASM support in the Rust implementation make it attractive to commercial integrators and IoT vendors. Researchers, emergency responders, and product teams benefit from camera-less, hardware-agnostic sensing on commodity routers, and the combination of strong performance, test coverage, and practical APIs explains its growing interest.</p>
                    </div>
                </div>
            </section>

            <section class="repo" data-seen-before="0">
                <div class="repo-header-row">
                    <h3>8. <a href="https://github.com/Zipstack/unstract" target="_blank" rel="noopener noreferrer">Zipstack/unstract</a> </h3>
                    <button type="button" class="repo-toggle" aria-expanded="true">Hide details</button>
                </div>
                <div class="repo-body">
                    <p class="description">No-code LLM Platform to launch APIs and ETL Pipelines to structure unstructured documents</p>
                    <p class="meta">
                        <span class="language">Python</span> |
                        <span class="stars">6,353 stars</span>
                        | <span class="today">40 stars today</span>
                    </p>
                    <p class="history">First seen: February 15, 2026 | Consecutive daily streak: 1 day</p>
                    <div class="ai-summary">
                        <h4>Analysis</h4>
                        <p>Unstract is a no-code LLM platform that automates extraction of structured JSON from unstructured documents by letting users define schemas in a Prompt Studio, compare outputs across LLMs, and launch extraction APIs or ETL pipelines with one click. It provides multiple integration targets—MCP servers, REST API deployments, ETL connectors and n8n nodes—and supports a broad set of file types, LLM providers, embedding models, vector databases and text extractors. Technically it chains text extractors, LLM inference, embeddings and vector DBs, offers token-saving modes (SinglePass, SummarizedExtraction), ensemble verification (LLMChallenge), human-in-the-loop review, and can be self-hosted via Docker; adapter credentials are encrypted with an ENCRYPTION_KEY and optional Posthog telemetry is included.</p>
<p>This platform is valuable for engineering and data teams, ML/product teams building agentic workflows, and low-code/ops users who need to turn large volumes of documents into clean, load-ready JSON for analytics or automation, reducing manual effort and error rates. Enterprise features like SSO, self-hosting, numerous storage and warehouse connectors (Snowflake, BigQuery, Redshift, Postgres, etc.), and human review workflows make it suitable for regulated or large-scale deployments. Its momentum reflects rising demand to operationalize LLMs for document-centric workflows, the cost/accuracy gains from ensemble and token-efficient strategies, and the lowered barrier from no-code prompt engineering and one-click API launches.</p>
                    </div>
                </div>
            </section>

            <section class="repo" data-seen-before="0">
                <div class="repo-header-row">
                    <h3>9. <a href="https://github.com/letta-ai/letta-code" target="_blank" rel="noopener noreferrer">letta-ai/letta-code</a> </h3>
                    <button type="button" class="repo-toggle" aria-expanded="true">Hide details</button>
                </div>
                <div class="repo-body">
                    <p class="description">The memory-first coding agent</p>
                    <p class="meta">
                        <span class="language">TypeScript</span> |
                        <span class="stars">1,264 stars</span>
                        | <span class="today">55 stars today</span>
                    </p>
                    <p class="history">First seen: February 15, 2026 | Consecutive daily streak: 1 day</p>
                    <div class="ai-summary">
                        <h4>Analysis</h4>
                        <p>Letta Code is a memory-first coding harness and CLI built on the Letta API that gives you a persisted coding agent rather than ephemeral sessions. It provides persistent memory, cross-model portability (Claude Sonnet/Opus 4.5, GPT-5.2-Codex, Gemini 3 Pro, GLM-4.7, etc.), and a skill system (.skills) with commands like /init, /remember, /skill, /connect and /model to configure behavior and APIs. Technically the agent stores and updates long-lived memory across sessions, supports user-supplied LLM keys or the Letta API, and can point at an external server via LETTABASEURL; /clear creates a new conversational thread while preserving the agent’s learned state. The package is distributed as an npm CLI and includes tooling for skill learning and persistent agent workflows.</p>
<p>This approach is valuable for developers and teams who want a personalized, evolving coding assistant that retains project context, conventions, and learned skills over time, reducing repetitive setup and onboarding friction. It’s useful for long-lived projects, pair programming augmentation, code review assistance, and experimentation across multiple LLMs because agents remain portable and improvable. The project is timely given the rise of agent-based workflows and multi-model experimentation, which prioritize persistent memory and long-term assistant utility over single-session interactions.</p>
                    </div>
                </div>
            </section>

            <section class="repo" data-seen-before="0">
                <div class="repo-header-row">
                    <h3>10. <a href="https://github.com/ruby/ruby" target="_blank" rel="noopener noreferrer">ruby/ruby</a> </h3>
                    <button type="button" class="repo-toggle" aria-expanded="true">Hide details</button>
                </div>
                <div class="repo-body">
                    <p class="description">The Ruby Programming Language</p>
                    <p class="meta">
                        <span class="language">Ruby</span> |
                        <span class="stars">23,387 stars</span>
                        | <span class="today">15 stars today</span>
                    </p>
                    <p class="history">First seen: February 15, 2026 | Consecutive daily streak: 1 day</p>
                    <div class="ai-summary">
                        <h4>Analysis</h4>
                        <p>This project is the reference implementation and source repository for the Ruby programming language, an interpreted, object-oriented language commonly used for web development and scripting. It implements core language features such as classes, mix-ins, singleton methods, iterators and closures, exception handling, operator overloading, and garbage collection, and includes the runtime, standard library, build scripts, tests, and documentation. Technically the repository contains the interpreter/virtual machine and native runtime (the MRI/YARV implementation), platform-specific build tooling, and support for dynamic loading and portability across Unix-like systems, Windows, and macOS. The codebase and docs enable developers to build, extend, and maintain the language implementation itself.</p>
<p>The project is valuable to developers and organizations that need a productive, expressive language for web applications, scripting, automation, prototyping, and text processing. Web developers (especially those using Rails), DevOps engineers, library authors, and educators benefit from the mature ecosystem, comprehensive documentation, and the ability to inspect or modify the runtime for performance or platform needs. Its long-standing community, extensive gem ecosystem, and readable syntax keep Ruby widely used and relevant in modern development workflows. Maintaining an active, portable reference implementation makes it practical for teams to adopt, contribute to, or customize Ruby for specific use cases.</p>
                    </div>
                </div>
            </section>

            </div>
        </article>
    </main>
    <footer>
        <p>Generated automatically. Data from <a href="https://github.com/trending">GitHub Trending</a>.</p>
    </footer>

<script>
(() => {
    const readKey = "gtd:read_days:gh:v1";
    const dayStr = "2026-02-15";

    let stored = [];
    try {
        stored = JSON.parse(localStorage.getItem(readKey) || "[]");
        if (!Array.isArray(stored)) {
            stored = [];
        }
    } catch (_err) {
        stored = [];
    }

    if (!stored.includes(dayStr)) {
        stored.push(dayStr);
        stored.sort();
        localStorage.setItem(readKey, JSON.stringify(stored));
    }

    const collapseParam = new URLSearchParams(window.location.search).get("collapse_seen");
    const collapseSeen = collapseParam === "0" ? false : true;

    function setCollapsed(repoEl, collapsed) {
        repoEl.classList.toggle("collapsed", collapsed);
        const button = repoEl.querySelector(".repo-toggle");
        if (button) {
            button.textContent = collapsed ? "Show details" : "Hide details";
            button.setAttribute("aria-expanded", String(!collapsed));
        }
    }

    const repos = Array.from(document.querySelectorAll("section.repo[data-seen-before]"));

    repos.forEach((repoEl) => {
        const toggle = repoEl.querySelector(".repo-toggle");
        if (!toggle) {
            return;
        }

        toggle.addEventListener("click", () => {
            setCollapsed(repoEl, !repoEl.classList.contains("collapsed"));
        });
    });

    const collapseBtn = document.getElementById("collapse-seen-btn");
    const expandBtn = document.getElementById("expand-all-btn");

    if (collapseBtn) {
        collapseBtn.addEventListener("click", () => {
            repos.forEach((repoEl) => {
                if (repoEl.dataset.seenBefore === "1") {
                    setCollapsed(repoEl, true);
                }
            });
        });
    }

    if (expandBtn) {
        expandBtn.addEventListener("click", () => {
            repos.forEach((repoEl) => setCollapsed(repoEl, false));
        });
    }

    if (collapseSeen) {
        repos.forEach((repoEl) => {
            if (repoEl.dataset.seenBefore === "1") {
                setCollapsed(repoEl, true);
            }
        });
    }
})();
</script>

</body>
</html>
