<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>GitHub Trending - February 15, 2026</title>
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <header>
        <h1>GitHub Trending Digest - February 15, 2026</h1>
        <nav>
            <a href="../">&larr; Back to Calendar</a>
        </nav>
    </header>
    <main>
        <article>
            <div class="repos">

            <section class="repo">
                <h3>1. <a href="https://github.com/tambo-ai/tambo" target="_blank">tambo-ai/tambo</a></h3>
                <p class="description">Generative UI SDK for React</p>
                <p class="meta">
                    <span class="language">TypeScript</span> |
                    <span class="stars">9,776 stars</span>
                    | <span class="today">127 stars today</span>
                </p>
                <div class="ai-summary">
                    <h4>Analysis</h4>
                    <p>Tambo is a React toolkit for building generative UIs where an LLM-driven agent decides which registered components to render and streams props to them in real time. Developers register components with Zod schemas that become LLM tool definitions; the included agent runs the conversation loop, selects components, and streams updates with built-in cancellation, error recovery, and reconnection. The project ships as a fullstack solution (Tambo Cloud or self-hosted via Docker) with provider configuration, hooks (useTambo, useTamboThreadInput), local browser-executable tools, and MCP integrations, and it supports multiple LLM providers or agent frameworks.</p>
<p>Tambo is useful for frontend teams and product engineers who want adaptive, conversational interfaces—examples include AI-driven analytics dashboards, interactive charts, chat interfaces that spawn components, shopping carts, and task boards—because it automates component selection and incremental prop streaming. It accelerates prototyping and production by providing persistent, stateful interactable components, schema-driven safety for LLM calls, and flexible hosting options for enterprise needs. Its traction reflects growing demand for richer, real-time AI frontends and the convenience of removing manual tool-to-component wiring while supporting multiple LLM ecosystems.</p>

                </div>
            </section>

            <section class="repo">
                <h3>2. <a href="https://github.com/SynkraAI/aios-core" target="_blank">SynkraAI/aios-core</a></h3>
                <p class="description">Synkra AIOS: AI-Orchestrated System for Full Stack Development - Core Framework v4.0</p>
                <p class="meta">
                    <span class="language">JavaScript</span> |
                    <span class="stars">638 stars</span>
                    | <span class="today">209 stars today</span>
                </p>
                <div class="ai-summary">
                    <h4>Analysis</h4>
                    <p>Synkra AIOS is an AI-orchestrated, self-modifying core framework that automates full‑stack development workflows by coordinating specialized agent roles through a CLI‑first architecture. Its core features include a two‑phase agentic process—planning agents (analyst, PM, architect) that produce detailed PRDs and architectures, followed by development agents (SM, dev, QA) that emit implementation‑ready story files—plus observability (SSE dashboard, logs, metrics) and optional UI components like Kanban. Technically it is a Node.js (v18+) CLI tool distributed via npx with an interactive installer, IDE integration presets (Windsurf, Cursor, Claude Code), prompt‑engineering with human‑in‑the‑loop refinement, meta‑agents, and file‑based artifacts to preserve context across the lifecycle.</p>
<p>The project benefits engineering teams, product managers, and organizations seeking to scale software delivery with AI by eliminating planning inconsistency and preserving end‑to‑end context from PRD to QA, accelerating handoffs and reducing rework. It’s also applicable to non‑technical domains—creative writing, business strategy, education—where repeatable, agentic workflows encode domain expertise into deliverables. Because it is CLI‑first, updatable via npx, and emphasizes observability and reproducible automation, it fits modern developer toolchains and is gaining traction as teams adopt agent‑based AI to speed iteration and maintain traceability.</p>

                </div>
            </section>

            <section class="repo">
                <h3>3. <a href="https://github.com/rowboatlabs/rowboat" target="_blank">rowboatlabs/rowboat</a></h3>
                <p class="description">Open-source AI coworker, with memory</p>
                <p class="meta">
                    <span class="language">TypeScript</span> |
                    <span class="stars">6,234 stars</span>
                    | <span class="today">217 stars today</span>
                </p>
                <div class="ai-summary">
                    <h4>Analysis</h4>
                    <p>Rowboat is an open-source, local-first AI coworker that builds and maintains a long-lived knowledge graph of your work by storing plain Markdown notes (Obsidian-compatible) on your machine. It ingests context from sources like Gmail, Google Drive/Calendar, meeting-notes services (Granola, Fireflies), and optional voice transcription (Deepgram) to extract decisions, action items, and relevant threads, then uses that context to draft emails, prepare meeting briefs, generate docs and PDF slides, and record voice memos. Technically, Rowboat keeps an inspectable Markdown vault with backlinks as its working memory, supports background agents for recurring automation, and is model-agnostic—running local models (Ollama, LM Studio) or hosted APIs and integrating external tools via the Model Context Protocol (MCP). All writes are explicit and reviewable, preserving privacy and editability by design.</p>
<p>The project is valuable for knowledge workers, product managers, executives, and privacy-conscious teams who need persistent, actionable context to reduce repetitive explanations and improve meeting prep, follow-ups, and document generation. Its local-first, editable memory model and background automation suit both personal and team workflows where data ownership and auditability matter, while MCP and BYO-model support enable flexible integrations and deployment choices. Rowboat aligns with current demand for AI assistants that offer persistent memory, real workflow integration, and data control, which helps explain its traction among users seeking practical, private productivity tooling.</p>

                </div>
            </section>

            <section class="repo">
                <h3>4. <a href="https://github.com/minio/minio" target="_blank">minio/minio</a></h3>
                <p class="description">MinIO is a high-performance, S3 compatible object store, open sourced under GNU AGPLv3 license.</p>
                <p class="meta">
                    <span class="language">Go</span> |
                    <span class="stars">60,291 stars</span>
                    | <span class="today">29 stars today</span>
                </p>
                <div class="ai-summary">
                    <h4>Analysis</h4>
                    <p>MinIO is a high-performance, S3‑compatible object storage server (licensed under AGPLv3) designed for AI/ML, analytics, and other data‑intensive workloads. It offers S3 API compatibility, fast object operations, an embedded web Console, the mc CLI, language SDKs, and deployment options including building from source, Docker images, and Kubernetes via Helm or an operator; it also employs erasure coding and distributed deployment patterns to scale and protect data. Implemented in Go, MinIO runs as a standalone or distributed server process that exposes S3 semantics for seamless integration with existing S3 tools and data pipelines.</p>
<p>This project is useful for teams needing a lightweight, high‑throughput, self‑hosted S3 endpoint for AI/ML pipelines, analytics, backups, edge storage, or private cloud deployments where control, performance, and cost predictability matter; platform operators and developers benefit from broad tooling and straightforward orchestration. That said, the repository is marked as no longer maintained, so while the architecture and feature set explain its popularity for modern data workflows, prospective users should evaluate maintained forks or commercial alternatives (e.g., AIStor) before adopting it for production.</p>

                </div>
            </section>

            <section class="repo">
                <h3>5. <a href="https://github.com/ChromeDevTools/chrome-devtools-mcp" target="_blank">ChromeDevTools/chrome-devtools-mcp</a></h3>
                <p class="description">Chrome DevTools for coding agents</p>
                <p class="meta">
                    <span class="language">TypeScript</span> |
                    <span class="stars">25,153 stars</span>
                    | <span class="today">331 stars today</span>
                </p>
                <div class="ai-summary">
                    <h4>Analysis</h4>
                    <p>Chrome DevTools MCP is a Node.js-based MCP (Model-Context-Protocol) server that lets AI coding agents control and inspect a live Chrome browser through the Chrome DevTools protocol. It exposes DevTools capabilities—performance tracing, network analysis, screenshots, and console logs with source-mapped stack traces—and uses the DevTools frontend to extract actionable insights. Technically it automates Chrome with puppeteer, connects to a browser via a remote debugging URL (or launches one), and communicates with MCP clients (Gemini, Claude, Copilot, Cursor, etc.) using the MCP transport so agents can send commands and receive structured results. It also optionally queries CrUX for field data and collects usage statistics by default, with flags to opt out for privacy or CI environments.</p>
<p>This project benefits teams and tools that need reliable, programmatic browser introspection—AI-assisted development, automated debugging, performance engineering, QA, and SRE workflows—by enabling agents to reproduce issues, gather lab and field performance metrics, and perform complex automation with built-in waiting and DevTools-level inspection. Developers building coding assistants, observability platforms, or automation pipelines gain a standardized way to expose live browser context to agents and to integrate with a wide range of MCP-enabled IDEs and CLIs. Its momentum reflects the broader rise of agent-driven developer tooling and the convenience of plugging DevTools into AI workflows, though adopters should weigh the privacy/security implications of granting agent access to live browser contents.</p>

                </div>
            </section>

            <section class="repo">
                <h3>6. <a href="https://github.com/alibaba/zvec" target="_blank">alibaba/zvec</a></h3>
                <p class="description">A lightweight, lightning-fast, in-process vector database</p>
                <p class="meta">
                    <span class="language">C++</span> |
                    <span class="stars">1,416 stars</span>
                    | <span class="today">172 stars today</span>
                </p>
                <div class="ai-summary">
                    <h4>Analysis</h4>
                    <p>Zvec is an open-source, in-process vector database that provides low-latency, production-grade similarity search by embedding directly into applications. It offers blazing-fast searches over billions of vectors, native support for dense and sparse embeddings, multi-vector queries, and hybrid search that combines semantic similarity with structured filters. Technically, Zvec is built on Alibaba’s Proxima search engine and exposes lightweight client libraries (Python and Node.js) so it runs wherever your code runs without separate servers or complex configuration. It supports common platforms (Linux x86_64/ARM64 and macOS ARM64) and can be installed via pip/npm or built from source for custom deployments.</p>
<p>Zvec is valuable for developers, ML engineers, and teams building semantic search, recommendation systems, retrieval-augmented generation, or other applications that need fast, local vector search with minimal operational overhead. Its in-process design and hybrid search capabilities make it well-suited for latency-sensitive services, edge deployments, and environments where simple integration and small operational footprint matter. Organizations that need to scale to large vector collections while avoiding heavy infrastructure will benefit, which helps explain its traction among practitioners seeking practical, high-performance embedding search.</p>

                </div>
            </section>

            <section class="repo">
                <h3>7. <a href="https://github.com/ruvnet/wifi-densepose" target="_blank">ruvnet/wifi-densepose</a></h3>
                <p class="description">Production-ready implementation of InvisPose - a revolutionary WiFi-based dense human pose estimation system that enables real-time full-body tracking through walls using commodity mesh routers</p>
                <p class="meta">
                    <span class="language">Python</span> |
                    <span class="stars">6,230 stars</span>
                    | <span class="today">98 stars today</span>
                </p>
                <div class="ai-summary">
                    <h4>Analysis</h4>
                    <p>WiFi-DensePose is a production-ready system that estimates full-body human pose from commodity WiFi Channel State Information (CSI) instead of cameras, enabling real-time, privacy-preserving multi-person tracking. The pipeline ingests CSI via a hardware interface, performs signal processing (phase sanitization, feature extraction, motion/Doppler analysis), and passes processed features to a DensePose neural network and multi-object tracker to produce per-frame keypoints delivered over REST and WebSocket APIs. A high-performance Rust port provides validated numerical correctness and large speed/memory improvements for low-latency, high-throughput edge deployments, and the repository includes deployment tooling, monitoring, and comprehensive test coverage. The project also bundles analytics (fall detection, activity recognition) and a specialized WiFi‑Mat disaster-response module for vital-sign detection and 3D localization through debris.</p>
<p>This technology is useful for healthcare providers (patient and fall monitoring), smart-home and fitness applications, security and occupancy analytics, and emergency responders who need through-wall or through-debris sensing without cameras. Developers, enterprises, and researchers benefit from the production-grade features (authentication, rate limiting, observability) and hardware-agnostic design that supports edge runtimes (including WASM via Rust). The WiFi‑Mat extension expands applicability to search-and-rescue and disaster triage where visual sensing is infeasible. The combination of camera-free privacy, low latency, multi-person capability, and a hardened codebase explains its strong relevance for privacy-conscious and mission-critical deployments.</p>

                </div>
            </section>

            <section class="repo">
                <h3>8. <a href="https://github.com/Zipstack/unstract" target="_blank">Zipstack/unstract</a></h3>
                <p class="description">No-code LLM Platform to launch APIs and ETL Pipelines to structure unstructured documents</p>
                <p class="meta">
                    <span class="language">Python</span> |
                    <span class="stars">6,353 stars</span>
                    | <span class="today">40 stars today</span>
                </p>
                <div class="ai-summary">
                    <h4>Analysis</h4>
                    <p>Unstract is a no-code LLM platform that automates extraction of structured JSON from unstructured documents by letting users define schemas in a Prompt Studio, compare outputs across LLMs, and launch extraction APIs or ETL pipelines with one click. It provides multiple integration targets—MCP servers, REST API deployments, ETL connectors and n8n nodes—and supports a broad set of file types, LLM providers, embedding models, vector databases and text extractors. Technically it chains text extractors, LLM inference, embeddings and vector DBs, offers token-saving modes (SinglePass, SummarizedExtraction), ensemble verification (LLMChallenge), human-in-the-loop review, and can be self-hosted via Docker; adapter credentials are encrypted with an ENCRYPTION_KEY and optional Posthog telemetry is included.</p>
<p>This platform is valuable for engineering and data teams, ML/product teams building agentic workflows, and low-code/ops users who need to turn large volumes of documents into clean, load-ready JSON for analytics or automation, reducing manual effort and error rates. Enterprise features like SSO, self-hosting, numerous storage and warehouse connectors (Snowflake, BigQuery, Redshift, Postgres, etc.), and human review workflows make it suitable for regulated or large-scale deployments. Its momentum reflects rising demand to operationalize LLMs for document-centric workflows, the cost/accuracy gains from ensemble and token-efficient strategies, and the lowered barrier from no-code prompt engineering and one-click API launches.</p>

                </div>
            </section>

            <section class="repo">
                <h3>9. <a href="https://github.com/letta-ai/letta-code" target="_blank">letta-ai/letta-code</a></h3>
                <p class="description">The memory-first coding agent</p>
                <p class="meta">
                    <span class="language">TypeScript</span> |
                    <span class="stars">1,264 stars</span>
                    | <span class="today">55 stars today</span>
                </p>
                <div class="ai-summary">
                    <h4>Analysis</h4>
                    <p>Letta Code is a memory-first coding harness built on top of the Letta API that provides a CLI-driven, persisted coding agent which learns over time and can run against multiple LLM backends (Claude Sonnet/Opus 4.5, GPT-5.2-Codex, Gemini 3 Pro, GLM-4.7, etc.). Distributed as an npm package, it exposes commands such as /connect to configure API keys, /model to swap models, /init to initialize memory, /remember to add memories, and /skill to teach or extract reusable capabilities; /clear starts a new thread while memory persists. Technically it manages agent state and skills (e.g., a .skills directory and AGENTS.md), supports skill learning, and can point to an external Letta server via LETTABASEURL for self-hosting or Docker-based deployments.</p>
<p>This project benefits developers, teams, and maintainers who want a persistent, evolving coding assistant that retains project history, preferences, and learned behaviors across sessions to improve consistency, onboarding, and long-term productivity. It’s especially useful for multi-model experimentation, reproducible coding plans, and automating recurring tasks because it supports plugging in different LLM providers and local/self-hosted backends. Its traction comes from addressing the limits of ephemeral session-based CLIs by offering long-lived agents that accumulate memory and skills, making interactions more stateful and collaborative.</p>

                </div>
            </section>

            <section class="repo">
                <h3>10. <a href="https://github.com/ruby/ruby" target="_blank">ruby/ruby</a></h3>
                <p class="description">The Ruby Programming Language</p>
                <p class="meta">
                    <span class="language">Ruby</span> |
                    <span class="stars">23,387 stars</span>
                    | <span class="today">15 stars today</span>
                </p>
                <div class="ai-summary">
                    <h4>Analysis</h4>
                    <p>This project is the reference implementation and source repository for the Ruby programming language, an interpreted, object-oriented language commonly used for web development and scripting. It implements core language features such as classes, mix-ins, singleton methods, iterators and closures, exception handling, operator overloading, and garbage collection, and includes the runtime, standard library, build scripts, tests, and documentation. Technically the repository contains the interpreter/virtual machine and native runtime (the MRI/YARV implementation), platform-specific build tooling, and support for dynamic loading and portability across Unix-like systems, Windows, and macOS. The codebase and docs enable developers to build, extend, and maintain the language implementation itself.</p>
<p>The project is valuable to developers and organizations that need a productive, expressive language for web applications, scripting, automation, prototyping, and text processing. Web developers (especially those using Rails), DevOps engineers, library authors, and educators benefit from the mature ecosystem, comprehensive documentation, and the ability to inspect or modify the runtime for performance or platform needs. Its long-standing community, extensive gem ecosystem, and readable syntax keep Ruby widely used and relevant in modern development workflows. Maintaining an active, portable reference implementation makes it practical for teams to adopt, contribute to, or customize Ruby for specific use cases.</p>

                </div>
            </section>

            </div>
        </article>
    </main>
    <footer>
        <p>Generated automatically. Data from <a href="https://github.com/trending">GitHub Trending</a>.</p>
    </footer>
</body>
</html>
