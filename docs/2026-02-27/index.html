<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>GitHub Trending - February 27, 2026</title>
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <header>
        <h1>GitHub Trending Digest - February 27, 2026</h1>
        <nav>
            <a href="../">&larr; GitHub Calendar</a>
            <a href="../hn/2026-02-27/">Hacker News</a>
        </nav>
    </header>
    <main>
        <div class="repo-controls">
            <button id="collapse-seen-btn" type="button">Collapse Repos Not New Today</button>
            <button id="expand-all-btn" type="button">Expand All</button>
        </div>
        <p class="seen-help">Repos marked "Not new today" appeared on one or more previous daily pages.</p>
        <article>
            <div class="repos">

            <section class="repo" data-seen-before="0">
                <div class="repo-header-row">
                    <h3>1. <a href="https://github.com/clockworklabs/SpacetimeDB" target="_blank" rel="noopener noreferrer">clockworklabs/SpacetimeDB</a> </h3>
                    <button type="button" class="repo-toggle" aria-expanded="true">Hide details</button>
                </div>
                <div class="repo-body">
                    <p class="description">Development at the speed of light</p>
                    <p class="meta">
                        <span class="language">Rust</span> |
                        <span class="stars">21,033</span>
                        | <span class="today">441 stars today</span>
                    </p>
                    <p class="history">First seen: February 27, 2026 | Consecutive daily streak: 1 day</p>
                    <div class="ai-summary">
                        <h4>Analysis</h4>
                        <p>SpacetimeDB is a combined relational database and application server that lets you upload application logic as &quot;modules&quot; (Rust-based stored procedures) and have clients execute that logic directly inside the database. Key features include single-language (Rust) modules, direct client-to-database execution (no separate web/game server), in-memory application state for ultra-low latency, and durability via a write-ahead log (WAL) for recovery. The system is optimized for real-time workloads‚Äîgames, chat, and collaboration‚Äîby keeping state resident in RAM and persisting only the WAL to enable fast synchronous processing and simple single-binary deployment.</p>
<p>This project benefits teams building latency-sensitive real-time applications (MMOs, live collaboration, chat) and developers who want to simplify operations by removing separate servers, containers, and orchestration layers. It appeals to Rust-centric shops and small teams that prefer a single deployable artifact and deterministic, low-latency behavior, and it has traction because it combines high performance, operational simplicity, and modern systems-language tooling. The README‚Äôs example of running an entire MMORPG backend on a SpacetimeDB module highlights why performance and reduced DevOps overhead are driving interest.</p>
                    </div>
                </div>
            </section>

            <section class="repo" data-seen-before="1">
                <div class="repo-header-row">
                    <h3>2. <a href="https://github.com/obra/superpowers" target="_blank" rel="noopener noreferrer">obra/superpowers</a> <span class="seen-badge">Not new today</span></h3>
                    <button type="button" class="repo-toggle" aria-expanded="true">Hide details</button>
                </div>
                <div class="repo-body">
                    <p class="description">An agentic skills framework &amp; software development methodology that works.</p>
                    <p class="meta">
                        <span class="language">Shell</span> |
                        <span class="stars">63,701</span>
                        | <span class="today">1,532 stars today</span>
                    </p>
                    <p class="history">First seen: February 04, 2026 | Consecutive daily streak: 3 days</p>
                    <div class="ai-summary">
                        <h4>Analysis</h4>
                        <p>Superpowers is a complete software development workflow for coding agents built around composable &quot;skills&quot; and starter instructions that make agents follow a consistent process. Key features include a modular skills library (TDD, systematic debugging, brainstorming, plan-writing, subagent-driven development, code review, and git worktree management), an enforced workflow that moves from spec elicitation to plan to task-level subagents, and a two-stage review system (spec compliance then code quality). Technically it ships as a plugin (Claude Code marketplace) with manual install paths for Codex and OpenCode, stores skills directly in the repository, and has agents fetch and execute skills, spawn subagents per task, and run automated verification and test-driven cycles. The repo also contains contribution guidance and update mechanics so skills can be extended and updated easily.</p>
<p>This project benefits developers, teams, and tool builders who want repeatable, evidence-driven AI-assisted engineering‚Äîespecially those who need automated testing, clear task plans, and controlled parallel work via subagents. It‚Äôs gaining traction because it addresses practical pain points in agentic coding (avoiding ad-hoc behavior, enforcing RED‚ÄëGREEN‚ÄëREFACTOR, and integrating with git workflows) while offering an easy integration path for Claude Code and extensibility via an open-source MIT-licensed skill library.</p>
                    </div>
                </div>
            </section>

            <section class="repo" data-seen-before="1">
                <div class="repo-header-row">
                    <h3>3. <a href="https://github.com/muratcankoylan/Agent-Skills-for-Context-Engineering" target="_blank" rel="noopener noreferrer">muratcankoylan/Agent-Skills-for-Context-Engineering</a> <span class="seen-badge">Not new today</span></h3>
                    <button type="button" class="repo-toggle" aria-expanded="true">Hide details</button>
                </div>
                <div class="repo-body">
                    <p class="description">A comprehensive collection of Agent Skills for context engineering, multi-agent architectures, and production agent systems. Use when building, optimizing, or debugging agent systems that require effective context management.</p>
                    <p class="meta">
                        <span class="language">Python</span> |
                        <span class="stars">11,909</span>
                        | <span class="today">922 stars today</span>
                    </p>
                    <p class="history">First seen: February 24, 2026 | Consecutive daily streak: 4 days</p>
                    <div class="ai-summary">
                        <h4>Analysis</h4>
                        <p>This repository is a curated collection of &quot;Agent Skills&quot; for context engineering that codifies practical patterns, tools, and examples to manage the information fed into LLM-based agents. Key features include foundational material on context mechanics and degradation, architectural modules for multi-agent patterns, memory systems, tool and filesystem integration, operational modules for compression/optimization and evaluation, and cognitive BDI mental-state transformations. Technically, skills are packaged as platform-agnostic modules (Claude Code marketplace plugins, Cursor/Codex/IDE rules or standalone skill files), use progressive disclosure so full skill content loads only when activated, and ship Python-pseudocode, triggers, and installation commands that let agent runtimes discover and apply relevant skills at runtime. The repo also documents attention-aware tactics (compaction, masking, caching), LLM-as-judge evaluation methods, and hosted-agent designs for sandboxed background execution.</p>
<p>This project benefits AI engineers, multi-agent architects, ops teams, and researchers who need repeatable, production-grade patterns for conserving model attention and debugging context failures. It accelerates development by turning abstract attention mechanics into actionable skills, templates, and plugin workflows that integrate with existing agent platforms. The collection is timely and trending because managing limited context windows is a growing practical bottleneck as agentic systems scale, and because the work has received academic recognition for helping bridge manual skill engineering and dynamic evolution of agent skills.</p>
                    </div>
                </div>
            </section>

            <section class="repo" data-seen-before="1">
                <div class="repo-header-row">
                    <h3>4. <a href="https://github.com/bytedance/deer-flow" target="_blank" rel="noopener noreferrer">bytedance/deer-flow</a> <span class="seen-badge">Not new today</span></h3>
                    <button type="button" class="repo-toggle" aria-expanded="true">Hide details</button>
                </div>
                <div class="repo-body">
                    <p class="description">An open-source SuperAgent harness that researches, codes, and creates. With the help of sandboxes, memories, tools, skills and subagents, it handles different levels of tasks that could take minutes to hours.</p>
                    <p class="meta">
                        <span class="language">TypeScript</span> |
                        <span class="stars">21,285</span>
                        | <span class="today">617 stars today</span>
                    </p>
                    <p class="history">First seen: February 26, 2026 | Consecutive daily streak: 2 days</p>
                    <div class="ai-summary">
                        <h4>Analysis</h4>
                        <p>DeerFlow is an open-source &quot;super agent&quot; harness that orchestrates sub-agents, persistent memory, sandboxed execution, and extensible skills to carry out complex, multi-step tasks. Technically it is a batteries-included runtime built on LangGraph and LangChain that runs tasks inside isolated sandboxes (local, Docker, or Kubernetes), provides a filesystem and toolset (web fetch, bash, file ops), and dynamically loads Markdown-defined skills and tools while managing context via summarization and long-term memory. The lead agent can spawn parallel scoped sub-agents with their own contexts and termination rules, collect structured results, and synthesize outputs such as reports, slide decks, or web pages. Configuration is driven by config.yaml and environment keys, with Docker and local development workflows supported for fast provisioning and reproducibility.</p>
<p>DeerFlow is useful for AI researchers, data scientists, engineering teams, and content creators who need agents that do real work‚Äînot just chat‚Äîbecause it combines execution, tooling, and memory in an auditable, extensible framework. It‚Äôs trending because it addresses practical limitations of single-pass LLM interactions (execution isolation, token management, long-running tasks, and parallel decomposition), ships with ready-made skills while remaining highly customizable, and benefits from an active rewrite and community-focused tooling that make it easy to experiment with modern LLMs and production workflows.</p>
                    </div>
                </div>
            </section>

            <section class="repo" data-seen-before="1">
                <div class="repo-header-row">
                    <h3>5. <a href="https://github.com/huggingface/skills" target="_blank" rel="noopener noreferrer">huggingface/skills</a> <span class="seen-badge">Not new today</span></h3>
                    <button type="button" class="repo-toggle" aria-expanded="true">Hide details</button>
                </div>
                <div class="repo-body">
                    <p class="description">No description</p>
                    <p class="meta">
                        <span class="language">Python</span> |
                        <span class="stars">7,071</span>
                        | <span class="today">715 stars today</span>
                    </p>
                    <p class="history">First seen: February 22, 2026 | Consecutive daily streak: 6 days</p>
                    <div class="ai-summary">
                        <h4>Analysis</h4>
                        <p>This repository defines &quot;skills&quot;: self-contained folders that package instructions (SKILL.md with YAML frontmatter), scripts, templates, and resources so coding agents can perform common AI/ML tasks such as dataset creation, model training, evaluation, job management, and more. It includes ready-made skills (hf CLI, datasets, evaluation, jobs, model trainer, paper publisher, tool builder, Trackio) and provides installation/manifest support for multiple agent ecosystems (Claude/Claude Code, OpenAI Codex, Google Gemini CLI, Cursor) via AGENTS.md, gemini-extension.json, and Cursor plugin files. Technically, skills are interoperable artifacts that agents load to obtain step-by-step guidance and helper tooling; the repo also provides scripts to publish/regenerate manifests and CI checks to validate metadata and marketplace listings. The result is a standardized, reusable package format that lets agents orchestrate local or Hugging Face cloud workflows reliably.</p>
<p>This project is useful for ML engineers, MLOps teams, researchers, and developers who want to automate common model and dataset workflows through coding agents and the Hugging Face platform. It reduces manual orchestration, improves reproducibility, and makes it easy to share and extend operational patterns as installable agent plugins. Its momentum comes from the growing adoption of LLM-driven coding agents, the need for cross-platform interoperability, and tight integration with Hugging Face&#x27;s tooling and cloud compute, which together lower friction for running end-to-end ML tasks.</p>
                    </div>
                </div>
            </section>

            <section class="repo" data-seen-before="0">
                <div class="repo-header-row">
                    <h3>6. <a href="https://github.com/farion1231/cc-switch" target="_blank" rel="noopener noreferrer">farion1231/cc-switch</a> </h3>
                    <button type="button" class="repo-toggle" aria-expanded="true">Hide details</button>
                </div>
                <div class="repo-body">
                    <p class="description">A cross-platform desktop All-in-One assistant tool for Claude Code, Codex, OpenCode &amp; Gemini CLI.</p>
                    <p class="meta">
                        <span class="language">Rust</span> |
                        <span class="stars">20,883</span>
                        | <span class="today">418 stars today</span>
                    </p>
                    <p class="history">First seen: February 27, 2026 | Consecutive daily streak: 1 day</p>
                    <div class="ai-summary">
                        <h4>Analysis</h4>
                        <p>cc-switch is a cross-platform desktop assistant that centralizes management and quick switching of Claude Code, Codex, and Gemini CLI configurations and services. It provides provider management (including AWS Bedrock presets), MCP server lifecycle controls, skills and prompts management with one-click install/update, deep-link protocol support, speed testing, import/export and environment conflict detection. The app runs as a single-instance daemon with a system tray UI, atomic updates/rollbacks and a dual-layer persistence architecture (SQLite for syncable data + JSON for device-level state), plus platform-native autostart and built-in auto-updater for seamless cross-platform operation.</p>
<p>This project is aimed at developers, AI engineers, and teams who run multiple LLM CLIs or manage custom plugins and MCP servers, offering streamlined workflows for switching providers, syncing skills, and managing prompts across environments. It‚Äôs trending because it merges multi-vendor CLI support (including recent Gemini CLI integration), robust tooling for lifecycle and conflict management, and active feature development (deep links, skills marketplace scanning, and cloud-sync-ready storage) that addresses real pain points in multi-LLM development and deployment. By consolidating configuration, lifecycle, and UI conveniences into one app, cc-switch significantly reduces friction when moving between different AI toolchains and environments.</p>
                    </div>
                </div>
            </section>

            <section class="repo" data-seen-before="1">
                <div class="repo-header-row">
                    <h3>7. <a href="https://github.com/D4Vinci/Scrapling" target="_blank" rel="noopener noreferrer">D4Vinci/Scrapling</a> <span class="seen-badge">Not new today</span></h3>
                    <button type="button" class="repo-toggle" aria-expanded="true">Hide details</button>
                </div>
                <div class="repo-body">
                    <p class="description">üï∑Ô∏è An adaptive Web Scraping framework that handles everything from a single request to a full-scale crawl!</p>
                    <p class="meta">
                        <span class="language">Python</span> |
                        <span class="stars">17,069</span>
                        | <span class="today">2,902 stars today</span>
                    </p>
                    <p class="history">First seen: February 25, 2026 | Consecutive daily streak: 3 days</p>
                    <div class="ai-summary">
                        <h4>Analysis</h4>
                        <p>Scrapling is an adaptive web-scraping framework that scales from single HTTP requests to full, concurrent crawls with pause/resume and real-time streaming. Its key features include an adaptive parser that relocates elements after site changes, several fetcher types that spoof fingerprints and bypass anti-bot systems (including Cloudflare Turnstile), Playwright-backed dynamic fetchers for JS-heavy sites, session and proxy rotation, and a Scrapy-like Spider API for async, multi-session crawls with checkpoint persistence. Technically it combines fast HTTP fetchers with TLS/fingerprint impersonation, headless browser automation for dynamic content, intelligent similarity algorithms for selector recovery, and built-in serialization/streaming pipelines to deliver high-throughput, memory-efficient scraping.</p>
<p>This project benefits web scrapers, data engineers, researchers, and companies that need reliable, scalable extraction from modern, bot-protected sites because it reduces maintenance when sites change and simplifies anti-bot handling. It‚Äôs trending due to its AI integration (MCP server for cost-efficient, targeted extraction), strong developer ergonomics (full type coverage, interactive shell, Docker images), and production-ready features like proxy rotation, real-time streaming, and robust test coverage that make adoption attractive for both individual scrapers and teams.</p>
                    </div>
                </div>
            </section>

            <section class="repo" data-seen-before="0">
                <div class="repo-header-row">
                    <h3>8. <a href="https://github.com/ruvnet/claude-flow" target="_blank" rel="noopener noreferrer">ruvnet/claude-flow</a> </h3>
                    <button type="button" class="repo-toggle" aria-expanded="true">Hide details</button>
                </div>
                <div class="repo-body">
                    <p class="description">üåä The leading agent orchestration platform for Claude. Deploy intelligent multi-agent swarms, coordinate autonomous workflows, and build conversational AI systems. Features enterprise-grade architecture, distributed swarm intelligence, RAG integration, and native Claude Code / Codex Integration</p>
                    <p class="meta">
                        <span class="language">TypeScript</span> |
                        <span class="stars">15,084</span>
                        | <span class="today">215 stars today</span>
                    </p>
                    <p class="history">First seen: February 27, 2026 | Consecutive daily streak: 1 day</p>
                    <div class="ai-summary">
                        <h4>Analysis</h4>
                        <p>Ruflo (Ruflo v3) is an enterprise-grade AI orchestration platform that transforms Claude Code into a multi-agent development system, deploying 60+ specialized agents in coordinated swarms to automate complex software engineering workflows. Key features include hierarchical and mesh swarm topologies with queen/worker coordination, fault-tolerant consensus (Raft/BFT/Gossip/CRDT), RAG-style vector memory and knowledge graphs, native MCP/Claude Code integration, multi‚Äëprovider LLM failover, and an extensible plugin/marketplace. Architecturally, requests flow from CLI/MCP through a Q‚ÄëLearning/MoE router into agent swarms that consult RuVector-accelerated components (HNSW, Flash Attention, LoRA, Int8 quant, SONA, EWC++) and a closed learning loop (RETRIEVE‚ÜíJUDGE‚ÜíDISTILL‚ÜíCONSOLIDATE‚ÜíROUTE) to adapt routing and behavior. The stack also supports local model runtimes, WebAssembly transforms, diagnostics, and production security controls for safe automated execution.</p>
<p>Organizations building conversational AI, large-scale code automation, or ML‚Äëops pipelines‚Äîespecially engineering and security teams in enterprises‚Äîwill benefit most from Ruflo‚Äôs coordinated agent orchestration and audit-ready architecture. It is especially useful for automating coding, testing, review, documentation, and security workflows where agents must share context, reach consensus, and learn from outcomes over time. Ruflo is gaining traction because it combines high-performance vector search and model-efficiency techniques (LoRA/quantization), multi‚ÄëLLM flexibility, and native Claude Code tooling into a production-oriented platform that addresses scalability, cost, and safety concerns driving current multi-agent AI adoption.</p>
                    </div>
                </div>
            </section>

            <section class="repo" data-seen-before="1">
                <div class="repo-header-row">
                    <h3>9. <a href="https://github.com/ruvnet/ruvector" target="_blank" rel="noopener noreferrer">ruvnet/ruvector</a> <span class="seen-badge">Not new today</span></h3>
                    <button type="button" class="repo-toggle" aria-expanded="true">Hide details</button>
                </div>
                <div class="repo-body">
                    <p class="description">RuVector is a High Performance, Real-Time, Self-Learning, Vector Graph Neural Network, and Database built in Rust.</p>
                    <p class="meta">
                        <span class="language">Rust</span> |
                        <span class="stars">1,682</span>
                        | <span class="today">436 stars today</span>
                    </p>
                    <p class="history">First seen: February 25, 2026 | Consecutive daily streak: 3 days</p>
                    <div class="ai-summary">
                        <h4>Analysis</h4>
                        <p>RuVector is a high-performance, real-time vector graph database and learning platform written in Rust that unifies vector search, graph queries, on-device LLM inference, and self-learning GNN layers in one package. It stores embeddings with HNSW (including hyperbolic variants), exposes Cypher-like graph queries, and applies SONA/GNN learning so search quality improves from usage; it supports local model runtimes (GGUF with Metal/CUDA/ANE), 46 attention mechanisms, sublinear solvers, and SQL-accessible math and topological routines. Operationally it can ship as a single .rvf cognitive container that boots as a Linux microservice, accelerates hot paths with eBPF, runs in-browser via a tiny WASM runtime, and maintains tamper-evident witness chains plus copy-on-write branching for distributed replication (Raft, multi-master vector clocks, auto-sharding). The project also integrates with Postgres/pgvector, Model Context Protocol, and orchestration platforms for agented workflows.</p>
<p>Teams building RAG systems, multi-agent orchestration, edge/offline AI, or high-throughput domain applications (e.g., genomics, diagnostics, search/recommendation) benefit from RuVector because it combines storage, real-time graph reasoning, and local inference into a single deployable stack. Its single-file cognitive container, open-source MIT license, and local/wasm runtimes reduce infrastructure and per-query dependencies while providing cryptographic auditability and horizontal scaling, which appeals to security-conscious and cost-sensitive deployments. Researchers and engineers gain access to a wide set of ML primitives (attention variants, spiking nets, routing, LoRA/EWC++) and sublinear algorithms that accelerate large-scale analytics and model routing, helping explain why the project is gaining attention in the vector/AI tooling space.</p>
                    </div>
                </div>
            </section>

            <section class="repo" data-seen-before="1">
                <div class="repo-header-row">
                    <h3>10. <a href="https://github.com/moonshine-ai/moonshine" target="_blank" rel="noopener noreferrer">moonshine-ai/moonshine</a> <span class="seen-badge">Not new today</span></h3>
                    <button type="button" class="repo-toggle" aria-expanded="true">Hide details</button>
                </div>
                <div class="repo-body">
                    <p class="description">Fast and accurate automatic speech recognition (ASR) for edge devices</p>
                    <p class="meta">
                        <span class="language">C</span> |
                        <span class="stars">5,422</span>
                        | <span class="today">245 stars today</span>
                    </p>
                    <p class="history">First seen: February 16, 2026 | Consecutive daily streak: 1 day</p>
                    <div class="ai-summary">
                        <h4>Analysis</h4>
                        <p>Moonshine Voice is an open-source AI toolkit for building real-time, on-device automatic speech recognition and voice interfaces. Its main features include low-latency streaming models optimized for live transcription, speaker diarization, and intent/command recognition, with models ranging from tiny (‚âà26 MB) to larger, production-grade sizes trained from scratch and supporting multiple languages. Technically it avoids Whisper‚Äôs fixed 30-second window and redundant reprocessing by using streaming architectures and caching so computation is done incrementally while the user is still talking, yielding much lower latency and smaller parameter counts. Cross-platform libraries and examples (Python, C++/cmake, iOS, Android, macOS, Windows, Raspberry Pi) make deployment on a wide range of edge devices straightforward.</p>
<p>This project is valuable for developers and product teams who need responsive, private, offline voice interfaces on mobile, IoT, wearables, and other constrained hardware. It enables use cases such as hands-free controls, real-time transcription, command recognition, and accessibility features where latency, privacy, and power/size constraints are critical. Moonshine is gaining traction because of growing demand for on-device AI, the limitations of batch-oriented models like Whisper for streaming applications, and the appeal of a unified, optimized toolkit for heterogeneous edge platforms.</p>
                    </div>
                </div>
            </section>

            </div>
        </article>
    </main>
    <footer>
        <p>Generated automatically. Data from <a href="https://github.com/trending">GitHub Trending</a>.</p>
    </footer>

<script>
(() => {
    const readKey = "gtd:read_days:gh:v1";
    const dayStr = "2026-02-27";

    let stored = [];
    try {
        stored = JSON.parse(localStorage.getItem(readKey) || "[]");
        if (!Array.isArray(stored)) {
            stored = [];
        }
    } catch (_err) {
        stored = [];
    }

    if (!stored.includes(dayStr)) {
        stored.push(dayStr);
        stored.sort();
        localStorage.setItem(readKey, JSON.stringify(stored));
    }

    const collapseParam = new URLSearchParams(window.location.search).get("collapse_seen");
    const collapseSeen = collapseParam === "0" ? false : true;

    function setCollapsed(repoEl, collapsed) {
        repoEl.classList.toggle("collapsed", collapsed);
        const button = repoEl.querySelector(".repo-toggle");
        if (button) {
            button.textContent = collapsed ? "Show details" : "Hide details";
            button.setAttribute("aria-expanded", String(!collapsed));
        }
    }

    const repos = Array.from(document.querySelectorAll("section.repo[data-seen-before]"));

    repos.forEach((repoEl) => {
        const toggle = repoEl.querySelector(".repo-toggle");
        if (!toggle) {
            return;
        }

        toggle.addEventListener("click", () => {
            setCollapsed(repoEl, !repoEl.classList.contains("collapsed"));
        });
    });

    const collapseBtn = document.getElementById("collapse-seen-btn");
    const expandBtn = document.getElementById("expand-all-btn");

    if (collapseBtn) {
        collapseBtn.addEventListener("click", () => {
            repos.forEach((repoEl) => {
                if (repoEl.dataset.seenBefore === "1") {
                    setCollapsed(repoEl, true);
                }
            });
        });
    }

    if (expandBtn) {
        expandBtn.addEventListener("click", () => {
            repos.forEach((repoEl) => setCollapsed(repoEl, false));
        });
    }

    if (collapseSeen) {
        repos.forEach((repoEl) => {
            if (repoEl.dataset.seenBefore === "1") {
                setCollapsed(repoEl, true);
            }
        });
    }
})();
</script>

</body>
</html>
