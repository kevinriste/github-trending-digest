<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>GitHub Trending - February 12, 2026</title>
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <header>
        <h1>GitHub Trending Digest - February 12, 2026</h1>
        <nav>
            <a href="../">&larr; Back to Calendar</a>
        </nav>
    </header>
    <main>
        <article>
            <div class="repos">

            <section class="repo">
                <h3>1. <a href="https://github.com/google/langextract" target="_blank">google/langextract</a></h3>
                <p class="description">A Python library for extracting structured information from unstructured text using LLMs with precise source grounding and interactive visualization.</p>
                <p class="meta">
                    <span class="language">Python</span> |
                    <span class="stars">30,706 stars</span>
                    | <span class="today">3,186 stars today</span>
                </p>
                <div class="ai-summary">
                    <h4>Analysis</h4>
                    <p>LangExtract is a Python library that uses large language models to extract structured information from unstructured text while precisely grounding every extraction to its location in the source. Its main features include enforced structured outputs via few‑shot examples and controlled-generation-capable models (e.g., Gemini), optimized long‑document handling through chunking, parallel processing and multiple passes, and instant interactive HTML visualization plus JSONL output for auditability. Technically, users provide a prompt description and exemplar extractions; the library orchestrates model calls, records extraction offsets for traceability, and supports cloud and local model backends (including Vertex AI batch and Ollama) for scaling. The tool is designed for easy integration into review and downstream workflows by persisting structured annotations and producing self-contained visual reviews.</p>
<p>This project is valuable for teams that need to turn noisy text into auditable, schema‑conformant data—examples include clinical and radiology reporting, legal and compliance document review, literature analysis, and large‑scale data labeling. Data scientists, ML engineers, clinicians, and compliance officers benefit from its source grounding and interactive review which reduce verification effort and support regulatory traceability. LangExtract is timely because the surge in LLM capability has increased demand for reliable, explainable extraction tools that scale to long documents without model fine‑tuning, and its flexible model support and visualization lower the barrier to adoption. It also accelerates prototyping and production deployment by combining model orchestration, scalable execution options, and human-in-the-loop validation.</p>

                </div>
            </section>

            <section class="repo">
                <h3>2. <a href="https://github.com/github/gh-aw" target="_blank">github/gh-aw</a></h3>
                <p class="description">GitHub Agentic Workflows</p>
                <p class="meta">
                    <span class="language">Go</span> |
                    <span class="stars">1,824 stars</span>
                    | <span class="today">390 stars today</span>
                </p>
                <div class="ai-summary">
                    <h4>Analysis</h4>
                    <p>GitHub Agentic Workflows lets you author &quot;agentic&quot; automation in natural-language Markdown and run those workflows directly in GitHub Actions. Key features include natural-language workflow authoring, sandboxed execution, default read-only permissions with write allowed only through sanitized safe-outputs, network isolation, SHA-pinned dependencies, tool allow-listing, compile-time validation, and optional human approval gates. Technically the system parses the Markdown workflows into Actions-compatible runs that invoke AI models and permitted tools within tightly controlled GitHub Action environments, while companion components like the Agent Workflow Firewall and MCP Gateway handle network egress control and unified model-gateway routing. The repo includes quick-starts, full documentation, examples (including Peli’s Agent Factory), and contribution guidelines to get teams started safely.</p>
<p>The project is valuable for teams that want to automate repository tasks—triage, CI workflows, code generation, documentation, and dependency maintenance—while maintaining audibility and security controls. Development teams, platform engineers, security teams, and open-source maintainers benefit from fast, repeatable agent-driven workflows that minimize manual effort without exposing uncontrolled write access or network activity. Its rise reflects growing demand for AI-driven automation combined with enterprise-grade guardrails and seamless integration into the existing GitHub Actions ecosystem.</p>

                </div>
            </section>

            <section class="repo">
                <h3>3. <a href="https://github.com/microsoft/PowerToys" target="_blank">microsoft/PowerToys</a></h3>
                <p class="description">Microsoft PowerToys is a collection of utilities that supercharge productivity and customization on Windows</p>
                <p class="meta">
                    <span class="language">C#</span> |
                    <span class="stars">129,472 stars</span>
                    | <span class="today">67 stars today</span>
                </p>
                <div class="ai-summary">
                    <h4>Analysis</h4>
                    <p>Microsoft PowerToys is a curated collection of over 25 Windows utilities that enhance productivity and customization, including tools like FancyZones (window layouts), PowerToys Run (quick launcher), Color Picker, PowerRename, Image Resizer, Keyboard Manager, Text Extractor, Command Palette, Advanced Paste and many more. The project is modular and extensible, with an extensions SDK and CI-built packages (MSIX/winget/Store), and it integrates with the OS via shell extensions, global hotkeys, clipboard hooks, DPI-aware UI, and enterprise controls (ADMX/GPO). Development includes both native and .NET components, localized UIs, and plug-in support so new functionality can be added as PowerToys extensions. Releases are published on GitHub and distributed via installers for per-user or machine-wide scopes, package managers, and the Microsoft Store.</p>
<p>PowerToys delivers tangible productivity gains for power users, developers, designers, and IT administrators by streamlining common workflows—window management, fast app/file launching, bulk renaming, quick color selection, clipboard transformations, and workspace snapshots—reducing repetitive tasks and context switching. Enterprise and IT teams benefit from policy controls and machine-wide deployment options, while hobbyists and accessibility-focused users gain custom keyboard and mouse remapping and accessibility utilities. The project’s active release cadence, broad community contributions, Microsoft backing, and addition of modern features (extensions SDK, AI-enabled clipboard tooling, improved multi-monitor handling) explain its strong adoption and ongoing relevance.</p>

                </div>
            </section>

            <section class="repo">
                <h3>4. <a href="https://github.com/ChromeDevTools/chrome-devtools-mcp" target="_blank">ChromeDevTools/chrome-devtools-mcp</a></h3>
                <p class="description">Chrome DevTools for coding agents</p>
                <p class="meta">
                    <span class="language">TypeScript</span> |
                    <span class="stars">24,075 stars</span>
                    | <span class="today">120 stars today</span>
                </p>
                <div class="ai-summary">
                    <h4>Analysis</h4>
                    <p>Chrome DevTools MCP exposes the full Chrome DevTools surface to coding agents by running a Node.js MCP (Model-Context-Protocol) server that connects to a live Chrome instance. It provides performance tracing and analysis, network request inspection, screenshots, and source‑mapped console stack traces, and uses Puppeteer for reliable automation and automatic waiting for action results. The server can optionally fetch field data from the Chrome User Experience Report (CrUX) and integrates with many MCP clients (Gemini, Claude, Copilot, Cursor, IDE plugins) via a simple npx-installable command. It collects anonymized usage statistics by default and offers flags and environment variables to opt out for privacy or CI environments.</p>
<p>This tool is valuable for AI-assisted development workflows, QA and performance engineers, SREs, and developers who need reproducible, programmatic access to browser state for debugging, testing, and optimization. By surfacing DevTools to agents, it enables automated trace analysis, network forensics, and actionable remediation suggestions directly from assistant platforms and IDEs, cutting manual triage time. It’s gaining traction because of rapid adoption of model-driven developer tooling and the demand for trusted, automatable interfaces that let AI assistants interact with real browsers across ecosystems.</p>

                </div>
            </section>

            <section class="repo">
                <h3>5. <a href="https://github.com/EveryInc/compound-engineering-plugin" target="_blank">EveryInc/compound-engineering-plugin</a></h3>
                <p class="description">Official Claude Code compound engineering plugin</p>
                <p class="meta">
                    <span class="language">TypeScript</span> |
                    <span class="stars">8,545 stars</span>
                    | <span class="today">272 stars today</span>
                </p>
                <div class="ai-summary">
                    <h4>Analysis</h4>
                    <p>This repository provides a Claude Code plugin marketplace entry called the Compound Engineering Plugin that packages agent-driven engineering workflows and utilities to make each unit of work easier than the last. It ships a Bun/TypeScript CLI that converts Claude Code plugins into OpenCode, Codex, and Factory Droid formats and installs them, writing output to standard locations (~/.config/opencode, ~/.codex/, ~/.factory/) while mapping tool names and trimming namespaces as needed. The tool also supports syncing personal Claude Code configuration (symlinking skills from ~/.claude/ and copying MCP server settings) and exposes high-level workflow commands (/workflows:plan, /workflows:work, /workflows:review, /workflows:compound) to implement the Plan → Work → Review → Compound cycle. Installation and conversion are driven by simple CLI commands (plugin marketplace install or bunx conversion), making cross-format exports and local development straightforward.</p>
<p>The project is useful for engineering teams and individuals adopting agent-centric workflows who need to standardize, migrate, or share skills across multiple agent ecosystems, or to codify review-driven processes that reduce future friction. By enabling portable exports, symlinked personal configs, and documented multi-agent workflows, it lowers the barrier to integrating Claude-based tooling into broader toolchains and accelerates onboarding. Teams focused on repeatable, high-quality execution and knowledge capture will benefit from the compound engineering philosophy that emphasizes planning and review to limit technical debt. Its emphasis on conversion and interoperability is timely given growing demand for cross-platform agent tooling and automation in software engineering.</p>

                </div>
            </section>

            <section class="repo">
                <h3>6. <a href="https://github.com/patchy631/ai-engineering-hub" target="_blank">patchy631/ai-engineering-hub</a></h3>
                <p class="description">In-depth tutorials on LLMs, RAGs and real-world AI agent applications.</p>
                <p class="meta">
                    <span class="language">Jupyter Notebook</span> |
                    <span class="stars">28,774 stars</span>
                    | <span class="today">154 stars today</span>
                </p>
                <div class="ai-summary">
                    <h4>Analysis</h4>
                    <p>AI Engineering Hub is a curated collection of 93+ production-ready projects and in-depth tutorials focused on LLMs, RAG, agents, multimodal applications, and model development. The repository organizes content by skill level (beginner → advanced) and provides runnable examples such as OCR, chat UIs, basic and agentic RAG pipelines, MCP integrations, and fine-tuning guides. Technically it supplies end-to-end reference stacks that combine model runtimes (Llama, Gemma, Qwen, DeepSeek, etc.), vector databases (Qdrant, Milvus), orchestration and UI tools (Streamlit, Chainlit), and agent frameworks (CrewAI, AutoGen), illustrating document ingestion, embedding/retrieval, prompt engineering, agent loops, and deployment patterns.</p>
<p>This hub is valuable for beginners, practitioners, researchers, and engineering teams who want to learn, prototype, benchmark, or productionize modern AI systems by following reproducible examples and architectures. Typical use cases include building private chatbots, document and multimodal RAG assistants, multi-agent workflows, model comparison/evaluation, and fine-tuning pipelines, all of which accelerate development and reduce integration friction. Its relevance is amplified by the rapid adoption of LLMs and agent frameworks and the growing need for practical, end-to-end implementations that combine retrieval, multimodality, and orchestration.</p>

                </div>
            </section>

            <section class="repo">
                <h3>7. <a href="https://github.com/cheahjs/free-llm-api-resources" target="_blank">cheahjs/free-llm-api-resources</a></h3>
                <p class="description">A list of free LLM inference resources accessible via API.</p>
                <p class="meta">
                    <span class="language">Python</span> |
                    <span class="stars">9,719 stars</span>
                    | <span class="today">440 stars today</span>
                </p>
                <div class="ai-summary">
                    <h4>Analysis</h4>
                    <p>This repository is a curated, machine-generated index of services that provide free or trial access to LLM inference via APIs. Its main features are categorized listings (free providers, providers with trial credits) that enumerate supported models, usage limits, verification requirements, and caveats (e.g., data training opt-ins or excluded illegitimate services). Technically the README is generated by a script (src/pullavailablemodels.py) that pulls available model and quota information and compiles provider-specific details so users can quickly compare options. The sheet-like format highlights per-provider quotas, example models, and any special restrictions to help developers pick appropriate endpoints.</p>
<p>The project is valuable for developers, researchers, educators, startups, and hobbyists who need to discover cost-free or low-cost LLM endpoints for prototyping, experimentation, or educational projects. By aggregating up-to-date quotas, verification requirements, and supported models it saves time and reduces vendor research overhead, enabling faster iteration and cost control. It’s trending because the LLM ecosystem is rapidly expanding with new models, providers, and trial offers, creating demand for a single reference that tracks usable API access.</p>

                </div>
            </section>

            </div>
        </article>
    </main>
    <footer>
        <p>Generated automatically. Data from <a href="https://github.com/trending">GitHub Trending</a>.</p>
    </footer>
</body>
</html>
