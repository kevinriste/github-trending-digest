<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>GitHub Trending - February 02, 2026</title>
    <link rel="stylesheet" href="style.css">
</head>
<body>
    <header>
        <h1>GitHub Trending Digest</h1>
        <p class="subtitle">February 02, 2026</p>
        <nav>
            <a href="index.html">&larr; Back to Calendar</a>
        </nav>
    </header>
    <main>
        <div class="repos">

            <article class="repo">
                <h3>1. <a href="https://github.com/openclaw/openclaw" target="_blank">openclaw/openclaw</a></h3>
                <p class="description">Your own personal AI assistant. Any OS. Any Platform. The lobster way. ðŸ¦ž</p>
                <p class="meta">
                    <span class="language">TypeScript</span> |
                    <span class="stars">144,844 stars</span>
                    | <span class="today">10,794 stars today</span>
                </p>
                <div class="ai-summary">
                    <h4>Analysis</h4>
                    <p>OpenClaw is a localâ€‘first personal AI assistant you run on your own devices that connects to the messaging channels you already use (WhatsApp, Telegram, Slack, Discord, Google Chat, Signal, iMessage, BlueBubbles, Matrix, Zalo, WebChat, etc.), plus native voice and Canvas experiences on macOS/iOS/Android. Technically it is built around a Gateway WebSocket control plane that manages sessions, presence, routing and tools, and routes work to Pi agent runtimes via RPC; a CLI onboarding wizard and optional daemon simplify installation and service lifecycle. It supports model subscriptions (Anthropic, OpenAI) with failover, streaming tool outputs, browser/CDP control, a live A2UI Canvas, voice wake/talk mode, and firstâ€‘class tooling for automation (cron, nodes, webhooks). The project targets Node â‰¥22, is installable via npm/pnpm/bun, and can be run from source for development and custom integrations.</p>
<p>OpenClaw is valuable to privacyâ€‘conscious individuals, power users, and developers who want an alwaysâ€‘on, extensible assistant that integrates with existing chat platforms and local devices without depending on hosted SaaS. Itâ€™s useful for automations, multimodal workflows (speech, camera, screen recording, browser actions), custom agent routing, and onâ€‘prem or personal deployments where security and DM policies matter. Organizations, hobbyists, and builders benefit from its extensibility (skills, tools, Canvas) and remote access options (Tailscale/SSH), enabling bespoke assistant workflows. Its momentum stems from being open source, platformâ€‘agnostic, featureâ€‘rich (multiâ€‘channel, voice, Canvas), and focused on safety and developer ergonomics.</p>

                </div>
            </article>

            <article class="repo">
                <h3>2. <a href="https://github.com/ThePrimeagen/99" target="_blank">ThePrimeagen/99</a></h3>
                <p class="description">Neovim AI agent done right</p>
                <p class="meta">
                    <span class="language">Lua</span> |
                    <span class="stars">2,811 stars</span>
                    | <span class="today">781 stars today</span>
                </p>
                <div class="ai-summary">
                    <h4>Analysis</h4>
                    <p>99 is a Neovim plugin that implements an AI agent workflow for in-editor code assistance, emphasizing restricted, reproducible actions rather than open-ended chat. Its main features include a skill/rule system (SKILL.md/AGENT.md), completion integration (cmp with @-prefixed skill completion), commands for fill-in-function and visual-edit flows, custom rule directories, and file-backed logging and debug facilities. Technically it is a Lua plugin intended to be loaded with Lazy, requires the opencode backend to execute model requests, and manipulates buffer contents (virtual text, replacements) while planning tighter Treesitter/LSP context gathering; the README notes various known UX and replacement edge cases because the project is still alpha.</p>
<p>The project is valuable for Neovim-centric developers who want tighter, auditable AI-assisted code editsâ€”useful for refactoring, instrumentation, and template-driven changesâ€”because it scopes AI actions with rules and logs to reduce surprising edits. It benefits users who prefer keyboard-driven workflows and customizable, project-specific skill files, and itâ€™s gaining attention due to ThePrimeagenâ€™s audience, Twitch deep dives, and its focus on integrating AI into an existing editor ecosystem. As an experimental, opinionated approach to in-editor AI, itâ€™s attractive to power users and plugin authors exploring reproducible, constrained LLM workflows.</p>

                </div>
            </article>

            <article class="repo">
                <h3>3. <a href="https://github.com/pedramamini/Maestro" target="_blank">pedramamini/Maestro</a></h3>
                <p class="description">Agent Orchestration Command Center</p>
                <p class="meta">
                    <span class="language">TypeScript</span> |
                    <span class="stars">1,057 stars</span>
                    | <span class="today">49 stars today</span>
                </p>
                <div class="ai-summary">
                    <h4>Analysis</h4>
                    <p>Maestro is a cross-platform desktop app for orchestrating fleets of AI agents and projects with a keyboard-first UX and a headless CLI. Its core features include Git worktrees for isolated parallel branches, Auto Run and filesystem-based playbooks that batch-process markdown checklists into per-task AI sessions, group chat with a moderator AI, multi-agent parallel terminals, message queueing, session discovery, and integrated file and git tooling. Technically it runs sessions against supported coding agents (Claude Code, OpenAI Codex, OpenCode), exposes a built-in web server for mobile remote control (with optional Cloudflare tunneling), and provides JSONL-friendly CLI output and automation hooks for CI/CD and cron-driven workflows.</p>
<p>Maestro is aimed at power users, developer teams, and AI-first hackers who need to manage many concurrent agent-driven tasks, automate long-running flows, and keep context clean across parallel efforts. Typical use cases are isolated agent worktrees for simultaneous feature work, reproducible playbooks for automation and deployments, multi-agent architecture discussions, and running agent-driven jobs from CI or cron. Itâ€™s gaining traction because it addresses the growing need for reliable agent orchestration, reproducible workflows, and developer-centric tooling that combines automation, analytics, and multi-provider support.</p>

                </div>
            </article>

            <article class="repo">
                <h3>4. <a href="https://github.com/kovidgoyal/calibre" target="_blank">kovidgoyal/calibre</a></h3>
                <p class="description">The official source code repository for the calibre ebook manager</p>
                <p class="meta">
                    <span class="language">Python</span> |
                    <span class="stars">23,680 stars</span>
                    | <span class="today">37 stars today</span>
                </p>
                <div class="ai-summary">
                    <h4>Analysis</h4>
                    <p>calibre is an eâ€‘book manager that lets users view, convert, edit and catalog e-books across major formats and sync to reading devices. Its feature set includes format conversion, metadata fetching, news downloading and conversion to e-book, an integrated reader/editor, a content server and commandâ€‘line utilities. Technically it is a crossâ€‘platform Python/Qt application with a modular plugin architecture and a conversion backend exposed via commandâ€‘line tools and a content server for remote access.</p>
<p>calibre is valuable for individual readers, librarians, publishers and developers who need to organize large collections, convert between formats, enrich metadata and deliver content to devices or over networks. Its broad format support, powerful conversion tools, scripting/command-line interfaces and plugin ecosystem make it suitable for automation, archival and publishing workflows. As a mature, actively maintained openâ€‘source project that runs on Linux, Windows and macOS, calibre remains popular for both end users and integrators seeking a flexible, extensible eâ€‘book management solution.</p>

                </div>
            </article>

            <article class="repo">
                <h3>5. <a href="https://github.com/badlogic/pi-mono" target="_blank">badlogic/pi-mono</a></h3>
                <p class="description">AI agent toolkit: coding agent CLI, unified LLM API, TUI &amp; web UI libraries, Slack bot, vLLM pods</p>
                <p class="meta">
                    <span class="language">TypeScript</span> |
                    <span class="stars">5,127 stars</span>
                    | <span class="today">613 stars today</span>
                </p>
                <div class="ai-summary">
                    <h4>Analysis</h4>
                    <p>Pi Monorepo is a toolkit for building and deploying AI agents and LLM-backed applications, provided as a set of interrelated Node/TypeScript packages. It exposes a unified multi-provider LLM API (@mariozechner/pi-ai) that can target OpenAI, Anthropic, Google, etc., an agent runtime with tool-calling and state management (@mariozechner/pi-agent-core), and end-user components including a coding-agent CLI, a Slack bot, a terminal UI library with differential rendering, web UI components, and a CLI for managing vLLM GPU pods. The project is organized as a monorepo with npm-based build/test workflows, TypeScript typings, and package-level responsibilities for runtime, UI, and deployment orchestration to let developers compose agents, UIs, and GPU deployments consistently.</p>
<p>This project is valuable to developers and platform engineers building production or experimental LLM agents, chat interfaces, and managed GPU deployments because it removes boilerplate around provider integration, agent orchestration, and UI wiring. Teams building coding assistants, internal chatbots (Slack), and GPU-backed inference clusters (vLLM) can reuse the agent runtime, unified API, and UIs to accelerate development and reduce vendor lock-in. Its relevance is driven by growing demand for agentized workflows, multi-provider flexibility, and scalable inference on GPU pods.</p>

                </div>
            </article>

        </div>
    </main>
    <footer>
        <p>Generated automatically. Data from <a href="https://github.com/trending">GitHub Trending</a>.</p>
    </footer>
</body>
</html>
