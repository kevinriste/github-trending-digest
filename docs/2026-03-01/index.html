<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>GitHub Trending - March 01, 2026</title>
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <header>
        <h1>GitHub Trending Digest - March 01, 2026</h1>
        <nav>
            <a href="../">&larr; GitHub Calendar</a>
            <a href="../hn/2026-03-01/">Hacker News</a>
        </nav>
    </header>
    <main>
        <div class="repo-controls">
            <button id="collapse-seen-btn" type="button">Collapse Repos Not New Today</button>
            <button id="expand-all-btn" type="button">Expand All</button>
        </div>
        <p class="seen-help">Repos marked "Not new today" appeared on one or more previous daily pages.</p>
        <article>
            <div class="repos">

            <section class="repo" data-seen-before="1">
                <div class="repo-header-row">
                    <h3>1. <a href="https://github.com/ruvnet/wifi-densepose" target="_blank" rel="noopener noreferrer">ruvnet/wifi-densepose</a> <span class="seen-badge">Not new today</span></h3>
                    <button type="button" class="repo-toggle" aria-expanded="true">Hide details</button>
                </div>
                <div class="repo-body">
                    <p class="description">Production-ready implementation of InvisPose - a revolutionary WiFi-based dense human pose estimation system that enables real-time full-body tracking through walls using commodity mesh routers</p>
                    <p class="meta">
                        <span class="language">Rust</span> |
                        <span class="stars">12,747</span>
                        | <span class="today">2,152 stars today</span>
                    </p>
                    <p class="history">First seen: February 15, 2026 | Consecutive daily streak: 2 days</p>
                    <div class="ai-summary">
                        <h4>Analysis</h4>
                        <p>WiFi-DensePose is a production-ready system that estimates dense human pose and tracks multiple people in real time using WiFi Channel State Information (CSI) instead of cameras. It fuses CSI preprocessing, phase sanitization, and feature extraction with a DensePose neural head and multi-object tracking, and exposes REST and WebSocket APIs for live streaming and control. The project includes a high-performance Rust port with validated microbenchmarks and full test coverage, and supports deployment via pip, Docker, or building from source for low-latency, resource-efficient operation.</p>
<p>This technology delivers privacy-preserving, through-wall sensing suited to healthcare (fall and vital-sign monitoring), fitness, smart homes, security, and disaster response (the WiFi‚ÄëMat module for survivor detection and triage). Enterprise features‚Äîauthentication, rate limiting, monitoring‚Äîand WASM support in the Rust implementation make it attractive to commercial integrators and IoT vendors. Researchers, emergency responders, and product teams benefit from camera-less, hardware-agnostic sensing on commodity routers, and the combination of strong performance, test coverage, and practical APIs explains its growing interest.</p>
                    </div>
                </div>
            </section>

            <section class="repo" data-seen-before="1">
                <div class="repo-header-row">
                    <h3>2. <a href="https://github.com/moeru-ai/airi" target="_blank" rel="noopener noreferrer">moeru-ai/airi</a> <span class="seen-badge">Not new today</span></h3>
                    <button type="button" class="repo-toggle" aria-expanded="true">Hide details</button>
                </div>
                <div class="repo-body">
                    <p class="description">üíñüß∏ Self hosted, you-owned Grok Companion, a container of souls of waifu, cyber livings to bring them into our worlds, wishing to achieve Neuro-sama&#x27;s altitude. Capable of realtime voice chat, Minecraft, Factorio playing. Web / macOS / Windows supported.</p>
                    <p class="meta">
                        <span class="language">TypeScript</span> |
                        <span class="stars">19,489</span>
                        | <span class="today">1,065 stars today</span>
                    </p>
                    <p class="history">First seen: February 28, 2026 | Consecutive daily streak: 2 days</p>
                    <div class="ai-summary">
                        <h4>Analysis</h4>
                        <p>AIRI is a self‚Äëhosted, cross‚Äëplatform &quot;Grok Companion&quot; framework that creates persistent, interactive digital companions capable of real‚Äëtime voice chat, playing games like Minecraft and Factorio, and driving VTuber avatars (VRM/Live2D). It stitches together modular subsystems ‚Äî Brain (LLM integrations via xsai), Ears (audio input and client‚Äëside speech recognition), Mouth (TTS including ElevenLabs), Body (avatar control with WebGPU/WebAudio/WebAssembly), and in‚Äëbrowser databases (DuckDB WASM, pglite) ‚Äî and can offload to native acceleration (CUDA/Apple Metal via HuggingFace/candle) for local inference. The project provides web, desktop (tamagotchi) and mobile (PWA/capacitor) stages, extensive LLM provider support, and an extensible plugin and subproject ecosystem for automation, RAG/memory, and live streaming workflows.</p>
<p>AIRI is ideal for VTubers, streamers, hobbyists, researchers, and developers who want a privacy‚Äërespecting, customizable alternative to closed agents like Neuro‚Äësama, enabling game automation, multimodal interaction, and self‚Äëhosting. It appeals because of active development, broad backend/provider support, cross‚Äëplatform delivery (browser, desktop, mobile), and deployment tooling (pnpm, Nix, containers), making experimentation and production easier. The project is trending due to rising demand for live interactive AI agents, improvements in local and hybrid inference pipelines, and a growing open‚Äësource ecosystem that lowers the barrier to building real‚Äëtime digital companions.</p>
                    </div>
                </div>
            </section>

            <section class="repo" data-seen-before="1">
                <div class="repo-header-row">
                    <h3>3. <a href="https://github.com/anthropics/claude-code" target="_blank" rel="noopener noreferrer">anthropics/claude-code</a> <span class="seen-badge">Not new today</span></h3>
                    <button type="button" class="repo-toggle" aria-expanded="true">Hide details</button>
                </div>
                <div class="repo-body">
                    <p class="description">Claude Code is an agentic coding tool that lives in your terminal, understands your codebase, and helps you code faster by executing routine tasks, explaining complex code, and handling git workflows - all through natural language commands.</p>
                    <p class="meta">
                        <span class="language">Shell</span> |
                        <span class="stars">71,958</span>
                        | <span class="today">699 stars today</span>
                    </p>
                    <p class="history">First seen: February 22, 2026 | Consecutive daily streak: 2 days</p>
                    <div class="ai-summary">
                        <h4>Analysis</h4>
                        <p>Claude Code is an agentic coding tool that runs in your terminal, IDE, or as a GitHub tag and interacts with your local codebase through natural-language commands. Key features include executing routine tasks, explaining complex code, managing git workflows, and an extensible plugins directory that adds custom commands and agents. Technically it inspects repository files, runs shell and git commands on your behalf, and uses a language-model-based agent to interpret instructions and orchestrate actions. Installation is supported via a single-script installer, Homebrew/WinGet, or other platform-specific methods, and the project collects usage and conversation feedback with documented privacy safeguards.</p>
<p>This tool benefits individual developers, reviewers, and engineering teams who want to speed up repetitive workflows, improve onboarding, and reduce context switching when navigating unfamiliar code. It is trending because AI-driven developer tooling and natural-language interfaces substantially raise productivity, the plugin architecture enables customization for diverse workflows, and easy installation plus community channels (Discord, GitHub) accelerate adoption. The combination of code-aware automation, git integration, and extensibility makes it an attractive option for modern development environments.</p>
                    </div>
                </div>
            </section>

            <section class="repo" data-seen-before="0">
                <div class="repo-header-row">
                    <h3>4. <a href="https://github.com/tukaani-project/xz" target="_blank" rel="noopener noreferrer">tukaani-project/xz</a> </h3>
                    <button type="button" class="repo-toggle" aria-expanded="true">Hide details</button>
                </div>
                <div class="repo-body">
                    <p class="description">XZ Utils</p>
                    <p class="meta">
                        <span class="language">C</span> |
                        <span class="stars">1,245</span>
                        | <span class="today">107 stars today</span>
                    </p>
                    <p class="history">First seen: March 01, 2026 | Consecutive daily streak: 1 day</p>
                    <div class="ai-summary">
                        <h4>Analysis</h4>
                        <p>XZ Utils is a suite of compression tools and a C library that implement the .xz and legacy .lzma container formats using the LZMA2 compression algorithm. It provides command-line utilities (xz, xzcat, xzdec, lzma*/lzcat) and liblzma for embedding in applications, supports multiple integrity checks (CRC32, CRC64, SHA-256), and offers filter chains such as delta and BCJ to improve compression of specific data types like executables. The implementation uses a range coder-based LZMA2 engine with streaming APIs and optional multithreaded block compression to balance high compression ratios with practical compression/decompression throughput. Packaging features include support for stream concatenation, recovery records, and tunable presets for speed vs. size trade-offs.</p>
<p>Developers, system integrators, package maintainers, and users who need efficient archival or distribution of large datasets and executables benefit from XZ Utils because it delivers strong compression ratios with broad platform support and an embeddable library. It is widely used in Linux distributions and build systems, which drives visibility and ongoing contributions, and recent focus on multithreading, maintenance, and modern filter support keeps it relevant as data volumes and performance expectations increase. The combination of open-source licensing, proven stability, and strong compression makes it a popular choice for storage- and bandwidth-sensitive workflows.</p>
                    </div>
                </div>
            </section>

            <section class="repo" data-seen-before="1">
                <div class="repo-header-row">
                    <h3>5. <a href="https://github.com/Shubhamsaboo/awesome-llm-apps" target="_blank" rel="noopener noreferrer">Shubhamsaboo/awesome-llm-apps</a> <span class="seen-badge">Not new today</span></h3>
                    <button type="button" class="repo-toggle" aria-expanded="true">Hide details</button>
                </div>
                <div class="repo-body">
                    <p class="description">Collection of awesome LLM apps with AI Agents and RAG using OpenAI, Anthropic, Gemini and opensource models.</p>
                    <p class="meta">
                        <span class="language">Python</span> |
                        <span class="stars">98,330</span>
                        | <span class="today">635 stars today</span>
                    </p>
                    <p class="history">First seen: February 10, 2026 | Consecutive daily streak: 1 day</p>
                    <div class="ai-summary">
                        <h4>Analysis</h4>
                        <p>This repository is a curated collection of LLM-powered applications and templates showcasing Retrieval-Augmented Generation (RAG), AI agents, multi-agent teams, MCP, voice agents, and related tooling across OpenAI, Anthropic, Google Gemini, xAI and open-source models like Qwen and Llama. It organizes starter and advanced projects‚Äîagentic RAG, memory-enabled chat, voice/RAG agents, MCP integrations, multi-agent orchestration, and optimization/fine-tuning tutorials‚Äîeach with project-specific READMEs and run instructions. Technically the examples demonstrate embedding-based retrieval, agent orchestration patterns (function calling, tools, Pydantic structured outputs), hybrid local/cloud deployments, and workflows for fine-tuning and cost/context optimization, with reproducible setup via pip requirements and per-project code.</p>
<p>The collection provides practical blueprints for developers, researchers, and product teams to prototype, compare, and deploy LLM applications or learn agent design patterns and RAG pipelines. It‚Äôs useful for anyone building conversational apps, knowledge assistants, voice interfaces, autonomous agents, and domain-specific retrieval systems because it consolidates interoperable examples across major and open-source models. Its popularity stems from the rapid rise of agentic and multimodal LLM use cases, broad model support, and curated, well-documented reference implementations that accelerate adoption and experimentation.</p>
                    </div>
                </div>
            </section>

            <section class="repo" data-seen-before="1">
                <div class="repo-header-row">
                    <h3>6. <a href="https://github.com/ruvnet/ruflo" target="_blank" rel="noopener noreferrer">ruvnet/ruflo</a> <span class="seen-badge">Not new today</span></h3>
                    <button type="button" class="repo-toggle" aria-expanded="true">Hide details</button>
                </div>
                <div class="repo-body">
                    <p class="description">üåä The leading agent orchestration platform for Claude. Deploy intelligent multi-agent swarms, coordinate autonomous workflows, and build conversational AI systems. Features enterprise-grade architecture, distributed swarm intelligence, RAG integration, and native Claude Code / Codex Integration</p>
                    <p class="meta">
                        <span class="language">TypeScript</span> |
                        <span class="stars">16,686</span>
                        | <span class="today">928 stars today</span>
                    </p>
                    <p class="history">First seen: February 28, 2026 | Consecutive daily streak: 2 days</p>
                    <div class="ai-summary">
                        <h4>Analysis</h4>
                        <p>Ruflo v3 is an enterprise AI orchestration platform that converts Claude Code into a production-ready multi-agent system, enabling coordinated swarms of 60+ specialized agents for coding, testing, review, security, documentation, and DevOps. Key features include self-learning routing (SONA, Q-learning, MoE, SemanticRouter), fault-tolerant swarm coordination (Raft/BFT/Gossip/CRDT), fast vector RAG and memory (HNSW, ReasoningBank, Hyperbolic embeddings), multi-LLM provider failover, and an extensible plugin SDK. Technically it runs WASM kernels written in Rust for the policy engine, embeddings, and proof system, leverages optimized primitives (Flash Attention, LoRA, Int8 quantization) and a RETRIEVE‚ÜíJUDGE‚ÜíDISTILL‚ÜíCONSOLIDATE‚ÜíROUTE learning loop, and enforces production security controls like prompt-injection and command-injection protections. The stack also exposes MCP integration for native Claude Code usage and supports local and cloud LLMs with routing and cost/quality tradeoffs.</p>
<p>Organizations building conversational AI, autonomous workflows, or automated software engineering pipelines ‚Äî including engineering teams, SREs, AI researchers, and enterprises requiring auditability and scalability ‚Äî will benefit most from Ruflo. It‚Äôs trending because it packages contemporary capabilities (multi-agent orchestration, RAG, self-optimization, consensus protocols) into a performant, production-oriented stack with Rust/WASM performance, rapid HNSW retrieval, and a plugin marketplace, making it practical to move from prototypes to resilient deployments while optimizing cost and model quality.</p>
                    </div>
                </div>
            </section>

            <section class="repo" data-seen-before="1">
                <div class="repo-header-row">
                    <h3>7. <a href="https://github.com/bytedance/deer-flow" target="_blank" rel="noopener noreferrer">bytedance/deer-flow</a> <span class="seen-badge">Not new today</span></h3>
                    <button type="button" class="repo-toggle" aria-expanded="true">Hide details</button>
                </div>
                <div class="repo-body">
                    <p class="description">An open-source SuperAgent harness that researches, codes, and creates. With the help of sandboxes, memories, tools, skills and subagents, it handles different levels of tasks that could take minutes to hours.</p>
                    <p class="meta">
                        <span class="language">Python</span> |
                        <span class="stars">22,689</span>
                        | <span class="today">899 stars today</span>
                    </p>
                    <p class="history">First seen: February 26, 2026 | Consecutive daily streak: 4 days</p>
                    <div class="ai-summary">
                        <h4>Analysis</h4>
                        <p>DeerFlow is an open-source &quot;super agent&quot; harness that orchestrates sub-agents, persistent memory, sandboxed execution, and extensible skills to carry out complex, multi-step tasks. Technically it is a batteries-included runtime built on LangGraph and LangChain that runs tasks inside isolated sandboxes (local, Docker, or Kubernetes), provides a filesystem and toolset (web fetch, bash, file ops), and dynamically loads Markdown-defined skills and tools while managing context via summarization and long-term memory. The lead agent can spawn parallel scoped sub-agents with their own contexts and termination rules, collect structured results, and synthesize outputs such as reports, slide decks, or web pages. Configuration is driven by config.yaml and environment keys, with Docker and local development workflows supported for fast provisioning and reproducibility.</p>
<p>DeerFlow is useful for AI researchers, data scientists, engineering teams, and content creators who need agents that do real work‚Äînot just chat‚Äîbecause it combines execution, tooling, and memory in an auditable, extensible framework. It‚Äôs trending because it addresses practical limitations of single-pass LLM interactions (execution isolation, token management, long-running tasks, and parallel decomposition), ships with ready-made skills while remaining highly customizable, and benefits from an active rewrite and community-focused tooling that make it easy to experiment with modern LLMs and production workflows.</p>
                    </div>
                </div>
            </section>

            <section class="repo" data-seen-before="0">
                <div class="repo-header-row">
                    <h3>8. <a href="https://github.com/Wei-Shaw/claude-relay-service" target="_blank" rel="noopener noreferrer">Wei-Shaw/claude-relay-service</a> </h3>
                    <button type="button" class="repo-toggle" aria-expanded="true">Hide details</button>
                </div>
                <div class="repo-body">
                    <p class="description">CRS-Ëá™Âª∫Claude CodeÈïúÂÉèÔºå‰∏ÄÁ´ôÂºèÂºÄÊ∫ê‰∏≠ËΩ¨ÊúçÂä°ÔºåËÆ© Claude„ÄÅOpenAI„ÄÅGemini„ÄÅDroid ËÆ¢ÈòÖÁªü‰∏ÄÊé•ÂÖ•ÔºåÊîØÊåÅÊãºËΩ¶ÂÖ±‰∫´ÔºåÊõ¥È´òÊïàÂàÜÊëäÊàêÊú¨ÔºåÂéüÁîüÂ∑•ÂÖ∑Êó†Áºù‰ΩøÁî®„ÄÇ</p>
                    <p class="meta">
                        <span class="language">JavaScript</span> |
                        <span class="stars">8,713</span>
                        | <span class="today">171 stars today</span>
                    </p>
                    <p class="history">First seen: March 01, 2026 | Consecutive daily streak: 1 day</p>
                    <div class="ai-summary">
                        <h4>Analysis</h4>
                        <p>Claude Relay Service is a self-hosted API gateway that mirrors Anthropic Claude (and other model providers like Gemini, OpenAI/Codex and Droid) so teams can centralize multi-account management, per-user API keys, usage/cost tracking, rate/concurrency limits, and proxy support. Technically it is a Node.js application with Redis for state and caching, offers OAuth-based account onboarding and account-rotation/load-balancing, exposes multiple endpoints (e.g., /api, /antigravity, /openai, /gemini) for client compatibility, and includes a web admin panel, monitoring, connection pooling and deployment scripts (shell installer, Docker Compose) for one‚Äëclick installs. Configuration relies on JWT and a 32‚Äëbyte encryption key for secrets, and the service integrates with tools like Claude Code, Gemini CLI, Codex and Droid via environment variables or local config.</p>
<p>This project is most useful for users or teams in regions with restricted access to Claude/Gemini, privacy‚Äëconscious operators who want to avoid third‚Äëparty mirrors, and groups who want to share subscription costs while keeping auditing and quotas under their control. It‚Äôs popular because it gives predictable performance, transparent cost accounting and easy local integration with popular CLIs and IDE plugins, while being simple to deploy with scripts and Docker. Note it can raise service‚Äëterm and security concerns (the README flags a critical admin auth bypass in older releases and warns about potential TOS violations), so operators should update to patched versions and review provider agreements before using it.</p>
                    </div>
                </div>
            </section>

            <section class="repo" data-seen-before="0">
                <div class="repo-header-row">
                    <h3>9. <a href="https://github.com/NousResearch/hermes-agent" target="_blank" rel="noopener noreferrer">NousResearch/hermes-agent</a> </h3>
                    <button type="button" class="repo-toggle" aria-expanded="true">Hide details</button>
                </div>
                <div class="repo-body">
                    <p class="description">No description</p>
                    <p class="meta">
                        <span class="language">Python</span> |
                        <span class="stars">1,182</span>
                        | <span class="today">182 stars today</span>
                    </p>
                    <p class="history">First seen: March 01, 2026 | Consecutive daily streak: 1 day</p>
                    <div class="ai-summary">
                        <h4>Analysis</h4>
                        <p>Hermes Agent is a fully open-source, self-hosted autonomous AI assistant you install on a machine to act as a persistent personal agent: it connects to your messaging accounts, learns projects, creates and manages skills, runs scheduled jobs, and can execute tasks via tool calls. Key features include model-agnostic inference (Nous Portal, OpenRouter, OpenAI-compatible or custom VLLM/SGLang endpoints), persistent memory/skills/cron, a multi-platform messaging gateway (Telegram, Discord, Slack, WhatsApp), and configurable sandboxed execution backends (SSH, Docker, Modal) for safer command execution. Installation automates Python 3.11, submodule checkout, virtual environment setup, and a global CLI, while runtime configuration and secrets live under ~/.hermes; the agent can run as a systemd service and orchestrates LLM calls, tools, and a terminal/process manager. Under the hood it also integrates with Nous Research tooling (mini-swe-agent, tinker-atropos) and is designed to support batch data generation and RL training workflows for tool-calling models.</p>
<p>This project benefits developers, privacy-conscious users, small teams, and researchers who want a customizable, persistent assistant without vendor lock-in ‚Äî especially those needing message integration, automation, or on-premises data retention. It‚Äôs compelling because it is easy to install, model-agnostic, supports sandboxing for safety, and maintains long-term state for personalization and automation. Hermes is trending amid growing interest in autonomous agents and open-source alternatives to hosted assistants, as it bridges practical production use (gateway, cron, sandboxing) with research-oriented capabilities (data generation, RL tooling). Its combination of local control, multi-provider flexibility, and integration with Nous Research infrastructure makes it attractive for both experimentation and deployment.</p>
                    </div>
                </div>
            </section>

            <section class="repo" data-seen-before="0">
                <div class="repo-header-row">
                    <h3>10. <a href="https://github.com/superset-sh/superset" target="_blank" rel="noopener noreferrer">superset-sh/superset</a> </h3>
                    <button type="button" class="repo-toggle" aria-expanded="true">Hide details</button>
                </div>
                <div class="repo-body">
                    <p class="description">IDE for the AI Agents Era - Run an army of Claude Code, Codex, etc. on your machine</p>
                    <p class="meta">
                        <span class="language">TypeScript</span> |
                        <span class="stars">2,514</span>
                        | <span class="today">181 stars today</span>
                    </p>
                    <p class="history">First seen: March 01, 2026 | Consecutive daily streak: 1 day</p>
                    <div class="ai-summary">
                        <h4>Analysis</h4>
                        <p>Superset is a turbocharged terminal and desktop app for running and managing many CLI-based coding agents (Claude Code, Codex, Gemini, Copilot, etc.) simultaneously on your machine. It provides parallel execution, per-task git worktree isolation, built-in monitoring and notifications, a diff viewer/editor for reviewing agent-made changes, and workspace presets to automate environment setup and teardown. Technically it orchestrates agents as terminal processes tied to isolated git branches and working directories, relies on Bun for runtime, uses Git/GitHub CLI for VCS operations and Caddy as a dev reverse proxy, and exposes configurable setup/teardown scripts via .superset/config.json.</p>
<p>This project benefits individual developers and teams who want to scale AI-assisted coding workflows without losing control of code, enabling rapid parallel experimentation, safe isolation of changes, and quick context switching. It&#x27;s trending because the number and capability of CLI coding agents are rapidly increasing, creating a need for tooling that coordinates multiple agents, preserves reproducibility and local privacy, and integrates with existing editors and CI/VCS workflows.</p>
                    </div>
                </div>
            </section>

            </div>
        </article>
    </main>
    <footer>
        <p>Generated automatically. Data from <a href="https://github.com/trending">GitHub Trending</a>.</p>
    </footer>

<script>
(() => {
    const readKey = "gtd:read_days:gh:v1";
    const dayStr = "2026-03-01";

    let stored = [];
    try {
        stored = JSON.parse(localStorage.getItem(readKey) || "[]");
        if (!Array.isArray(stored)) {
            stored = [];
        }
    } catch (_err) {
        stored = [];
    }

    if (!stored.includes(dayStr)) {
        stored.push(dayStr);
        stored.sort();
        localStorage.setItem(readKey, JSON.stringify(stored));
    }

    const collapseParam = new URLSearchParams(window.location.search).get("collapse_seen");
    const collapseSeen = collapseParam === "0" ? false : true;

    function setCollapsed(repoEl, collapsed) {
        repoEl.classList.toggle("collapsed", collapsed);
        const button = repoEl.querySelector(".repo-toggle");
        if (button) {
            button.textContent = collapsed ? "Show details" : "Hide details";
            button.setAttribute("aria-expanded", String(!collapsed));
        }
    }

    const repos = Array.from(document.querySelectorAll("section.repo[data-seen-before]"));

    repos.forEach((repoEl) => {
        const toggle = repoEl.querySelector(".repo-toggle");
        if (!toggle) {
            return;
        }

        toggle.addEventListener("click", () => {
            setCollapsed(repoEl, !repoEl.classList.contains("collapsed"));
        });
    });

    const collapseBtn = document.getElementById("collapse-seen-btn");
    const expandBtn = document.getElementById("expand-all-btn");

    if (collapseBtn) {
        collapseBtn.addEventListener("click", () => {
            repos.forEach((repoEl) => {
                if (repoEl.dataset.seenBefore === "1") {
                    setCollapsed(repoEl, true);
                }
            });
        });
    }

    if (expandBtn) {
        expandBtn.addEventListener("click", () => {
            repos.forEach((repoEl) => setCollapsed(repoEl, false));
        });
    }

    if (collapseSeen) {
        repos.forEach((repoEl) => {
            if (repoEl.dataset.seenBefore === "1") {
                setCollapsed(repoEl, true);
            }
        });
    }
})();
</script>

</body>
</html>
