<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>GitHub Trending - February 05, 2026</title>
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <header>
        <h1>GitHub Trending Digest - February 05, 2026</h1>
        <nav>
            <a href="../">&larr; Back to Calendar</a>
        </nav>
    </header>
    <main>
        <article>
            <div class="repos">

            <section class="repo">
                <h3>1. <a href="https://github.com/thedotmack/claude-mem" target="_blank">thedotmack/claude-mem</a></h3>
                <p class="description">A Claude Code plugin that automatically captures everything Claude does during your coding sessions, compresses it with AI (using Claude&#x27;s agent-sdk), and injects relevant context back into future sessions.</p>
                <p class="meta">
                    <span class="language">TypeScript</span> |
                    <span class="stars">22,858 stars</span>
                    | <span class="today">2,638 stars today</span>
                </p>
                <div class="ai-summary">
                    <h4>Analysis</h4>
                    <p>Claude-Mem is a Claude Code plugin that automatically captures everything Claude does during coding sessions, compresses observations with Claude&#x27;s agent-sdk, stores them, and injects relevant context back into future sessions. Its main features include persistent memory across sessions, progressive disclosure with a token-efficient 3-layer MCP search workflow (search → timeline → get_observations), a mem-search skill, a real-time web viewer and desktop integration, privacy tags to exclude sensitive content, and citationable observation IDs. Technically it hooks into session lifecycle events, runs a local Worker service (managed by Bun) on port 37777 with a SQLite database plus a Chroma vector store for hybrid semantic+keyword search, and exposes HTTP/search endpoints and hook scripts to capture and summarize tool usage.</p>
<p>Claude-Mem is valuable to individual developers and teams who use Claude Code by preserving project-specific knowledge, reducing repeated context provisioning, and enabling fast semantic search over past debugging, code changes, and design decisions. It speeds onboarding, bug triage, and long-running feature work by surfacing compact, relevant observations and only fetching full details when needed, saving tokens and maintaining performance. With local storage, privacy controls, and plugin-based deployment (plus beta features like Endless Mode), it fits both personal and enterprise workflows and aligns with the current trend toward AI-native developer tooling that maintains continuity across sessions.</p>

                </div>
            </section>

            <section class="repo">
                <h3>2. <a href="https://github.com/openai/skills" target="_blank">openai/skills</a></h3>
                <p class="description">Skills Catalog for Codex</p>
                <p class="meta">
                    <span class="language">Python</span> |
                    <span class="stars">3,841 stars</span>
                    | <span class="today">746 stars today</span>
                </p>
                <div class="ai-summary">
                    <h4>Analysis</h4>
                    <p>This repository catalogs &quot;Agent Skills&quot; — self-contained folders of instructions, scripts, and resources that Codex agents can discover and use to perform specific tasks. Its main features include a directory-based skill format (with .system, .curated, and .experimental locations), an in-agent $skill-installer for adding curated or experimental skills by name or GitHub URL, and per-skill licensing files. Technically, Codex discovers and loads these packaged skills at runtime (with .system skills auto-installed), and newly installed skills are picked up after restarting the agent; the project also references an open standard and documentation for creating and using skills.</p>
<p>The project is valuable for developers, teams, and organizations that want to package, share, and reuse agent capabilities for repeatable automation, workflow orchestration, and rapid prototyping. It lowers friction for distributing agent functionality across projects and contributors by standardizing skill packaging and providing installer tooling, making it especially useful for engineering teams, integrators, and community contributors. Its relevance is growing alongside increased adoption of agent-based automation and the need for modular, shareable LLM-driven tools.</p>

                </div>
            </section>

            <section class="repo">
                <h3>3. <a href="https://github.com/disler/claude-code-hooks-mastery" target="_blank">disler/claude-code-hooks-mastery</a></h3>
                <p class="description">Master Claude Code Hooks</p>
                <p class="meta">
                    <span class="language">Python</span> |
                    <span class="stars">2,439 stars</span>
                    | <span class="today">47 stars today</span>
                </p>
                <div class="ai-summary">
                    <h4>Analysis</h4>
                    <p>This repository is a practical toolkit and demo for mastering Claude Code hooks, implementing all 13 lifecycle events to add deterministic and configurable control over agent behavior. Hook logic lives as UV single-file Python scripts in .claude/hooks with inline dependency declarations, keeping each hook isolated and portable while UV handles fast dependency resolution. Key capabilities include prompt validation and context injection, pre/post tool security and transcript conversion, permission auditing, TTS-driven notifications, subagent orchestration, and automated logging of events as JSON; validators for linting and type checking (Ruff, mypy-style) and team-based builder/validator patterns are also included. The project integrates optional providers (ElevenLabs, OpenAI, Anthropic, Ollama) for TTS and LLM fallbacks and demonstrates session lifecycle management, error handling, and compaction/backups for robust agent workflows.</p>
<p>This toolkit is valuable for engineers and teams building production Claude Code agents who need observability, safety, and repeatable control over LLM-driven tooling—especially where audit logging, permission gating, and automated code-quality validation matter. It’s also useful for researchers and educators wanting a clear reference implementation of hook lifecycles, subagent patterns, and multi-provider TTS/LLM integration without polluting project environments thanks to UV scripts. Because organizations increasingly require governance, reproducibility, and orchestration for generative AI systems, a portable, test-covered hook framework that enforces security and validation around tool use is highly practical and timely.</p>

                </div>
            </section>

            <section class="repo">
                <h3>4. <a href="https://github.com/OpenBMB/ChatDev" target="_blank">OpenBMB/ChatDev</a></h3>
                <p class="description">ChatDev 2.0: Dev All through LLM-powered Multi-Agent Collaboration</p>
                <p class="meta">
                    <span class="language">Python</span> |
                    <span class="stars">30,035 stars</span>
                    | <span class="today">227 stars today</span>
                </p>
                <div class="ai-summary">
                    <h4>Analysis</h4>
                    <p>ChatDev 2.0 (DevAll) is a zero-code multi-agent orchestration platform that enables users to design, configure, and execute customized multi-agent systems via a visual workflow canvas or YAML-based instances. It exposes a web console (Vite + Vue frontend) and a Python SDK for programmatic runs, supports environment/model key configuration, real-time logs, human-in-the-loop feedback, and safe execution options (Docker/Git modes). Technically, it composes LLM-powered agents into node-based workflows and supports advanced orchestration paradigms—chain-topologies from ChatDev 1.0, MacNet DAGs for scalable collaboration, and a puppeteer-style central orchestrator optimized with reinforcement learning—to sequence, activate, and manage agents efficiently. The repository also bundles research implementations and papers (e.g., MacNet, IER, NeurIPS 2025 puppeteer branch) illustrating evolving orchestration and experience refinement techniques.</p>
<p>This project benefits developers, AI researchers, startups, and enterprises seeking to prototype or automate complex tasks (software development, data analysis, visualization, 3D generation) without writing orchestration code. By offering a zero-code interface, reusable YAML workflows, and programmatic SDK access, it reduces engineering overhead and speeds iteration while scaling to many agents and optimizing compute via learned orchestration. Researchers gain a practical testbed and reference implementations for multi-agent collaboration methods, and product teams can integrate with LLM providers through simple API key configuration. Given the surge in interest in multi-agent LLM systems, ChatDev is timely: it operationalizes collaborative agent patterns, supports human-in-the-loop and CI-friendly workflows, and bridges research ideas to deployable tooling.</p>

                </div>
            </section>

            <section class="repo">
                <h3>5. <a href="https://github.com/ankitects/anki" target="_blank">ankitects/anki</a></h3>
                <p class="description">Anki is a smart spaced repetition flashcard program</p>
                <p class="meta">
                    <span class="language">Rust</span> |
                    <span class="stars">26,205 stars</span>
                    | <span class="today">28 stars today</span>
                </p>
                <div class="ai-summary">
                    <h4>Analysis</h4>
                    <p>Anki is the computer-version source for a spaced repetition flashcard program that helps users retain information by scheduling reviews at optimal intervals. The project implements core features such as customizable decks and card types, rich-media support, a configurable scheduling algorithm, and an extensible add‑on system. Technically, the desktop application is primarily implemented in Python with a Qt-based user interface, persists collections in a local database (SQLite), and includes build and packaging infrastructure as well as the scheduling logic that computes review intervals.</p>
<p>This project is valuable for students, medical professionals, language learners, educators, and anyone who needs reliable long‑term retention of factual knowledge because it automates review timing and supports highly customizable study content. Its open-source codebase, cross‑platform desktop builds and integrations with AnkiWeb and mobile clients make it adaptable for curricula, research, or personalized study workflows. Because spaced repetition is evidence‑backed and the app is extensible with a large community, Anki remains a trending tool for efficient, long‑term memorization.</p>

                </div>
            </section>

            <section class="repo">
                <h3>6. <a href="https://github.com/open-telemetry/opentelemetry-collector-contrib" target="_blank">open-telemetry/opentelemetry-collector-contrib</a></h3>
                <p class="description">Contrib repository for the OpenTelemetry Collector</p>
                <p class="meta">
                    <span class="language">Go</span> |
                    <span class="stars">4,364 stars</span>
                    | <span class="today">3 stars today</span>
                </p>
                <div class="ai-summary">
                    <h4>Analysis</h4>
                    <p>OpenTelemetry Collector Contrib is a community-maintained repository of additional components for the OpenTelemetry Collector that do not belong in the core repo, providing a broad set of receivers, processors, exporters and extensions for ingesting, transforming and exporting telemetry (traces, metrics, logs). Its main features include a large ecosystem of integrations (e.g., Jaeger, Prometheus and many vendor-specific exporters), per-signal stability levels, gated features for controlled rollouts, and clear support and contribution policies. Technically, components are implemented as pluggable factories that compose into Collector pipelines and are packaged into official &quot;contrib&quot; distributions or custom builds via the Collector Builder. The project is governed through CODEOWNERS, maintainers/approvers/triagers roles, and a PR/review process to ensure consistency and quality across components.</p>
<p>This repository is valuable for observability engineers, SREs, vendors and platform teams who need flexible, vendor-neutral telemetry pipelines and broad integration coverage beyond the core Collector. It enables organizations to ingest diverse telemetry sources, apply processing/transformation, and export to multiple backends without vendor lock-in, making it ideal for hybrid cloud and multi-tool observability stacks. The active community, clear stability gradations per signal, and ability to assemble custom Collector binaries help teams adopt new integrations quickly and safely. Its growth is driven by the demand for extensible, standard telemetry tooling and the large ecosystem of contributors adding integrations and exporters.</p>

                </div>
            </section>

            <section class="repo">
                <h3>7. <a href="https://github.com/Canner/WrenAI" target="_blank">Canner/WrenAI</a></h3>
                <p class="description">⚡️ GenBI (Generative BI) queries any database in natural language, generates accurate SQL (Text-to-SQL), charts (Text-to-Chart), and AI-powered business intelligence in seconds.</p>
                <p class="meta">
                    <span class="language">TypeScript</span> |
                    <span class="stars">14,014 stars</span>
                    | <span class="today">89 stars today</span>
                </p>
                <div class="ai-summary">
                    <h4>Analysis</h4>
                    <p>Wren AI (GenBI) is an open-source Generative BI tool that lets users query any supported database in natural language and produces precise SQL, visual charts, and AI-written insights in seconds. Key features include Text-to-SQL and Text-to-Chart generation, AI-written summaries and reports (GenBI Insights), a semantic layer (MDL models) that encodes schema/metrics/joins for governance, and an API/embeddable interface with demos (Streamlit) and managed cloud options. Technically it connects to a wide range of data sources (Postgres, Redshift, BigQuery, Snowflake, DuckDB, Trino, etc.), integrates with many LLM providers (OpenAI, Azure, Google Gemini/Vertex, Anthropic, Bedrock, Ollama, Databricks, etc.), and uses the semantic layer plus LLM prompts to constrain and generate accurate SQL and chart specifications. The project emphasizes quick local setup, documentation, and configuration examples for different models and connectors.</p>
<p>This project is valuable for data analysts, BI teams, product managers, and application developers who want to reduce the SQL learning curve and deliver decision-ready context quickly, or embed natural-language analytics into SaaS products. The semantic layer and multi-model support help keep outputs accurate and governed, making it suitable for teams that need consistent metrics and traceability in reporting. It’s trending because it combines the recent advances in LLMs with practical BI workflows—offering fast prototyping via OSS plus the scalability of a managed cloud—and because multi-vendor model support and broad connector coverage lower integration friction for enterprises.</p>

                </div>
            </section>

            <section class="repo">
                <h3>8. <a href="https://github.com/pedramamini/Maestro" target="_blank">pedramamini/Maestro</a></h3>
                <p class="description">Agent Orchestration Command Center</p>
                <p class="meta">
                    <span class="language">TypeScript</span> |
                    <span class="stars">1,704 stars</span>
                    | <span class="today">187 stars today</span>
                </p>
                <div class="ai-summary">
                    <h4>Analysis</h4>
                    <p>Maestro is a cross-platform desktop app for orchestrating fleets of AI coding agents and projects, offering a keyboard-first interface to run, monitor, and automate parallel agent workflows. Key features include Git worktrees for isolated branch-based agents, Auto Run and filesystem playbooks for batch task execution, multi-agent group chat, mobile remote control via a built-in web server, and a CLI for headless operation. Technically it functions as a pass-through to supported AI providers (Claude Code, OpenAI Codex, OpenCode, Factory Droid), spawning fresh or resumed sessions per task, managing conversation context and workspaces, queuing messages, and integrating with git and optional remote tunneling. It also provides session discovery, token/cost tracking, analytics, and a document graph for visualizing project knowledge.</p>
<p>Maestro is aimed at developers, AI researchers, and power users who need to run multiple agentic workflows in parallel, automate repeatable playbooks, or maintain long-running unattended sessions tied to codebases. Teams benefit from isolated worktree agents that produce PR-ready outputs and from embedding agent runs into cron/CI via the CLI, while solo practitioners gain speed from keyboard-driven controls and rapid context switching. Its analytics, cost tracking, and knowledge-graph features support governance and project visibility, and the pass-through model preserves existing provider permissions and tooling—attributes that make it relevant in the growing trend of agent orchestration tools.</p>

                </div>
            </section>

            <section class="repo">
                <h3>9. <a href="https://github.com/nvm-sh/nvm" target="_blank">nvm-sh/nvm</a></h3>
                <p class="description">Node Version Manager - POSIX-compliant bash script to manage multiple active node.js versions</p>
                <p class="meta">
                    <span class="language">Shell</span> |
                    <span class="stars">91,271 stars</span>
                    | <span class="today">35 stars today</span>
                </p>
                <div class="ai-summary">
                    <h4>Analysis</h4>
                    <p>nvm is a lightweight, POSIX-compliant bash script that lets users install, manage, and switch between multiple Node.js versions per shell and per user. It installs to a user directory (default ~/.nvm or XDG-config path), clones the repository or downloads release assets with git/curl/wget, and exposes commands such as install, use, list, and alias by sourcing nvm.sh to manipulate PATH and active node/npm binaries. The installer can edit shell profile files (bash, zsh, ksh, sh) or be configured for non-interactive environments (BASH_ENV) like Docker, supports mirrors and auth headers for binary downloads, and includes features like .nvmrc, default versions, and global package migration. Its implementation as a pure shell script keeps dependencies minimal and makes it portable across Unix, macOS, and WSL.</p>
<p>This project is valuable for developers, CI/CD pipelines, and teams that need reproducible Node environments, per-project versioning, and easy switching between LTS and newer releases. It simplifies testing across Node versions, containerized builds, and collaborator workflows by allowing projects to pin versions via .nvmrc and by providing straightforward Docker/CI integration. Because it is simple to install, actively maintained, shell-agnostic, and works without system-wide privileges, it has broad adoption and remains a trending tool in the Node ecosystem.</p>

                </div>
            </section>

            <section class="repo">
                <h3>10. <a href="https://github.com/microsoft/qlib" target="_blank">microsoft/qlib</a></h3>
                <p class="description">Qlib is an AI-oriented Quant investment platform that aims to use AI tech to empower Quant Research, from exploring ideas to implementing productions. Qlib supports diverse ML modeling paradigms, including supervised learning, market dynamics modeling, and RL, and is now equipped withhttps://github.com/microsoft/RD-Agentto automate R&amp;D process.</p>
                <p class="meta">
                    <span class="language">Python</span> |
                    <span class="stars">36,601 stars</span>
                    | <span class="today">83 stars today</span>
                </p>
                <div class="ai-summary">
                    <h4>Analysis</h4>
                    <p>Qlib is an open‑source, AI-oriented quantitative investment platform that provides an end-to-end machine learning pipeline for alpha discovery, risk modeling, portfolio optimization, backtesting, and order execution. Its modular architecture supports diverse paradigms—supervised learning, market-dynamics modeling (including concept-drift handling), and reinforcement learning—along with many built-in models (e.g., KRNN, Transformer variants, TCN, TabNet) and utilities like point-in-time databases, high-frequency data examples, online serving, and automatic model rolling. Technically it exposes APIs and CLI tools (e.g., qrun), runs on Python 3.8–3.12 with pip/source install options, and integrates data providers and training/serving components so researchers can prototype, validate, and deploy ML-driven trading strategies. The project now also integrates RD-Agent, an LLM-based multi-agent system that automates factor mining and joint model optimization to accelerate R&amp;D workflows.</p>
<p>Qlib is valuable to quantitative researchers, data scientists, academic groups, and asset managers who need a reproducible, scalable platform to prototype, backtest, and productionize ML-driven trading strategies. Typical use cases include automated factor discovery, model selection and tuning, market-dynamics experiments, RL-based trading policy development, and low-cost online model serving for production portfolios. Its traction comes from combining a mature open-source quant stack with recent innovations—LLM-driven RD-Agent, an Auto Quant Factory, and continual additions of SOTA models and datasets—making research-to-production workflows faster, more automated, and easier to reproduce.</p>

                </div>
            </section>

            </div>
        </article>
    </main>
    <footer>
        <p>Generated automatically. Data from <a href="https://github.com/trending">GitHub Trending</a>.</p>
    </footer>
</body>
</html>
