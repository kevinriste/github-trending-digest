<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>GitHub Trending - February 09, 2026</title>
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <header>
        <h1>GitHub Trending Digest - February 09, 2026</h1>
        <nav>
            <a href="../">&larr; GitHub Calendar</a>
            <a href="../hn/">Hacker News</a>
        </nav>
    </header>
    <main>
        <div class="repo-controls">
            <button id="collapse-seen-btn" type="button">Collapse Seen Repos</button>
            <button id="expand-all-btn" type="button">Expand All</button>
        </div>
        <article>
            <div class="repos">

            <section class="repo" data-seen-before="1">
                <div class="repo-header-row">
                    <h3>1. <a href="https://github.com/KeygraphHQ/shannon" target="_blank" rel="noopener noreferrer">KeygraphHQ/shannon</a> <span class="seen-badge">Seen before</span></h3>
                    <button type="button" class="repo-toggle" aria-expanded="true">Hide details</button>
                </div>
                <div class="repo-body">
                    <p class="description">Fully autonomous AI hacker to find actual exploits in your web apps. Shannon has achieved a 96.15% success rate on the hint-free, source-aware XBOW Benchmark.</p>
                    <p class="meta">
                        <span class="language">TypeScript</span> |
                        <span class="stars">14,131 stars</span>
                        | <span class="today">3,479 stars today</span>
                    </p>
                    <p class="history">First seen: February 08, 2026 | Consecutive daily streak: 2 days</p>
                    <div class="ai-summary">
                        <h4>Analysis</h4>
                        <p>Shannon is an autonomous AI pentester that analyzes a target application&#x27;s source code and then autonomously hunts for and executes real-world exploits using a built-in browser and command-line actions to produce reproducible proof-of-concept attacks. Its main features include detection and validation of critical OWASP vulnerabilities (injection, XSS, SSRF, broken auth/authorization), code-aware dynamic testing, parallelized workflows, and integrations with reconnaissance/testing tools such as Nmap, Subfinder, WhatWeb, and Schemathesis. Technically it runs as containerized workflows (Docker), leverages an LLM provider (Anthropic or Claude) to drive decision-making, and exposes monitoring via a workflow UI while emitting pentester-grade reports with copy-and-paste PoCs. This repository contains Shannon Lite (AGPL-3.0) for white-box source-available testing; a commercial Pro edition adds deeper LLM-powered data-flow analysis and enterprise integrations.</p>
<p>Shannon delivers clear value to security teams, DevOps, and independent researchers who need on-demand, reproducible penetration testing to close the gap between rapid shipping and infrequent manual pentests, because it verifies exploitable issues rather than just flagging potential vulnerabilities. It‚Äôs well suited for organizations that can provide source repositories and want continuous testing or CI/CD and compliance automation (e.g., SOC 2/HIPAA evidence collection), and for red teams that need scalable, repeatable attack validation. The project is gaining attention because it combines modern LLM orchestration with established security tooling to accelerate exploit discovery, reduce false positives through validated PoCs, and lower the operational cost of frequent security validation. Small teams can experiment with the AGPL Lite, while enterprises benefit from the Pro edition‚Äôs advanced analysis and support.</p>
                    </div>
                </div>
            </section>

            <section class="repo" data-seen-before="0">
                <div class="repo-header-row">
                    <h3>2. <a href="https://github.com/pydantic/monty" target="_blank" rel="noopener noreferrer">pydantic/monty</a> </h3>
                    <button type="button" class="repo-toggle" aria-expanded="true">Hide details</button>
                </div>
                <div class="repo-body">
                    <p class="description">A minimal, secure Python interpreter written in Rust for use by AI</p>
                    <p class="meta">
                        <span class="language">Rust</span> |
                        <span class="stars">3,093 stars</span>
                        | <span class="today">456 stars today</span>
                    </p>
                    <p class="history">First seen: February 09, 2026 | Consecutive daily streak: 1 day</p>
                    <div class="ai-summary">
                        <h4>Analysis</h4>
                        <p>Monty is a minimal, secure Python interpreter implemented in Rust designed for running LLM-generated code inside agents. It executes a restricted but useful subset of Python, enforces complete isolation of host resources (filesystem, env, network) unless you expose specific external functions, supports async/sync calls, type checking, stdout/stderr capture, and resource limits (memory, stack, time). Technically it embeds a custom Python runtime with no CPython dependency, is serializable (can snapshot parsed code and in-flight execution to bytes), and can be called from Rust, Python, or JavaScript with microsecond-scale startup and comparable runtime performance to CPython. It intentionally omits most of the stdlib and third-party libraries and currently lacks some language features (classes, match) while under active development.</p>
<p>Monty is valuable to teams building AI agents, tool-call frameworks, and code-mode workflows that need low-latency, safe execution of model-written code without the overhead and risk of container sandboxes. Developers who want deterministic, auditable interactions between LLMs and host functions‚Äîplus the ability to cache and resume interpreter state across processes‚Äîwill benefit most. Its momentum stems from practical demand for faster, simpler, and safer in-process code execution for agents (e.g., Pydantic AI, Anthropic-style tool calling, and Cloudflare codemode), filling a gap between unsafe direct execution and heavyweight sandboxing.</p>
                    </div>
                </div>
            </section>

            <section class="repo" data-seen-before="1">
                <div class="repo-header-row">
                    <h3>3. <a href="https://github.com/openai/skills" target="_blank" rel="noopener noreferrer">openai/skills</a> <span class="seen-badge">Seen before</span></h3>
                    <button type="button" class="repo-toggle" aria-expanded="true">Hide details</button>
                </div>
                <div class="repo-body">
                    <p class="description">Skills Catalog for Codex</p>
                    <p class="meta">
                        <span class="language">Python</span> |
                        <span class="stars">7,157 stars</span>
                        | <span class="today">1,425 stars today</span>
                    </p>
                    <p class="history">First seen: February 04, 2026 | Consecutive daily streak: 6 days</p>
                    <div class="ai-summary">
                        <h4>Analysis</h4>
                        <p>This repository catalogs &quot;Agent Skills&quot;‚Äîself-contained folders of instructions, scripts, and resources that Codex-based AI agents can discover and use to perform specific tasks. Its main features include a curated and experimental skills layout, automatic installation of .system skills, and a $skill-installer command that adds skills by name, folder, or GitHub directory URL; each skill carries its own LICENSE.txt and follows the Agent Skills open standard. Technically, skills are packaged as directory-based modules that Codex imports at runtime (requiring a restart to pick up new skills), enabling repeatable, shareable capabilities composed of prompts, scripts, and ancillary assets. The repository therefore acts as both a registry and distribution mechanism for modular agent behaviors.</p>
<p>The project is valuable to developers, product teams, and organizations building agent-driven automation because it centralizes reusable, testable components that speed development and reduce duplication of effort. Use cases include domain-specific automation, standardized prompt and workflow sharing, rapid prototyping of agent capabilities, and collaboration across teams that need consistent agent behavior. It‚Äôs gaining traction because modular, discoverable skills align with the broader trend toward agentization and platformization of AI tooling, making it easier to scale and govern autonomous workflows.</p>
                    </div>
                </div>
            </section>

            <section class="repo" data-seen-before="1">
                <div class="repo-header-row">
                    <h3>4. <a href="https://github.com/virattt/dexter" target="_blank" rel="noopener noreferrer">virattt/dexter</a> <span class="seen-badge">Seen before</span></h3>
                    <button type="button" class="repo-toggle" aria-expanded="true">Hide details</button>
                </div>
                <div class="repo-body">
                    <p class="description">An autonomous agent for deep financial research</p>
                    <p class="meta">
                        <span class="language">TypeScript</span> |
                        <span class="stars">12,882 stars</span>
                        | <span class="today">1,039 stars today</span>
                    </p>
                    <p class="history">First seen: February 04, 2026 | Consecutive daily streak: 1 day</p>
                    <div class="ai-summary">
                        <h4>Analysis</h4>
                        <p>Dexter is an autonomous financial research agent that converts complex finance questions into structured, step-by-step research plans, executes those steps using live market and financial datasets, and iterates via self-validation until it produces a confident, data-backed answer. Key features include intelligent task planning, autonomous tool selection and execution, self-reflection and error checking, and safety mechanisms such as loop detection and step limits. Technically it runs on the Bun runtime, integrates with LLM providers (OpenAI, Anthropic, XAI, OpenRouter, or local Ollama), pulls institutional-grade data via a Financial Datasets API and optional web search (Exa/Tavily), and logs every action to newline-delimited JSON scratchpads for traceability. An evaluation suite using LangSmith and an LLM-as-judge enables benchmarking on a dataset of financial questions.</p>
<p>This project is useful for sell-side and buy-side analysts, quantitative researchers, fintech engineers, and anyone needing reproducible, fast financial due diligence or idea vetting. It automates tedious data collection and initial analysis, provides auditable scratchpad logs and an eval pipeline for continuous improvement, and is extensible to different LLMs and data sources. Its traction comes from combining domain-specific tooling, live data access, and autonomous agent patterns that accelerate high-stakes research workflows while maintaining safety and traceability.</p>
                    </div>
                </div>
            </section>

            <section class="repo" data-seen-before="1">
                <div class="repo-header-row">
                    <h3>5. <a href="https://github.com/microsoft/litebox" target="_blank" rel="noopener noreferrer">microsoft/litebox</a> <span class="seen-badge">Seen before</span></h3>
                    <button type="button" class="repo-toggle" aria-expanded="true">Hide details</button>
                </div>
                <div class="repo-body">
                    <p class="description">A security-focused library OS supporting kernel- and user-mode execution</p>
                    <p class="meta">
                        <span class="language">Rust</span> |
                        <span class="stars">1,516 stars</span>
                        | <span class="today">359 stars today</span>
                    </p>
                    <p class="history">First seen: February 08, 2026 | Consecutive daily streak: 2 days</p>
                    <div class="ai-summary">
                        <h4>Analysis</h4>
                        <p>LiteBox is a sandboxing library OS that minimizes the host interface to reduce attack surface while enabling both kernel- and user-mode execution. It provides a Rust-y, nix/rustix-inspired &quot;North&quot; API that is paired with pluggable &quot;South&quot; Platform backends, allowing a wide variety of North‚ÄìSouth combinations and easy interoperation of shims and platforms. Technically, the project is modular: the North surface exposes POSIX-like primitives to runtimes and applications, and the South adapters implement those primitives against different hosts (Windows, Linux, SEV SNP, OP-TEE, LVBS, etc.), enabling unmodified binaries or constrained workloads to run across environments.</p>
<p>The project is valuable for security-focused system and platform developers, cloud and virtualization engineers, and anyone needing portable sandboxing or compatibility layers (for example, running Linux programs on Windows or isolating Linux workloads). Its pluggable architecture and focus on minimizing host interfaces make it attractive for mitigating attack surface in sensitive deployments, integrating with enclave technologies, and experimenting with alternative runtime models. Growing interest in Rust-based systems, supply-chain security, and hardware-backed isolation helps explain why a flexible library OS like LiteBox is gaining attention.</p>
                    </div>
                </div>
            </section>

            <section class="repo" data-seen-before="0">
                <div class="repo-header-row">
                    <h3>6. <a href="https://github.com/google/langextract" target="_blank" rel="noopener noreferrer">google/langextract</a> </h3>
                    <button type="button" class="repo-toggle" aria-expanded="true">Hide details</button>
                </div>
                <div class="repo-body">
                    <p class="description">A Python library for extracting structured information from unstructured text using LLMs with precise source grounding and interactive visualization.</p>
                    <p class="meta">
                        <span class="language">Python</span> |
                        <span class="stars">24,889 stars</span>
                        | <span class="today">438 stars today</span>
                    </p>
                    <p class="history">First seen: February 09, 2026 | Consecutive daily streak: 1 day</p>
                    <div class="ai-summary">
                        <h4>Analysis</h4>
                        <p>LangExtract is a Python library that uses large language models to extract structured information from unstructured text while precisely grounding each extraction to its original source location. Key features include few‚Äëshot, prompt-driven extraction schemas, controlled generation for robust structured outputs (with support for models like Google Gemini and local LLMs via Ollama), optimized handling of long documents through chunking, parallel/multi‚Äëpass extraction to boost recall, and self‚Äëcontained interactive HTML visualizations alongside JSONL outputs for review. Technically, users provide a prompt and example extractions, the library divides texts into context windows, runs model calls (optionally in parallel or batch), aligns results to source offsets, and assembles validated, schema‚Äëconstrained outputs for downstream use.</p>
<p>This tooling is valuable for teams that need reliable, auditable structure from noisy text‚Äîclinical documentation (e.g., radiology/medication extraction), legal/contract review, research literature mining, content moderation, and annotation pipelines‚Äîbecause it combines scale with traceability and reviewer-friendly visualization. Organizations benefit from faster, consistent data extraction without model fine‚Äëtuning, and the project‚Äôs support for cloud and local providers, batch modes, and emphasis on explainability aligns with current demands for practical, controllable LLM applications, making it a timely and widely applicable solution.</p>
                    </div>
                </div>
            </section>

            <section class="repo" data-seen-before="1">
                <div class="repo-header-row">
                    <h3>7. <a href="https://github.com/obra/superpowers" target="_blank" rel="noopener noreferrer">obra/superpowers</a> <span class="seen-badge">Seen before</span></h3>
                    <button type="button" class="repo-toggle" aria-expanded="true">Hide details</button>
                </div>
                <div class="repo-body">
                    <p class="description">An agentic skills framework &amp; software development methodology that works.</p>
                    <p class="meta">
                        <span class="language">Shell</span> |
                        <span class="stars">47,973 stars</span>
                        | <span class="today">813 stars today</span>
                    </p>
                    <p class="history">First seen: February 04, 2026 | Consecutive daily streak: 2 days</p>
                    <div class="ai-summary">
                        <h4>Analysis</h4>
                        <p>Superpowers is a complete software development workflow for coding agents built around composable &quot;skills&quot; and starter instructions that make agents follow a consistent process. Key features include a modular skills library (TDD, systematic debugging, brainstorming, plan-writing, subagent-driven development, code review, and git worktree management), an enforced workflow that moves from spec elicitation to plan to task-level subagents, and a two-stage review system (spec compliance then code quality). Technically it ships as a plugin (Claude Code marketplace) with manual install paths for Codex and OpenCode, stores skills directly in the repository, and has agents fetch and execute skills, spawn subagents per task, and run automated verification and test-driven cycles. The repo also contains contribution guidance and update mechanics so skills can be extended and updated easily.</p>
<p>This project benefits developers, teams, and tool builders who want repeatable, evidence-driven AI-assisted engineering‚Äîespecially those who need automated testing, clear task plans, and controlled parallel work via subagents. It‚Äôs gaining traction because it addresses practical pain points in agentic coding (avoiding ad-hoc behavior, enforcing RED‚ÄëGREEN‚ÄëREFACTOR, and integrating with git workflows) while offering an easy integration path for Claude Code and extensibility via an open-source MIT-licensed skill library.</p>
                    </div>
                </div>
            </section>

            <section class="repo" data-seen-before="1">
                <div class="repo-header-row">
                    <h3>8. <a href="https://github.com/OpenBMB/MiniCPM-o" target="_blank" rel="noopener noreferrer">OpenBMB/MiniCPM-o</a> <span class="seen-badge">Seen before</span></h3>
                    <button type="button" class="repo-toggle" aria-expanded="true">Hide details</button>
                </div>
                <div class="repo-body">
                    <p class="description">A Gemini 2.5 Flash Level MLLM for Vision, Speech, and Full-Duplex Multimodal Live Streaming on Your Phone</p>
                    <p class="meta">
                        <span class="language">Python</span> |
                        <span class="stars">23,489 stars</span>
                        | <span class="today">212 stars today</span>
                    </p>
                    <p class="history">First seen: February 08, 2026 | Consecutive daily streak: 2 days</p>
                    <div class="ai-summary">
                        <h4>Analysis</h4>
                        <p>MiniCPM-o is an open-source series of on‚Äëdevice multimodal large language models (MLLMs) that take images, video, text, and audio as inputs and produce high‚Äëquality text and speech outputs end-to-end. The flagship MiniCPM-o 4.5 (~9B parameters) and the efficient MiniCPM‚ÄëV 4.0 (~4B) introduce full‚Äëduplex multimodal live streaming‚Äîsimultaneous real‚Äëtime input (video/audio) and output (speech/text)‚Äîalong with bilingual real‚Äëtime voice, voice cloning, OCR, and multilingual understanding. Technically the project provides optimized model weights plus inference tooling and demos (gguf/llama.cpp‚Äëomni, vLLM, Ollama integrations, WebRTC demo) and an official Docker image for local, low‚Äëlatency deployment on phones and Macs. It also supplies alignment and fine‚Äëtuning pipelines (e.g., RLAIF‚ÄëV) and supports quantized and multi‚Äëframework workflows for efficient on‚Äëdevice inference.</p>
<p>The project is useful to developers, researchers, and product teams building privacy‚Äësensitive, low‚Äëlatency multimodal assistants, AR/VR interactions, real‚Äëtime customer service agents, and accessibility tools that require simultaneous vision and speech understanding and generation. Because the models claim parity with high‚Äëend commercial systems while remaining small enough for local deployment, teams can prototype and ship interactive experiences without heavy cloud dependency. The extensive tooling, demos, and cookbook lower the integration and fine‚Äëtuning barrier for the open‚Äësource community. Its combination of strong multimodal capability, efficient inference, and active ecosystem support explains why it has gained traction and trending attention.</p>
                    </div>
                </div>
            </section>

            <section class="repo" data-seen-before="1">
                <div class="repo-header-row">
                    <h3>9. <a href="https://github.com/likec4/likec4" target="_blank" rel="noopener noreferrer">likec4/likec4</a> <span class="seen-badge">Seen before</span></h3>
                    <button type="button" class="repo-toggle" aria-expanded="true">Hide details</button>
                </div>
                <div class="repo-body">
                    <p class="description">Visualize, collaborate, and evolve the software architecture with always actual and live diagrams from your code</p>
                    <p class="meta">
                        <span class="language">TypeScript</span> |
                        <span class="stars">2,437 stars</span>
                        | <span class="today">271 stars today</span>
                    </p>
                    <p class="history">First seen: February 07, 2026 | Consecutive daily streak: 1 day</p>
                    <div class="ai-summary">
                        <h4>Analysis</h4>
                        <p>LikeC4 is a modeling language and toolchain that produces live, always-up-to-date software architecture diagrams from code and model files. Its main features include a flexible DSL inspired by the C4 Model and Structurizr, customizable element types and notation, support for arbitrary nested levels, and a CLI preview workflow (npx likec4 start) with a template repository and deployed example. Technically it parses LikeC4 model sources and generates diagrammatic outputs and web previews, enabling diagrams to be synchronized with the codebase and edited as code.</p>
<p>This project is valuable for software architects, engineering teams, and technical writers who need accurate, versioned architecture documentation for onboarding, design reviews, and continuous integration. It fits organizations that prefer ‚Äúarchitecture as code‚Äù because it reduces documentation drift, supports collaboration and customization to domain-specific notations, and integrates into developer workflows. Its open-source license, templates, tutorial, and community channels (Discord, GitHub Discussions) make adoption and contribution straightforward, which helps explain its growing interest.</p>
                    </div>
                </div>
            </section>

            <section class="repo" data-seen-before="0">
                <div class="repo-header-row">
                    <h3>10. <a href="https://github.com/iOfficeAI/AionUi" target="_blank" rel="noopener noreferrer">iOfficeAI/AionUi</a> </h3>
                    <button type="button" class="repo-toggle" aria-expanded="true">Hide details</button>
                </div>
                <div class="repo-body">
                    <p class="description">Free, local, open-source 24/7 Cowork and OpenClaw for Gemini CLI, Claude Code, Codex, OpenCode, Qwen Code, Goose CLI, Auggie, and more | üåü Star if you like it!</p>
                    <p class="meta">
                        <span class="language">TypeScript</span> |
                        <span class="stars">13,352 stars</span>
                        | <span class="today">335 stars today</span>
                    </p>
                    <p class="history">First seen: February 09, 2026 | Consecutive daily streak: 1 day</p>
                    <div class="ai-summary">
                        <h4>Analysis</h4>
                        <p>AionUi is a free, open-source multi-agent desktop and WebUI that provides a unified graphical interface for command-line AI tools (with a built-in Gemini CLI out of the box) and auto-detects and integrates local CLIs such as Claude Code, Codex, Qwen Code, Goose AI, OpenClaw and others. Its main features include multi-session/local storage with independent conversation contexts, scheduled task automation, smart file management (batch rename, classification, merging), real-time previews for 9+ file formats, AI image generation/editing, and an extensible assistants/skills ecosystem (pptx, pdf, mermaid, etc.). Technically it runs as a cross-platform (macOS/Windows/Linux) WebUI and desktop app that orchestrates local and remote models via direct CLI integration or model gateways (NewAPI), supports local model runtimes like Ollama/LM Studio, and exposes remote access through WebUI and chat platforms (Telegram, Feishu/Lark) while keeping data stored locally.</p>
<p>AionUi is valuable for developers, data analysts, knowledge workers, and teams who rely on multiple command-line AI tools but need persistent sessions, file-centric automation, and an easier GUI-driven workflow; it streamlines office automation tasks like scheduled reports, batch file processing, and document generation. Its cross-platform, multi-model support and local-data focus make it a compelling, cost-free alternative to vendor-locked tools (e.g., Claude Cowork), and its extensible assistants and remote access options explain its traction among users looking to run 24/7 AI assistants and integrate AI into everyday workflows.</p>
                    </div>
                </div>
            </section>

            </div>
        </article>
    </main>
    <footer>
        <p>Generated automatically. Data from <a href="https://github.com/trending">GitHub Trending</a>.</p>
    </footer>

<script>
(() => {
    const readKey = "gtd:read_days:gh:v1";
    const dayStr = "2026-02-09";

    let stored = [];
    try {
        stored = JSON.parse(localStorage.getItem(readKey) || "[]");
        if (!Array.isArray(stored)) {
            stored = [];
        }
    } catch (_err) {
        stored = [];
    }

    if (!stored.includes(dayStr)) {
        stored.push(dayStr);
        stored.sort();
        localStorage.setItem(readKey, JSON.stringify(stored));
    }

    const collapseParam = new URLSearchParams(window.location.search).get("collapse_seen");
    const collapseSeen = collapseParam === "0" ? false : true;

    function setCollapsed(repoEl, collapsed) {
        repoEl.classList.toggle("collapsed", collapsed);
        const button = repoEl.querySelector(".repo-toggle");
        if (button) {
            button.textContent = collapsed ? "Show details" : "Hide details";
            button.setAttribute("aria-expanded", String(!collapsed));
        }
    }

    const repos = Array.from(document.querySelectorAll("section.repo[data-seen-before]"));

    repos.forEach((repoEl) => {
        const toggle = repoEl.querySelector(".repo-toggle");
        if (!toggle) {
            return;
        }

        toggle.addEventListener("click", () => {
            setCollapsed(repoEl, !repoEl.classList.contains("collapsed"));
        });
    });

    const collapseBtn = document.getElementById("collapse-seen-btn");
    const expandBtn = document.getElementById("expand-all-btn");

    if (collapseBtn) {
        collapseBtn.addEventListener("click", () => {
            repos.forEach((repoEl) => {
                if (repoEl.dataset.seenBefore === "1") {
                    setCollapsed(repoEl, true);
                }
            });
        });
    }

    if (expandBtn) {
        expandBtn.addEventListener("click", () => {
            repos.forEach((repoEl) => setCollapsed(repoEl, false));
        });
    }

    if (collapseSeen) {
        repos.forEach((repoEl) => {
            if (repoEl.dataset.seenBefore === "1") {
                setCollapsed(repoEl, true);
            }
        });
    }
})();
</script>

</body>
</html>
