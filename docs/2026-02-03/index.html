<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>GitHub Trending - February 03, 2026</title>
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <header>
        <h1>GitHub Trending Digest - February 03, 2026</h1>
        <nav>
            <a href="../">&larr; Back to Calendar</a>
        </nav>
    </header>
    <main>
        <article>
            <div class="repos">

            <section class="repo">
                <h3>1. <a href="https://github.com/thedotmack/claude-mem" target="_blank">thedotmack/claude-mem</a></h3>
                <p class="description">A Claude Code plugin that automatically captures everything Claude does during your coding sessions, compresses it with AI (using Claude&#x27;s agent-sdk), and injects relevant context back into future sessions.</p>
                <p class="meta">
                    <span class="language">TypeScript</span> |
                    <span class="stars">18,431 stars</span>
                    | <span class="today">1,474 stars today</span>
                </p>
                <div class="ai-summary">
                    <h4>Analysis</h4>
                    <p>Claude-Mem is a Claude Code plugin that automatically captures everything Claude does during coding sessions, compresses those observations with Claudeâ€™s agent-sdk, and injects relevant semantic context into future sessions to preserve continuity. Key features include persistent memory across sessions, progressive disclosure for token-efficient context priming, a mem-search skill for natural-language project queries, a web viewer and HTTP worker on port 37777, and privacy controls to exclude sensitive content. Technically it runs lifecycle hook scripts around session events, stores observations and summaries in SQLite with hybrid search backed by a Chroma vector store, and exposes MCP search tools (search, timeline, get_observations) so Claude can retrieve indexed contexts before fetching full details. The system is managed by a Bun worker, configurable via ~/.claude-mem/settings.json, and includes beta features like an Endless Mode for extended-session memory architectures.</p>
<p>This project is valuable for developers and teams who need continuity across intermittent coding sessions, as it reduces repetitive context-building, accelerates debugging and feature work, and preserves institutional memory for long-lived projects. Individual engineers, distributed teams, and those building on Claude/agent workflows benefit from token-cost-aware retrieval and searchable historical observations, making it easier to resume work or audit past decisions. Its integration as a Claude Code plugin and use of agent-sdk, vector search, and a lightweight web UI align with current trends in AI-assisted development and tool-augmented programming, which is why similar memory and context-augmentation projects are gaining traction.</p>

                </div>
            </section>

            <section class="repo">
                <h3>2. <a href="https://github.com/ThePrimeagen/99" target="_blank">ThePrimeagen/99</a></h3>
                <p class="description">Neovim AI agent done right</p>
                <p class="meta">
                    <span class="language">Lua</span> |
                    <span class="stars">3,074 stars</span>
                    | <span class="today">300 stars today</span>
                </p>
                <div class="ai-summary">
                    <h4>Analysis</h4>
                    <p>This repository implements a Neovim-native AI agent designed to provide constrained, rule-driven code assistance inside the editor. Itâ€™s a Lua plugin that integrates with the opencode backend and uses Neovim APIs (keymaps, virtual text, cwd heuristics) plus cmp for completion (skills prefixed with @) to offer actions like fill-in-function, visual-selection edits, and custom rule inclusion via SKILL.md/AGENT.md files. The plugin exposes a small API, configurable logging, and project-level custom rules while currently focusing on TypeScript and Lua support and planning tighter Treesitter/LSP context integration to reduce sent payloads and improve replacements.</p>
<p>This tool is valuable for power users who want AI assistance tightly integrated into a Neovim workflowâ€”especially developers working in Lua/TypeScript who prefer in-editor, repeatable transformations, quick debugging scaffolds, and project-specific behavior via rule files. It benefits maintainers and advanced editors who want constrained, auditable AI outputs rather than open-ended chat, and it&#x27;s well-suited for streamers or teams discussing live workflows (hence the Twitch deep-dive). Because it packages model calls, completion, and keybindings into a single extensible plugin and is promoted by a prominent Neovim influencer, it has traction despite being alpha and having known usability caveats.</p>

                </div>
            </section>

            <section class="repo">
                <h3>3. <a href="https://github.com/termux/termux-app" target="_blank">termux/termux-app</a></h3>
                <p class="description">Termux - a terminal emulator application for Android OS extendible by variety of packages.</p>
                <p class="meta">
                    <span class="language">Java</span> |
                    <span class="stars">50,001 stars</span>
                    | <span class="today">80 stars today</span>
                </p>
                <div class="ai-summary">
                    <h4>Analysis</h4>
                    <p>Termux is an Android terminal emulator and minimal Linux environment; this repository provides the app itself (user interface and terminal emulation) while the installable packages live in the separate termux-packages repo. Key features include an extensible package ecosystem, bootstrap zips for the initial userland, and several optional plugins (Termux:API, Boot, Float, Styling, Tasker, Widget) that expose device capabilities and automation. The project distributes builds via Fâ€‘Droid, GitHub releases/workflows and an experimental Google Play branch, with universal and architecture-specific APKs; technically it runs as a normal Android app with its own filesystem/runtime, requires matching APK signatures for plugins (sharedUserId com.termux), and has known Android 12+ process-killing limitations and special signing considerations for GitHub debug builds.</p>
<p>Termux is useful for developers, system administrators, security researchers and power users who need a portable, scriptable Linux shell and package manager on Androidâ€”enabling SSH, compilers, editors, automation and hardware/API access through plugins. Its open-source nature and rich package ecosystem make it ideal for mobile development, on-device testing, education, and lightweight embedded/IoT workflows where a full desktop is impractical. Recent attention is driven by Android platform changes (process management and Play Store policy constraints) and the community-driven distribution/signing model, which together highlight Termux as a focal project for bringing Unix tooling to mobile devices.</p>

                </div>
            </section>

            <section class="repo">
                <h3>4. <a href="https://github.com/pedramamini/Maestro" target="_blank">pedramamini/Maestro</a></h3>
                <p class="description">Agent Orchestration Command Center</p>
                <p class="meta">
                    <span class="language">TypeScript</span> |
                    <span class="stars">1,346 stars</span>
                    | <span class="today">336 stars today</span>
                </p>
                <div class="ai-summary">
                    <h4>Analysis</h4>
                    <p>Maestro is a cross-platform desktop app for orchestrating fleets of AI agents and managing parallel projects with a keyboard-first UI. Key features include Git worktrees for isolated per-agent branches, Auto Run and Playbooks for file-system-driven task batching, multi-agent group chat with a moderator agent, per-task clean AI sessions, a CLI for headless operation, a built-in web server for mobile remote control and tunneling, analytics, and a document graph for knowledge visualization. Technically it runs as a Node/npm-based desktop application (developer workflow uses npm install / npm run dev), integrates with multiple agent providers (Claude Code, OpenAI Codex, OpenCode, Factory Droid), spins agents in isolated worktrees and directories for clean context, and exposes programmatic interfaces (CLI, web) for automation and CI usage. It also includes UX features like draft autosave, keyboard shortcuts, cost tracking, and extensible slash commands to fit power-user workflows.</p>
<p>Maestro is valuable for developers, AI researchers, and engineering teams who need to run many agent-driven tasks in parallel, orchestrate automated playbooks, or maintain long-running unattended AI sessions without context bleed. It benefits users who want integrated git workflows, repeatable automation (cron/CI), cost and usage analytics, and a compact keyboard-centric interface for high-velocity iteration. Solo hackers juggling multiple projects and teams coordinating agent-assisted design or code reviews will find the parallel worktree model and group-chat moderator especially useful. The project is timely because the rise of agentic coding tools and multi-agent orchestration makes centralized, automated, and auditable agent management an increasingly practical productivity multiplier.</p>

                </div>
            </section>

            <section class="repo">
                <h3>5. <a href="https://github.com/netbirdio/netbird" target="_blank">netbirdio/netbird</a></h3>
                <p class="description">Connect your devices into a secure WireGuardÂ®-based overlay network with SSO, MFA and granular access controls.</p>
                <p class="meta">
                    <span class="language">Go</span> |
                    <span class="stars">22,035 stars</span>
                    | <span class="today">347 stars today</span>
                </p>
                <div class="ai-summary">
                    <h4>Analysis</h4>
                    <p>NetBird is an open-source platform that builds a WireGuard-based encrypted overlay network with centralized access control, SSO/MFA support, and an admin web UI for managing peers and policies. It provides peer-to-peer connectivity with automatic discovery and configuration, route and DNS management, fallback relay (TURN) for strict NATs, and additional features like device posture checks and Rosenpass-based quantum-resistant keys. Technically, each machine runs a NetBird agent that manages a WireGuard interface and connects to a Management Service to obtain network state; peer-to-peer candidate discovery and NAT traversal use WebRTC ICE (pion/ice) with STUN and signaling, and Coturn is commonly used for TURN/STUN relay functionality. The project supports self-hosting via Docker, various OS platforms (Linux, macOS, Windows, mobile, OpenWRT), and exposes automation hooks such as a public API and a Terraform provider.</p>
<p>NetBird is valuable for organizations and individuals who need secure, easy-to-manage remote access across distributed infrastructureâ€”teams migrating to hybrid cloud, remote workforces, IoT deployments, and home labs can all benefit from its zeroâ€‘configuration peer networking and centralized policy model. Its SSO/MFA and group-based access controls make it appealing for security-conscious enterprises, while being open source and self-hostable attracts operators who need auditability, privacy, or on-prem control. Because it eliminates complex firewall/VPN gateway setups, integrates with identity providers, and provides modern NAT traversal and automation tooling, it aligns with current trends toward software-defined networking and developer-friendly infrastructure.</p>

                </div>
            </section>

            <section class="repo">
                <h3>6. <a href="https://github.com/OpenBMB/ChatDev" target="_blank">OpenBMB/ChatDev</a></h3>
                <p class="description">ChatDev 2.0: Dev All through LLM-powered Multi-Agent Collaboration</p>
                <p class="meta">
                    <span class="language">Python</span> |
                    <span class="stars">29,531 stars</span>
                    | <span class="today">93 stars today</span>
                </p>
                <div class="ai-summary">
                    <h4>Analysis</h4>
                    <p>ChatDev 2.0 (DevAll) is a zero-code multi-agent orchestration platform that enables users to design, configure, and run custom LLM-powered agent systems via a visual workflow canvas or a Python SDK. Technically it composes specialized conversational agents (roles/nodes) into directed workflows (YAML/DAGs) and executes them on a Python backend with a Vite + Vue3 frontend, using environment-configured LLM APIs; advanced branches incorporate MacNet for DAG-based collaboration and a puppeteer-style learned orchestrator trained with reinforcement learning to dynamically activate and sequence agents. The system supports real-time logs, human-in-the-loop feedback, incremental development, Git/Docker execution modes, and artifact management, with configuration driven by YAML and .env variables. It is extensible for complex scenarios such as code generation, data visualization, 3D content, and research workflows.</p>
<p>This project is valuable for developers, ML/NLP researchers, product teams, and non-technical users who need to prototype or automate multi-step LLM workflows without coding, accelerating tasks like end-to-end software development, data analysis, content generation, and experimentation. Its zero-code visual orchestration, SDK integration, and scalable multimodal agent topologies lower the barrier to assemble complex agent pipelines while enabling reproducible automation and human oversight. The inclusion of research-backed advances (IER, MacNet, puppeteer RL) and peer-reviewed results (NeurIPS paper) make it attractive for teams exploring efficient, cost-aware multi-agent collaboration at scale.</p>

                </div>
            </section>

            <section class="repo">
                <h3>7. <a href="https://github.com/autobrr/qui" target="_blank">autobrr/qui</a></h3>
                <p class="description">A fast, single-binary qBittorrent web UI: manage multiple instances, automate torrent workflows, and cross-seed across trackers.</p>
                <p class="meta">
                    <span class="language">Go</span> |
                    <span class="stars">2,926 stars</span>
                    | <span class="today">77 stars today</span>
                </p>
                <div class="ai-summary">
                    <h4>Analysis</h4>
                    <p>qui is a lightweight, single-binary web interface for managing one or many qBittorrent instances from a single application. It provides a fast, modern UI with features like multi-instance management, rule-based automations, cross-seeding across trackers, scheduled backups and restores, and a transparent reverse-proxy for external apps. Technically it ships as a standalone executable or Docker image that serves a web UI (default port 7476) and communicates with remote qBittorrent instances via their APIs, optimizing for large torrent collections and low-resource deployment. The app emphasizes performance and automation to simplify torrent workflows and cross-tracker seeding.</p>
<p>This project is valuable for home labbers, seedbox operators, media collectors, and administrators who run multiple qBittorrent instances and need centralized control, automated workflows, and easy cross-seeding. It streamlines tasks like matching and adding torrents across trackers, enforcing rules, and maintaining backups, reducing manual effort and configuration drift. Its single-binary distribution, Docker support, and focus on responsiveness make it attractive for users who want a low-friction, high-performance management layer, which helps explain its growing popularity.</p>

                </div>
            </section>

            <section class="repo">
                <h3>8. <a href="https://github.com/badlogic/pi-mono" target="_blank">badlogic/pi-mono</a></h3>
                <p class="description">AI agent toolkit: coding agent CLI, unified LLM API, TUI &amp; web UI libraries, Slack bot, vLLM pods</p>
                <p class="meta">
                    <span class="language">TypeScript</span> |
                    <span class="stars">5,754 stars</span>
                    | <span class="today">878 stars today</span>
                </p>
                <div class="ai-summary">
                    <h4>Analysis</h4>
                    <p>This monorepo packages a suite of TypeScript/Node tools for building AI agents and managing LLM deployments. It includes a unified multi-provider LLM API (@mariozechner/pi-ai), an agent runtime with tool-calling and state management (@mariozechner/pi-agent-core), an interactive coding agent CLI, a Slack bot that delegates to the coding agent, terminal and web UI libraries (with differential rendering and web components), and a CLI to manage vLLM deployments on GPU pods. Technically the repo is organized as multiple npm packages with adapters for providers like OpenAI, Anthropic, and Google, an orchestrator core that handles tool invocations and state, and separate UI packages for different frontends; standard npm scripts handle build, checks, and tests (LLM tests are skipped without API keys).</p>
<p>This project is useful for developers, platform engineers, and research teams who want to prototype, deploy, and integrate LLM-powered agentsâ€”especially coding assistantsâ€”across CLI, Slack, web, and terminal interfaces. It lowers friction for multi-provider experimentation and for running private or on-prem vLLM deployments by providing unified APIs and pod management tooling. The modular agent runtime and reusable UI components accelerate development and experimentation, and the toolkit is timely given strong demand for composable LLM tooling, multi-vendor support, and easier production deployment paths.</p>

                </div>
            </section>

            <section class="repo">
                <h3>9. <a href="https://github.com/VectifyAI/PageIndex" target="_blank">VectifyAI/PageIndex</a></h3>
                <p class="description">ðŸ“‘ PageIndex: Document Index for Vectorless, Reasoning-based RAG</p>
                <p class="meta">
                    <span class="language">Python</span> |
                    <span class="stars">12,651 stars</span>
                    | <span class="today">793 stars today</span>
                </p>
                <div class="ai-summary">
                    <h4>Analysis</h4>
                    <p>PageIndex is an open-source, vectorless RAG system that builds a hierarchical, table-of-contents style tree index from long documents and uses LLMs to perform reasoning-driven retrieval via tree search rather than vector similarity. It parses PDFs (or markdown and page images), generates node metadata and summaries, and performs agentic in-context searches over the tree to locate the most relevant sections. Core features include no vector DB, no artificial chunking, and explainable, traceable retrieval with page/section references; an optional vision-native pipeline works directly on page images without OCR. The repository provides a CLI (runpageindex.py) with configurable model and node/page settings and can be run locally or integrated via API/MCP for chat-style document analysis.</p>
<p>PageIndex is especially useful for professionals and teams working with long, structured documentsâ€”finance, legal, regulatory, technical manuals, and academic textsâ€”where relevance requires multi-step reasoning rather than surface-level semantic similarity. By simulating human navigation and exposing reasoning paths it improves precision, auditability, and interpretability; the authors report a 98.7% result on the FinanceBench benchmark versus vector-based approaches. It reduces dependency on vector databases and chunking, simplifies pipelines (including OCR-free vision workflows), and is well suited to enterprise, research, and developer workflows that need auditable, high-precision retrieval. The approach is trending because it leverages modern LLM reasoning to address limitations of approximate semantic search for complex, domain-specific documents.</p>

                </div>
            </section>

            <section class="repo">
                <h3>10. <a href="https://github.com/karpathy/nanochat" target="_blank">karpathy/nanochat</a></h3>
                <p class="description">The best ChatGPT that $100 can buy.</p>
                <p class="meta">
                    <span class="language">Python</span> |
                    <span class="stars">41,745 stars</span>
                    | <span class="today">254 stars today</span>
                </p>
                <div class="ai-summary">
                    <h4>Analysis</h4>
                    <p>nanochat is a minimal, hackable end-to-end harness for training, evaluating, fineâ€‘tuning and serving transformer language models locally or on a single multiâ€‘GPU node. It bundles tokenization, dataset utilities, a PyTorch GPT implementation, an engine with KV cache for efficient inference, optimizers (AdamW + Muon), checkpoint management, CORE/coreâ€‘metric evaluation, and a ChatGPTâ€‘like web UI; orchestration is provided via simple scripts (e.g., runs/speedrun.sh) and torchrun for distributed runs. The code is intentionally compact and readable, uses gradient accumulation to emulate multiâ€‘GPU behavior on a single card, logs to wandb, and targets practical reproducibilityâ€”demonstrated by reaching GPTâ€‘2â€“grade capability on an 8Ã—H100 node in a few hours for a small cost. The repo also includes experimental and research utilities (scaling laws, synthetic data generation, miniseries runs) for quick iteration and ablation.</p>
<p>This project is valuable to researchers, ML engineers, hobbyists, and educators who want a lowâ€‘friction platform to experiment with LLM training, scaling studies, fineâ€‘tuning, and promptable chat UIs without the complexity of larger codebases. Use cases include rapid prototyping of model architectures and training recipes, reproducible benchmarking against GPTâ€‘2 CORE metrics, building personalized or specialized chat models, and teaching/learning endâ€‘toâ€‘end LLM pipelines. Its appeal comes from dramatically reduced time and monetary barriers to training capable models, a compact codebase thatâ€™s easy to modify, and an active emphasis on practical leaderboards and community-driven improvements.</p>

                </div>
            </section>

            </div>
        </article>
    </main>
    <footer>
        <p>Generated automatically. Data from <a href="https://github.com/trending">GitHub Trending</a>.</p>
    </footer>
</body>
</html>
