<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>GitHub Trending - February 03, 2026</title>
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <header>
        <h1>GitHub Trending Digest - February 03, 2026</h1>
        <nav>
            <a href="../">&larr; GitHub Calendar</a>
            <a href="../hn/">Hacker News</a>
        </nav>
    </header>
    <main>
        <div class="repo-controls">
            <button id="collapse-seen-btn" type="button">Collapse Seen Repos</button>
            <button id="expand-all-btn" type="button">Expand All</button>
        </div>
        <article>
            <div class="repos">

            <section class="repo" data-seen-before="1">
                <div class="repo-header-row">
                    <h3>1. <a href="https://github.com/thedotmack/claude-mem" target="_blank" rel="noopener noreferrer">thedotmack/claude-mem</a> <span class="seen-badge">Seen before</span></h3>
                    <button type="button" class="repo-toggle" aria-expanded="true">Hide details</button>
                </div>
                <div class="repo-body">
                    <p class="description">A Claude Code plugin that automatically captures everything Claude does during your coding sessions, compresses it with AI (using Claude&#x27;s agent-sdk), and injects relevant context back into future sessions.</p>
                    <p class="meta">
                        <span class="language">TypeScript</span> |
                        <span class="stars">18,431 stars</span>
                        | <span class="today">1,474 stars today</span>
                    </p>
                    <p class="history">First seen: February 02, 2026 | Consecutive daily streak: 2 days</p>
                    <div class="ai-summary">
                        <h4>Analysis</h4>
                        <p>Claude-Mem is a Claude Code plugin that automatically captures Claudeâ€™s tool usage and observations during coding sessions, compresses them with the Claude agent-sdk, stores them in a local database, and injects relevant context into future sessions. Technically it uses lifecycle hook scripts to intercept session events, a Bun-managed worker HTTP service (default port 37777) with a web viewer, SQLite (with FTS5) for persistence, and a Chroma vector DB for hybrid semantic search. Retrieval is token-efficient via MCP tools (search â†’ timeline â†’ get_observations) implementing progressive disclosure, and the system includes configuration, privacy exclusion tags, citationable observation IDs, and a beta channel for experimental features like Endless Mode.</p>
<p>This tool benefits individual developers and teams who need continuity across ephemeral AI sessions by reducing repetitive context copying, accelerating debugging and onboarding, and preserving project knowledge and decision history. Its token-aware progressive retrieval and hybrid search make it cost-efficient for large histories, while local storage and privacy controls suit sensitive codebases. Because it integrates directly into Claude Code and leverages the agent SDK for automated summarization and injection, itâ€™s attractive to users looking for persistent, reproducible AI-assisted coding workflows.</p>
                    </div>
                </div>
            </section>

            <section class="repo" data-seen-before="1">
                <div class="repo-header-row">
                    <h3>2. <a href="https://github.com/ThePrimeagen/99" target="_blank" rel="noopener noreferrer">ThePrimeagen/99</a> <span class="seen-badge">Seen before</span></h3>
                    <button type="button" class="repo-toggle" aria-expanded="true">Hide details</button>
                </div>
                <div class="repo-body">
                    <p class="description">Neovim AI agent done right</p>
                    <p class="meta">
                        <span class="language">Lua</span> |
                        <span class="stars">3,074 stars</span>
                        | <span class="today">300 stars today</span>
                    </p>
                    <p class="history">First seen: February 02, 2026 | Consecutive daily streak: 2 days</p>
                    <div class="ai-summary">
                        <h4>Analysis</h4>
                        <p>This repository implements a Neovim-native AI agent designed to provide constrained, rule-driven code assistance inside the editor. Itâ€™s a Lua plugin that integrates with the opencode backend and uses Neovim APIs (keymaps, virtual text, cwd heuristics) plus cmp for completion (skills prefixed with @) to offer actions like fill-in-function, visual-selection edits, and custom rule inclusion via SKILL.md/AGENT.md files. The plugin exposes a small API, configurable logging, and project-level custom rules while currently focusing on TypeScript and Lua support and planning tighter Treesitter/LSP context integration to reduce sent payloads and improve replacements.</p>
<p>This tool is valuable for power users who want AI assistance tightly integrated into a Neovim workflowâ€”especially developers working in Lua/TypeScript who prefer in-editor, repeatable transformations, quick debugging scaffolds, and project-specific behavior via rule files. It benefits maintainers and advanced editors who want constrained, auditable AI outputs rather than open-ended chat, and it&#x27;s well-suited for streamers or teams discussing live workflows (hence the Twitch deep-dive). Because it packages model calls, completion, and keybindings into a single extensible plugin and is promoted by a prominent Neovim influencer, it has traction despite being alpha and having known usability caveats.</p>
                    </div>
                </div>
            </section>

            <section class="repo" data-seen-before="0">
                <div class="repo-header-row">
                    <h3>3. <a href="https://github.com/termux/termux-app" target="_blank" rel="noopener noreferrer">termux/termux-app</a> </h3>
                    <button type="button" class="repo-toggle" aria-expanded="true">Hide details</button>
                </div>
                <div class="repo-body">
                    <p class="description">Termux - a terminal emulator application for Android OS extendible by variety of packages.</p>
                    <p class="meta">
                        <span class="language">Java</span> |
                        <span class="stars">50,001 stars</span>
                        | <span class="today">80 stars today</span>
                    </p>
                    <p class="history">First seen: February 03, 2026 | Consecutive daily streak: 1 day</p>
                    <div class="ai-summary">
                        <h4>Analysis</h4>
                        <p>Termux is an Android terminal emulator and minimal Linux environment; this repository provides the app itself (user interface and terminal emulation) while the installable packages live in the separate termux-packages repo. Key features include an extensible package ecosystem, bootstrap zips for the initial userland, and several optional plugins (Termux:API, Boot, Float, Styling, Tasker, Widget) that expose device capabilities and automation. The project distributes builds via Fâ€‘Droid, GitHub releases/workflows and an experimental Google Play branch, with universal and architecture-specific APKs; technically it runs as a normal Android app with its own filesystem/runtime, requires matching APK signatures for plugins (sharedUserId com.termux), and has known Android 12+ process-killing limitations and special signing considerations for GitHub debug builds.</p>
<p>Termux is useful for developers, system administrators, security researchers and power users who need a portable, scriptable Linux shell and package manager on Androidâ€”enabling SSH, compilers, editors, automation and hardware/API access through plugins. Its open-source nature and rich package ecosystem make it ideal for mobile development, on-device testing, education, and lightweight embedded/IoT workflows where a full desktop is impractical. Recent attention is driven by Android platform changes (process management and Play Store policy constraints) and the community-driven distribution/signing model, which together highlight Termux as a focal project for bringing Unix tooling to mobile devices.</p>
                    </div>
                </div>
            </section>

            <section class="repo" data-seen-before="1">
                <div class="repo-header-row">
                    <h3>4. <a href="https://github.com/pedramamini/Maestro" target="_blank" rel="noopener noreferrer">pedramamini/Maestro</a> <span class="seen-badge">Seen before</span></h3>
                    <button type="button" class="repo-toggle" aria-expanded="true">Hide details</button>
                </div>
                <div class="repo-body">
                    <p class="description">Agent Orchestration Command Center</p>
                    <p class="meta">
                        <span class="language">TypeScript</span> |
                        <span class="stars">1,346 stars</span>
                        | <span class="today">336 stars today</span>
                    </p>
                    <p class="history">First seen: February 02, 2026 | Consecutive daily streak: 2 days</p>
                    <div class="ai-summary">
                        <h4>Analysis</h4>
                        <p>Maestro is a cross-platform desktop app for orchestrating fleets of AI coding agents and projects, offering a keyboard-first interface to run, monitor, and automate parallel agent workflows. Key features include Git worktrees for isolated branch-based agents, Auto Run and filesystem playbooks for batch task execution, multi-agent group chat, mobile remote control via a built-in web server, and a CLI for headless operation. Technically it functions as a pass-through to supported AI providers (Claude Code, OpenAI Codex, OpenCode, Factory Droid), spawning fresh or resumed sessions per task, managing conversation context and workspaces, queuing messages, and integrating with git and optional remote tunneling. It also provides session discovery, token/cost tracking, analytics, and a document graph for visualizing project knowledge.</p>
<p>Maestro is aimed at developers, AI researchers, and power users who need to run multiple agentic workflows in parallel, automate repeatable playbooks, or maintain long-running unattended sessions tied to codebases. Teams benefit from isolated worktree agents that produce PR-ready outputs and from embedding agent runs into cron/CI via the CLI, while solo practitioners gain speed from keyboard-driven controls and rapid context switching. Its analytics, cost tracking, and knowledge-graph features support governance and project visibility, and the pass-through model preserves existing provider permissions and toolingâ€”attributes that make it relevant in the growing trend of agent orchestration tools.</p>
                    </div>
                </div>
            </section>

            <section class="repo" data-seen-before="0">
                <div class="repo-header-row">
                    <h3>5. <a href="https://github.com/netbirdio/netbird" target="_blank" rel="noopener noreferrer">netbirdio/netbird</a> </h3>
                    <button type="button" class="repo-toggle" aria-expanded="true">Hide details</button>
                </div>
                <div class="repo-body">
                    <p class="description">Connect your devices into a secure WireGuardÂ®-based overlay network with SSO, MFA and granular access controls.</p>
                    <p class="meta">
                        <span class="language">Go</span> |
                        <span class="stars">22,035 stars</span>
                        | <span class="today">347 stars today</span>
                    </p>
                    <p class="history">First seen: February 03, 2026 | Consecutive daily streak: 1 day</p>
                    <div class="ai-summary">
                        <h4>Analysis</h4>
                        <p>NetBird is an open-source platform that builds a WireGuard-based encrypted overlay network with centralized access control, SSO/MFA support, and an admin web UI for managing peers and policies. It provides peer-to-peer connectivity with automatic discovery and configuration, route and DNS management, fallback relay (TURN) for strict NATs, and additional features like device posture checks and Rosenpass-based quantum-resistant keys. Technically, each machine runs a NetBird agent that manages a WireGuard interface and connects to a Management Service to obtain network state; peer-to-peer candidate discovery and NAT traversal use WebRTC ICE (pion/ice) with STUN and signaling, and Coturn is commonly used for TURN/STUN relay functionality. The project supports self-hosting via Docker, various OS platforms (Linux, macOS, Windows, mobile, OpenWRT), and exposes automation hooks such as a public API and a Terraform provider.</p>
<p>NetBird is valuable for organizations and individuals who need secure, easy-to-manage remote access across distributed infrastructureâ€”teams migrating to hybrid cloud, remote workforces, IoT deployments, and home labs can all benefit from its zeroâ€‘configuration peer networking and centralized policy model. Its SSO/MFA and group-based access controls make it appealing for security-conscious enterprises, while being open source and self-hostable attracts operators who need auditability, privacy, or on-prem control. Because it eliminates complex firewall/VPN gateway setups, integrates with identity providers, and provides modern NAT traversal and automation tooling, it aligns with current trends toward software-defined networking and developer-friendly infrastructure.</p>
                    </div>
                </div>
            </section>

            <section class="repo" data-seen-before="0">
                <div class="repo-header-row">
                    <h3>6. <a href="https://github.com/OpenBMB/ChatDev" target="_blank" rel="noopener noreferrer">OpenBMB/ChatDev</a> </h3>
                    <button type="button" class="repo-toggle" aria-expanded="true">Hide details</button>
                </div>
                <div class="repo-body">
                    <p class="description">ChatDev 2.0: Dev All through LLM-powered Multi-Agent Collaboration</p>
                    <p class="meta">
                        <span class="language">Python</span> |
                        <span class="stars">29,531 stars</span>
                        | <span class="today">93 stars today</span>
                    </p>
                    <p class="history">First seen: February 03, 2026 | Consecutive daily streak: 1 day</p>
                    <div class="ai-summary">
                        <h4>Analysis</h4>
                        <p>ChatDev 2.0 (DevAll) is a zero-code multi-agent orchestration platform that enables users to design, configure, and execute customized multi-agent systems via a visual workflow canvas or YAML-based instances. It exposes a web console (Vite + Vue frontend) and a Python SDK for programmatic runs, supports environment/model key configuration, real-time logs, human-in-the-loop feedback, and safe execution options (Docker/Git modes). Technically, it composes LLM-powered agents into node-based workflows and supports advanced orchestration paradigmsâ€”chain-topologies from ChatDev 1.0, MacNet DAGs for scalable collaboration, and a puppeteer-style central orchestrator optimized with reinforcement learningâ€”to sequence, activate, and manage agents efficiently. The repository also bundles research implementations and papers (e.g., MacNet, IER, NeurIPS 2025 puppeteer branch) illustrating evolving orchestration and experience refinement techniques.</p>
<p>This project benefits developers, AI researchers, startups, and enterprises seeking to prototype or automate complex tasks (software development, data analysis, visualization, 3D generation) without writing orchestration code. By offering a zero-code interface, reusable YAML workflows, and programmatic SDK access, it reduces engineering overhead and speeds iteration while scaling to many agents and optimizing compute via learned orchestration. Researchers gain a practical testbed and reference implementations for multi-agent collaboration methods, and product teams can integrate with LLM providers through simple API key configuration. Given the surge in interest in multi-agent LLM systems, ChatDev is timely: it operationalizes collaborative agent patterns, supports human-in-the-loop and CI-friendly workflows, and bridges research ideas to deployable tooling.</p>
                    </div>
                </div>
            </section>

            <section class="repo" data-seen-before="0">
                <div class="repo-header-row">
                    <h3>7. <a href="https://github.com/autobrr/qui" target="_blank" rel="noopener noreferrer">autobrr/qui</a> </h3>
                    <button type="button" class="repo-toggle" aria-expanded="true">Hide details</button>
                </div>
                <div class="repo-body">
                    <p class="description">A fast, single-binary qBittorrent web UI: manage multiple instances, automate torrent workflows, and cross-seed across trackers.</p>
                    <p class="meta">
                        <span class="language">Go</span> |
                        <span class="stars">2,926 stars</span>
                        | <span class="today">77 stars today</span>
                    </p>
                    <p class="history">First seen: February 03, 2026 | Consecutive daily streak: 1 day</p>
                    <div class="ai-summary">
                        <h4>Analysis</h4>
                        <p>qui is a lightweight, single-binary web interface for managing one or many qBittorrent instances from a single application. It provides a fast, modern UI with features like multi-instance management, rule-based automations, cross-seeding across trackers, scheduled backups and restores, and a transparent reverse-proxy for external apps. Technically it ships as a standalone executable or Docker image that serves a web UI (default port 7476) and communicates with remote qBittorrent instances via their APIs, optimizing for large torrent collections and low-resource deployment. The app emphasizes performance and automation to simplify torrent workflows and cross-tracker seeding.</p>
<p>This project is valuable for home labbers, seedbox operators, media collectors, and administrators who run multiple qBittorrent instances and need centralized control, automated workflows, and easy cross-seeding. It streamlines tasks like matching and adding torrents across trackers, enforcing rules, and maintaining backups, reducing manual effort and configuration drift. Its single-binary distribution, Docker support, and focus on responsiveness make it attractive for users who want a low-friction, high-performance management layer, which helps explain its growing popularity.</p>
                    </div>
                </div>
            </section>

            <section class="repo" data-seen-before="1">
                <div class="repo-header-row">
                    <h3>8. <a href="https://github.com/badlogic/pi-mono" target="_blank" rel="noopener noreferrer">badlogic/pi-mono</a> <span class="seen-badge">Seen before</span></h3>
                    <button type="button" class="repo-toggle" aria-expanded="true">Hide details</button>
                </div>
                <div class="repo-body">
                    <p class="description">AI agent toolkit: coding agent CLI, unified LLM API, TUI &amp; web UI libraries, Slack bot, vLLM pods</p>
                    <p class="meta">
                        <span class="language">TypeScript</span> |
                        <span class="stars">5,754 stars</span>
                        | <span class="today">878 stars today</span>
                    </p>
                    <p class="history">First seen: February 02, 2026 | Consecutive daily streak: 2 days</p>
                    <div class="ai-summary">
                        <h4>Analysis</h4>
                        <p>This monorepo packages a suite of TypeScript/Node tools for building AI agents and managing LLM deployments. It includes a unified multi-provider LLM API (@mariozechner/pi-ai), an agent runtime with tool-calling and state management (@mariozechner/pi-agent-core), an interactive coding agent CLI, a Slack bot that delegates to the coding agent, terminal and web UI libraries (with differential rendering and web components), and a CLI to manage vLLM deployments on GPU pods. Technically the repo is organized as multiple npm packages with adapters for providers like OpenAI, Anthropic, and Google, an orchestrator core that handles tool invocations and state, and separate UI packages for different frontends; standard npm scripts handle build, checks, and tests (LLM tests are skipped without API keys).</p>
<p>This project is useful for developers, platform engineers, and research teams who want to prototype, deploy, and integrate LLM-powered agentsâ€”especially coding assistantsâ€”across CLI, Slack, web, and terminal interfaces. It lowers friction for multi-provider experimentation and for running private or on-prem vLLM deployments by providing unified APIs and pod management tooling. The modular agent runtime and reusable UI components accelerate development and experimentation, and the toolkit is timely given strong demand for composable LLM tooling, multi-vendor support, and easier production deployment paths.</p>
                    </div>
                </div>
            </section>

            <section class="repo" data-seen-before="0">
                <div class="repo-header-row">
                    <h3>9. <a href="https://github.com/VectifyAI/PageIndex" target="_blank" rel="noopener noreferrer">VectifyAI/PageIndex</a> </h3>
                    <button type="button" class="repo-toggle" aria-expanded="true">Hide details</button>
                </div>
                <div class="repo-body">
                    <p class="description">ðŸ“‘ PageIndex: Document Index for Vectorless, Reasoning-based RAG</p>
                    <p class="meta">
                        <span class="language">Python</span> |
                        <span class="stars">12,651 stars</span>
                        | <span class="today">793 stars today</span>
                    </p>
                    <p class="history">First seen: February 03, 2026 | Consecutive daily streak: 1 day</p>
                    <div class="ai-summary">
                        <h4>Analysis</h4>
                        <p>PageIndex is an open-source, vectorless RAG system that builds a hierarchical, table-of-contents style tree index from long documents and uses LLMs to perform reasoning-driven retrieval via tree search rather than vector similarity. It parses PDFs (or markdown and page images), generates node metadata and summaries, and performs agentic in-context searches over the tree to locate the most relevant sections. Core features include no vector DB, no artificial chunking, and explainable, traceable retrieval with page/section references; an optional vision-native pipeline works directly on page images without OCR. The repository provides a CLI (runpageindex.py) with configurable model and node/page settings and can be run locally or integrated via API/MCP for chat-style document analysis.</p>
<p>PageIndex is especially useful for professionals and teams working with long, structured documentsâ€”finance, legal, regulatory, technical manuals, and academic textsâ€”where relevance requires multi-step reasoning rather than surface-level semantic similarity. By simulating human navigation and exposing reasoning paths it improves precision, auditability, and interpretability; the authors report a 98.7% result on the FinanceBench benchmark versus vector-based approaches. It reduces dependency on vector databases and chunking, simplifies pipelines (including OCR-free vision workflows), and is well suited to enterprise, research, and developer workflows that need auditable, high-precision retrieval. The approach is trending because it leverages modern LLM reasoning to address limitations of approximate semantic search for complex, domain-specific documents.</p>
                    </div>
                </div>
            </section>

            <section class="repo" data-seen-before="0">
                <div class="repo-header-row">
                    <h3>10. <a href="https://github.com/karpathy/nanochat" target="_blank" rel="noopener noreferrer">karpathy/nanochat</a> </h3>
                    <button type="button" class="repo-toggle" aria-expanded="true">Hide details</button>
                </div>
                <div class="repo-body">
                    <p class="description">The best ChatGPT that $100 can buy.</p>
                    <p class="meta">
                        <span class="language">Python</span> |
                        <span class="stars">41,745 stars</span>
                        | <span class="today">254 stars today</span>
                    </p>
                    <p class="history">First seen: February 03, 2026 | Consecutive daily streak: 1 day</p>
                    <div class="ai-summary">
                        <h4>Analysis</h4>
                        <p>nanochat is a compact, hackable PyTorch harness for training, evaluating, and interacting with transformer LLMs on a single GPU node or small multiâ€‘GPU setups. It bundles tokenization (BPE), data loading, the GPT model implementation, optimizer (AdamW + Muon), distributed training utilities, evaluation (CORE metric, bitsâ€‘perâ€‘byte), KVâ€‘cache inference engine, checkpoint management, and a ChatGPTâ€‘like web UI; the repo includes runnable scripts (e.g., runs/speedrun.sh) that automate endâ€‘toâ€‘end pretraining and finetuning workflows using torchrun, gradient accumulation, and optional wandb logging. The code is intentionally minimal so users can reproduce GPTâ€‘2â€‘grade models quickly (targeting an 8Ã—H100 node in ~3 hours) and experiment with scaling laws, synthetic data, or custom SFT stages. It also provides support for CPU/MPS, smallâ€‘model quick experiments, and a leaderboard/metrics to compare runs.</p>
<p>This project is valuable for researchers, engineers, educators, and hobbyists who want an accessible, reproducible platform to prototype and study LLM training and finetuning without a large infrastructure investment. It enables rapid iteration on model depth, data budgets, and training recipes, making it useful for scalingâ€‘law research, model development, and building custom chatbots or fineâ€‘tuned assistants. nanochat is trending because it dramatically lowers the barrier to entry for training capable LLMs (cost/time reductions), offers a concise codebase thatâ€™s easy to modify, and provides community hooks (Discussions/Discord/leaderboard) for collaborative experimentation.</p>
                    </div>
                </div>
            </section>

            </div>
        </article>
    </main>
    <footer>
        <p>Generated automatically. Data from <a href="https://github.com/trending">GitHub Trending</a>.</p>
    </footer>

<script>
(() => {
    const readKey = "gtd:read_days:gh:v1";
    const dayStr = "2026-02-03";

    let stored = [];
    try {
        stored = JSON.parse(localStorage.getItem(readKey) || "[]");
        if (!Array.isArray(stored)) {
            stored = [];
        }
    } catch (_err) {
        stored = [];
    }

    if (!stored.includes(dayStr)) {
        stored.push(dayStr);
        stored.sort();
        localStorage.setItem(readKey, JSON.stringify(stored));
    }

    const collapseParam = new URLSearchParams(window.location.search).get("collapse_seen");
    const collapseSeen = collapseParam === "0" ? false : true;

    function setCollapsed(repoEl, collapsed) {
        repoEl.classList.toggle("collapsed", collapsed);
        const button = repoEl.querySelector(".repo-toggle");
        if (button) {
            button.textContent = collapsed ? "Show details" : "Hide details";
            button.setAttribute("aria-expanded", String(!collapsed));
        }
    }

    const repos = Array.from(document.querySelectorAll("section.repo[data-seen-before]"));

    repos.forEach((repoEl) => {
        const toggle = repoEl.querySelector(".repo-toggle");
        if (!toggle) {
            return;
        }

        toggle.addEventListener("click", () => {
            setCollapsed(repoEl, !repoEl.classList.contains("collapsed"));
        });
    });

    const collapseBtn = document.getElementById("collapse-seen-btn");
    const expandBtn = document.getElementById("expand-all-btn");

    if (collapseBtn) {
        collapseBtn.addEventListener("click", () => {
            repos.forEach((repoEl) => {
                if (repoEl.dataset.seenBefore === "1") {
                    setCollapsed(repoEl, true);
                }
            });
        });
    }

    if (expandBtn) {
        expandBtn.addEventListener("click", () => {
            repos.forEach((repoEl) => setCollapsed(repoEl, false));
        });
    }

    if (collapseSeen) {
        repos.forEach((repoEl) => {
            if (repoEl.dataset.seenBefore === "1") {
                setCollapsed(repoEl, true);
            }
        });
    }
})();
</script>

</body>
</html>
