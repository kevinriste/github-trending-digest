<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>GitHub Trending - February 28, 2026</title>
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <header>
        <h1>GitHub Trending Digest - February 28, 2026</h1>
        <nav>
            <a href="../">&larr; GitHub Calendar</a>
            <a href="../hn/2026-02-28/">Hacker News</a>
        </nav>
    </header>
    <main>
        <div class="repo-controls">
            <button id="collapse-seen-btn" type="button">Collapse Repos Not New Today</button>
            <button id="expand-all-btn" type="button">Expand All</button>
        </div>
        <p class="seen-help">Repos marked "Not new today" appeared on one or more previous daily pages.</p>
        <article>
            <div class="repos">

            <section class="repo" data-seen-before="1">
                <div class="repo-header-row">
                    <h3>1. <a href="https://github.com/ruvnet/wifi-densepose" target="_blank" rel="noopener noreferrer">ruvnet/wifi-densepose</a> <span class="seen-badge">Not new today</span></h3>
                    <button type="button" class="repo-toggle" aria-expanded="true">Hide details</button>
                </div>
                <div class="repo-body">
                    <p class="description">Production-ready implementation of InvisPose - a revolutionary WiFi-based dense human pose estimation system that enables real-time full-body tracking through walls using commodity mesh routers</p>
                    <p class="meta">
                        <span class="language">Python</span> |
                        <span class="stars">9,835</span>
                        | <span class="today">478 stars today</span>
                    </p>
                    <p class="history">First seen: February 15, 2026 | Consecutive daily streak: 1 day</p>
                    <div class="ai-summary">
                        <h4>Analysis</h4>
                        <p>WiFi-DensePose is a production-ready system that estimates dense human pose and tracks multiple people in real time using WiFi Channel State Information (CSI) instead of cameras. It fuses CSI preprocessing, phase sanitization, and feature extraction with a DensePose neural head and multi-object tracking, and exposes REST and WebSocket APIs for live streaming and control. The project includes a high-performance Rust port with validated microbenchmarks and full test coverage, and supports deployment via pip, Docker, or building from source for low-latency, resource-efficient operation.</p>
<p>This technology delivers privacy-preserving, through-wall sensing suited to healthcare (fall and vital-sign monitoring), fitness, smart homes, security, and disaster response (the WiFi‚ÄëMat module for survivor detection and triage). Enterprise features‚Äîauthentication, rate limiting, monitoring‚Äîand WASM support in the Rust implementation make it attractive to commercial integrators and IoT vendors. Researchers, emergency responders, and product teams benefit from camera-less, hardware-agnostic sensing on commodity routers, and the combination of strong performance, test coverage, and practical APIs explains its growing interest.</p>
                    </div>
                </div>
            </section>

            <section class="repo" data-seen-before="1">
                <div class="repo-header-row">
                    <h3>2. <a href="https://github.com/bytedance/deer-flow" target="_blank" rel="noopener noreferrer">bytedance/deer-flow</a> <span class="seen-badge">Not new today</span></h3>
                    <button type="button" class="repo-toggle" aria-expanded="true">Hide details</button>
                </div>
                <div class="repo-body">
                    <p class="description">An open-source SuperAgent harness that researches, codes, and creates. With the help of sandboxes, memories, tools, skills and subagents, it handles different levels of tasks that could take minutes to hours.</p>
                    <p class="meta">
                        <span class="language">TypeScript</span> |
                        <span class="stars">22,070</span>
                        | <span class="today">696 stars today</span>
                    </p>
                    <p class="history">First seen: February 26, 2026 | Consecutive daily streak: 3 days</p>
                    <div class="ai-summary">
                        <h4>Analysis</h4>
                        <p>DeerFlow is an open-source &quot;super agent&quot; harness that orchestrates sub-agents, persistent memory, sandboxed execution, and extensible skills to carry out complex, multi-step tasks. Technically it is a batteries-included runtime built on LangGraph and LangChain that runs tasks inside isolated sandboxes (local, Docker, or Kubernetes), provides a filesystem and toolset (web fetch, bash, file ops), and dynamically loads Markdown-defined skills and tools while managing context via summarization and long-term memory. The lead agent can spawn parallel scoped sub-agents with their own contexts and termination rules, collect structured results, and synthesize outputs such as reports, slide decks, or web pages. Configuration is driven by config.yaml and environment keys, with Docker and local development workflows supported for fast provisioning and reproducibility.</p>
<p>DeerFlow is useful for AI researchers, data scientists, engineering teams, and content creators who need agents that do real work‚Äînot just chat‚Äîbecause it combines execution, tooling, and memory in an auditable, extensible framework. It‚Äôs trending because it addresses practical limitations of single-pass LLM interactions (execution isolation, token management, long-running tasks, and parallel decomposition), ships with ready-made skills while remaining highly customizable, and benefits from an active rewrite and community-focused tooling that make it easy to experiment with modern LLMs and production workflows.</p>
                    </div>
                </div>
            </section>

            <section class="repo" data-seen-before="1">
                <div class="repo-header-row">
                    <h3>3. <a href="https://github.com/moonshine-ai/moonshine" target="_blank" rel="noopener noreferrer">moonshine-ai/moonshine</a> <span class="seen-badge">Not new today</span></h3>
                    <button type="button" class="repo-toggle" aria-expanded="true">Hide details</button>
                </div>
                <div class="repo-body">
                    <p class="description">Fast and accurate automatic speech recognition (ASR) for edge devices</p>
                    <p class="meta">
                        <span class="language">C</span> |
                        <span class="stars">5,949</span>
                        | <span class="today">593 stars today</span>
                    </p>
                    <p class="history">First seen: February 16, 2026 | Consecutive daily streak: 2 days</p>
                    <div class="ai-summary">
                        <h4>Analysis</h4>
                        <p>Moonshine Voice is an open-source AI toolkit for building real-time, on-device automatic speech recognition and voice interfaces. Its main features include low-latency streaming models optimized for live transcription, speaker diarization, and intent/command recognition, with models ranging from tiny (‚âà26 MB) to larger, production-grade sizes trained from scratch and supporting multiple languages. Technically it avoids Whisper‚Äôs fixed 30-second window and redundant reprocessing by using streaming architectures and caching so computation is done incrementally while the user is still talking, yielding much lower latency and smaller parameter counts. Cross-platform libraries and examples (Python, C++/cmake, iOS, Android, macOS, Windows, Raspberry Pi) make deployment on a wide range of edge devices straightforward.</p>
<p>This project is valuable for developers and product teams who need responsive, private, offline voice interfaces on mobile, IoT, wearables, and other constrained hardware. It enables use cases such as hands-free controls, real-time transcription, command recognition, and accessibility features where latency, privacy, and power/size constraints are critical. Moonshine is gaining traction because of growing demand for on-device AI, the limitations of batch-oriented models like Whisper for streaming applications, and the appeal of a unified, optimized toolkit for heterogeneous edge platforms.</p>
                    </div>
                </div>
            </section>

            <section class="repo" data-seen-before="1">
                <div class="repo-header-row">
                    <h3>4. <a href="https://github.com/muratcankoylan/Agent-Skills-for-Context-Engineering" target="_blank" rel="noopener noreferrer">muratcankoylan/Agent-Skills-for-Context-Engineering</a> <span class="seen-badge">Not new today</span></h3>
                    <button type="button" class="repo-toggle" aria-expanded="true">Hide details</button>
                </div>
                <div class="repo-body">
                    <p class="description">A comprehensive collection of Agent Skills for context engineering, multi-agent architectures, and production agent systems. Use when building, optimizing, or debugging agent systems that require effective context management.</p>
                    <p class="meta">
                        <span class="language">Python</span> |
                        <span class="stars">12,545</span>
                        | <span class="today">803 stars today</span>
                    </p>
                    <p class="history">First seen: February 24, 2026 | Consecutive daily streak: 5 days</p>
                    <div class="ai-summary">
                        <h4>Analysis</h4>
                        <p>This repository is a curated collection of &quot;Agent Skills&quot; for context engineering that codifies practical patterns, tools, and examples to manage the information fed into LLM-based agents. Key features include foundational material on context mechanics and degradation, architectural modules for multi-agent patterns, memory systems, tool and filesystem integration, operational modules for compression/optimization and evaluation, and cognitive BDI mental-state transformations. Technically, skills are packaged as platform-agnostic modules (Claude Code marketplace plugins, Cursor/Codex/IDE rules or standalone skill files), use progressive disclosure so full skill content loads only when activated, and ship Python-pseudocode, triggers, and installation commands that let agent runtimes discover and apply relevant skills at runtime. The repo also documents attention-aware tactics (compaction, masking, caching), LLM-as-judge evaluation methods, and hosted-agent designs for sandboxed background execution.</p>
<p>This project benefits AI engineers, multi-agent architects, ops teams, and researchers who need repeatable, production-grade patterns for conserving model attention and debugging context failures. It accelerates development by turning abstract attention mechanics into actionable skills, templates, and plugin workflows that integrate with existing agent platforms. The collection is timely and trending because managing limited context windows is a growing practical bottleneck as agentic systems scale, and because the work has received academic recognition for helping bridge manual skill engineering and dynamic evolution of agent skills.</p>
                    </div>
                </div>
            </section>

            <section class="repo" data-seen-before="1">
                <div class="repo-header-row">
                    <h3>5. <a href="https://github.com/obra/superpowers" target="_blank" rel="noopener noreferrer">obra/superpowers</a> <span class="seen-badge">Not new today</span></h3>
                    <button type="button" class="repo-toggle" aria-expanded="true">Hide details</button>
                </div>
                <div class="repo-body">
                    <p class="description">An agentic skills framework &amp; software development methodology that works.</p>
                    <p class="meta">
                        <span class="language">Shell</span> |
                        <span class="stars">65,074</span>
                        | <span class="today">1,546 stars today</span>
                    </p>
                    <p class="history">First seen: February 04, 2026 | Consecutive daily streak: 4 days</p>
                    <div class="ai-summary">
                        <h4>Analysis</h4>
                        <p>Superpowers is a complete software development workflow for coding agents built around composable &quot;skills&quot; and starter instructions that make agents follow a consistent process. Key features include a modular skills library (TDD, systematic debugging, brainstorming, plan-writing, subagent-driven development, code review, and git worktree management), an enforced workflow that moves from spec elicitation to plan to task-level subagents, and a two-stage review system (spec compliance then code quality). Technically it ships as a plugin (Claude Code marketplace) with manual install paths for Codex and OpenCode, stores skills directly in the repository, and has agents fetch and execute skills, spawn subagents per task, and run automated verification and test-driven cycles. The repo also contains contribution guidance and update mechanics so skills can be extended and updated easily.</p>
<p>This project benefits developers, teams, and tool builders who want repeatable, evidence-driven AI-assisted engineering‚Äîespecially those who need automated testing, clear task plans, and controlled parallel work via subagents. It‚Äôs gaining traction because it addresses practical pain points in agentic coding (avoiding ad-hoc behavior, enforcing RED‚ÄëGREEN‚ÄëREFACTOR, and integrating with git workflows) while offering an easy integration path for Claude Code and extensibility via an open-source MIT-licensed skill library.</p>
                    </div>
                </div>
            </section>

            <section class="repo" data-seen-before="0">
                <div class="repo-header-row">
                    <h3>6. <a href="https://github.com/ruvnet/ruflo" target="_blank" rel="noopener noreferrer">ruvnet/ruflo</a> </h3>
                    <button type="button" class="repo-toggle" aria-expanded="true">Hide details</button>
                </div>
                <div class="repo-body">
                    <p class="description">üåä The leading agent orchestration platform for Claude. Deploy intelligent multi-agent swarms, coordinate autonomous workflows, and build conversational AI systems. Features enterprise-grade architecture, distributed swarm intelligence, RAG integration, and native Claude Code / Codex Integration</p>
                    <p class="meta">
                        <span class="language">TypeScript</span> |
                        <span class="stars">15,860</span>
                        | <span class="today">531 stars today</span>
                    </p>
                    <p class="history">First seen: February 28, 2026 | Consecutive daily streak: 1 day</p>
                    <div class="ai-summary">
                        <h4>Analysis</h4>
                        <p>Ruflo v3 is an enterprise AI orchestration platform that converts Claude Code into a production-ready multi-agent system, enabling coordinated swarms of 60+ specialized agents for coding, testing, review, security, documentation, and DevOps. Key features include self-learning routing (SONA, Q-learning, MoE, SemanticRouter), fault-tolerant swarm coordination (Raft/BFT/Gossip/CRDT), fast vector RAG and memory (HNSW, ReasoningBank, Hyperbolic embeddings), multi-LLM provider failover, and an extensible plugin SDK. Technically it runs WASM kernels written in Rust for the policy engine, embeddings, and proof system, leverages optimized primitives (Flash Attention, LoRA, Int8 quantization) and a RETRIEVE‚ÜíJUDGE‚ÜíDISTILL‚ÜíCONSOLIDATE‚ÜíROUTE learning loop, and enforces production security controls like prompt-injection and command-injection protections. The stack also exposes MCP integration for native Claude Code usage and supports local and cloud LLMs with routing and cost/quality tradeoffs.</p>
<p>Organizations building conversational AI, autonomous workflows, or automated software engineering pipelines ‚Äî including engineering teams, SREs, AI researchers, and enterprises requiring auditability and scalability ‚Äî will benefit most from Ruflo. It‚Äôs trending because it packages contemporary capabilities (multi-agent orchestration, RAG, self-optimization, consensus protocols) into a performant, production-oriented stack with Rust/WASM performance, rapid HNSW retrieval, and a plugin marketplace, making it practical to move from prototypes to resilient deployments while optimizing cost and model quality.</p>
                    </div>
                </div>
            </section>

            <section class="repo" data-seen-before="1">
                <div class="repo-header-row">
                    <h3>7. <a href="https://github.com/datawhalechina/hello-agents" target="_blank" rel="noopener noreferrer">datawhalechina/hello-agents</a> <span class="seen-badge">Not new today</span></h3>
                    <button type="button" class="repo-toggle" aria-expanded="true">Hide details</button>
                </div>
                <div class="repo-body">
                    <p class="description">üìö „Ää‰ªéÈõ∂ÂºÄÂßãÊûÑÂª∫Êô∫ËÉΩ‰Ωì„Äã‚Äî‚Äî‰ªéÈõ∂ÂºÄÂßãÁöÑÊô∫ËÉΩ‰ΩìÂéüÁêÜ‰∏éÂÆûË∑µÊïôÁ®ã</p>
                    <p class="meta">
                        <span class="language">Python</span> |
                        <span class="stars">23,237</span>
                        | <span class="today">324 stars today</span>
                    </p>
                    <p class="history">First seen: February 26, 2026 | Consecutive daily streak: 1 day</p>
                    <div class="ai-summary">
                        <h4>Analysis</h4>
                        <p>This repository is an open-source, practice-focused tutorial and codebase that teaches how to build AI-native agents from first principles. Key features include step-by-step implementations of classic agent paradigms (ReAct, Plan-and-Solve, Reflection), a self-developed HelloAgents framework built on OpenAI APIs, memory and retrieval (RAG) modules, multi-agent communication protocols (MCP/A2A/ANP), and an Agentic-RL training pipeline from SFT to GRPO, accompanied by runnable case studies (e.g., travel assistant, DeepResearch, cyber town). Technically it combines detailed chapters with a code folder, low-code platform walkthroughs (Dify/Coze/n8n), downloadable PDF releases, and community contribution patterns to enable hands-on experimentation and end-to-end development. The project is licensed under CC BY-NC-SA 4.0 and invites issues, PRs, and community-contributed extras.</p>
<p>Practitioners who will benefit include AI developers, ML engineers, software engineers, students, and self-learners with basic Python and LLM knowledge because the material emphasizes hands-on code, frameworks, and real-world agent design patterns. It is trending because the industry focus has shifted from larger foundation models to agentic applications in 2024‚Äì2025, and this repo fills a gap by providing systematic, implementation-first guidance on building, training, evaluating, and deploying multi-agent systems. The combination of practical tutorials, a usable framework, training recipes, and curated case studies makes it a timely resource for anyone aiming to move from using LLMs to constructing production-capable agents.</p>
                    </div>
                </div>
            </section>

            <section class="repo" data-seen-before="1">
                <div class="repo-header-row">
                    <h3>8. <a href="https://github.com/abhigyanpatwari/GitNexus" target="_blank" rel="noopener noreferrer">abhigyanpatwari/GitNexus</a> <span class="seen-badge">Not new today</span></h3>
                    <button type="button" class="repo-toggle" aria-expanded="true">Hide details</button>
                </div>
                <div class="repo-body">
                    <p class="description">GitNexus: The Zero-Server Code Intelligence Engine - GitNexus is a client-side knowledge graph creator that runs entirely in your browser. Drop in a GitHub repo or ZIP file, and get an interactive knowledge graph wit a built in Graph RAG Agent. Perfect for code exploration</p>
                    <p class="meta">
                        <span class="language">TypeScript</span> |
                        <span class="stars">6,372</span>
                        | <span class="today">1,385 stars today</span>
                    </p>
                    <p class="history">First seen: February 22, 2026 | Consecutive daily streak: 1 day</p>
                    <div class="ai-summary">
                        <h4>Analysis</h4>
                        <p>GitNexus is a client-side knowledge-graph engine that indexes a codebase into a graph of dependencies, call chains, clusters, and execution flows, then exposes that structure to AI agents and an interactive browser UI. It provides a CLI with an MCP server for deep agent integrations and a zero-server Web UI (WASM) so you can drop a GitHub repo or ZIP and get a navigable graph plus a Graph RAG agent. Under the hood it uses Tree-sitter for parsing, KuzuDB for graph storage, hybrid BM25+semantic search with embeddings, and exposes tools such as query, context, impact, detect_changes, rename, and raw Cypher over MCP. The system also installs agent skills and editor hooks to automatically enrich agent workflows with codebase-aware context.</p>
<p>Developers, engineering teams, and anyone building or operating AI-powered code assistants benefit because GitNexus gives agents deterministic architectural context so they stop missing dependencies, breaking call chains, or shipping blind edits. Its local-first design (persistent KuzuDB for CLI, in-browser WASM for quick exploration) preserves privacy while scaling from single-repo exploration to multi-repo MCP deployments, with integrations for Claude Code, Cursor, and OpenCode. It‚Äôs gaining traction because the rise of LLM-based developer tools has exposed a need for precise, explainable, repo-level context‚ÄîGitNexus converts code into an actionable knowledge graph that makes both small and large models more accurate and trustworthy.</p>
                    </div>
                </div>
            </section>

            <section class="repo" data-seen-before="0">
                <div class="repo-header-row">
                    <h3>9. <a href="https://github.com/moeru-ai/airi" target="_blank" rel="noopener noreferrer">moeru-ai/airi</a> </h3>
                    <button type="button" class="repo-toggle" aria-expanded="true">Hide details</button>
                </div>
                <div class="repo-body">
                    <p class="description">üíñüß∏ Self hosted, you-owned Grok Companion, a container of souls of waifu, cyber livings to bring them into our worlds, wishing to achieve Neuro-sama&#x27;s altitude. Capable of realtime voice chat, Minecraft, Factorio playing. Web / macOS / Windows supported.</p>
                    <p class="meta">
                        <span class="language">TypeScript</span> |
                        <span class="stars">18,599</span>
                        | <span class="today">199 stars today</span>
                    </p>
                    <p class="history">First seen: February 28, 2026 | Consecutive daily streak: 1 day</p>
                    <div class="ai-summary">
                        <h4>Analysis</h4>
                        <p>AIRI is a self‚Äëhosted, cross‚Äëplatform &quot;Grok Companion&quot; framework that creates persistent, interactive digital companions capable of real‚Äëtime voice chat, playing games like Minecraft and Factorio, and driving VTuber avatars (VRM/Live2D). It stitches together modular subsystems ‚Äî Brain (LLM integrations via xsai), Ears (audio input and client‚Äëside speech recognition), Mouth (TTS including ElevenLabs), Body (avatar control with WebGPU/WebAudio/WebAssembly), and in‚Äëbrowser databases (DuckDB WASM, pglite) ‚Äî and can offload to native acceleration (CUDA/Apple Metal via HuggingFace/candle) for local inference. The project provides web, desktop (tamagotchi) and mobile (PWA/capacitor) stages, extensive LLM provider support, and an extensible plugin and subproject ecosystem for automation, RAG/memory, and live streaming workflows.</p>
<p>AIRI is ideal for VTubers, streamers, hobbyists, researchers, and developers who want a privacy‚Äërespecting, customizable alternative to closed agents like Neuro‚Äësama, enabling game automation, multimodal interaction, and self‚Äëhosting. It appeals because of active development, broad backend/provider support, cross‚Äëplatform delivery (browser, desktop, mobile), and deployment tooling (pnpm, Nix, containers), making experimentation and production easier. The project is trending due to rising demand for live interactive AI agents, improvements in local and hybrid inference pipelines, and a growing open‚Äësource ecosystem that lowers the barrier to building real‚Äëtime digital companions.</p>
                    </div>
                </div>
            </section>

            <section class="repo" data-seen-before="1">
                <div class="repo-header-row">
                    <h3>10. <a href="https://github.com/anthropics/claude-code" target="_blank" rel="noopener noreferrer">anthropics/claude-code</a> <span class="seen-badge">Not new today</span></h3>
                    <button type="button" class="repo-toggle" aria-expanded="true">Hide details</button>
                </div>
                <div class="repo-body">
                    <p class="description">Claude Code is an agentic coding tool that lives in your terminal, understands your codebase, and helps you code faster by executing routine tasks, explaining complex code, and handling git workflows - all through natural language commands.</p>
                    <p class="meta">
                        <span class="language">Shell</span> |
                        <span class="stars">71,338</span>
                        | <span class="today">494 stars today</span>
                    </p>
                    <p class="history">First seen: February 22, 2026 | Consecutive daily streak: 1 day</p>
                    <div class="ai-summary">
                        <h4>Analysis</h4>
                        <p>Claude Code is an agentic coding tool that runs in your terminal, IDE, or as a GitHub tag and interacts with your local codebase through natural-language commands. Key features include executing routine tasks, explaining complex code, managing git workflows, and an extensible plugins directory that adds custom commands and agents. Technically it inspects repository files, runs shell and git commands on your behalf, and uses a language-model-based agent to interpret instructions and orchestrate actions. Installation is supported via a single-script installer, Homebrew/WinGet, or other platform-specific methods, and the project collects usage and conversation feedback with documented privacy safeguards.</p>
<p>This tool benefits individual developers, reviewers, and engineering teams who want to speed up repetitive workflows, improve onboarding, and reduce context switching when navigating unfamiliar code. It is trending because AI-driven developer tooling and natural-language interfaces substantially raise productivity, the plugin architecture enables customization for diverse workflows, and easy installation plus community channels (Discord, GitHub) accelerate adoption. The combination of code-aware automation, git integration, and extensibility makes it an attractive option for modern development environments.</p>
                    </div>
                </div>
            </section>

            </div>
        </article>
    </main>
    <footer>
        <p>Generated automatically. Data from <a href="https://github.com/trending">GitHub Trending</a>.</p>
    </footer>

<script>
(() => {
    const readKey = "gtd:read_days:gh:v1";
    const dayStr = "2026-02-28";

    let stored = [];
    try {
        stored = JSON.parse(localStorage.getItem(readKey) || "[]");
        if (!Array.isArray(stored)) {
            stored = [];
        }
    } catch (_err) {
        stored = [];
    }

    if (!stored.includes(dayStr)) {
        stored.push(dayStr);
        stored.sort();
        localStorage.setItem(readKey, JSON.stringify(stored));
    }

    const collapseParam = new URLSearchParams(window.location.search).get("collapse_seen");
    const collapseSeen = collapseParam === "0" ? false : true;

    function setCollapsed(repoEl, collapsed) {
        repoEl.classList.toggle("collapsed", collapsed);
        const button = repoEl.querySelector(".repo-toggle");
        if (button) {
            button.textContent = collapsed ? "Show details" : "Hide details";
            button.setAttribute("aria-expanded", String(!collapsed));
        }
    }

    const repos = Array.from(document.querySelectorAll("section.repo[data-seen-before]"));

    repos.forEach((repoEl) => {
        const toggle = repoEl.querySelector(".repo-toggle");
        if (!toggle) {
            return;
        }

        toggle.addEventListener("click", () => {
            setCollapsed(repoEl, !repoEl.classList.contains("collapsed"));
        });
    });

    const collapseBtn = document.getElementById("collapse-seen-btn");
    const expandBtn = document.getElementById("expand-all-btn");

    if (collapseBtn) {
        collapseBtn.addEventListener("click", () => {
            repos.forEach((repoEl) => {
                if (repoEl.dataset.seenBefore === "1") {
                    setCollapsed(repoEl, true);
                }
            });
        });
    }

    if (expandBtn) {
        expandBtn.addEventListener("click", () => {
            repos.forEach((repoEl) => setCollapsed(repoEl, false));
        });
    }

    if (collapseSeen) {
        repos.forEach((repoEl) => {
            if (repoEl.dataset.seenBefore === "1") {
                setCollapsed(repoEl, true);
            }
        });
    }
})();
</script>

</body>
</html>
