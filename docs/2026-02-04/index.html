<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>GitHub Trending - February 04, 2026</title>
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <header>
        <h1>GitHub Trending Digest - February 04, 2026</h1>
        <nav>
            <a href="../">&larr; GitHub Calendar</a>
            <a href="../hn/">Hacker News</a>
        </nav>
    </header>
    <main>
        <div class="repo-controls">
            <button id="collapse-seen-btn" type="button">Collapse Seen Repos</button>
            <button id="expand-all-btn" type="button">Expand All</button>
        </div>
        <article>
            <div class="repos">

            <section class="repo" data-seen-before="1">
                <div class="repo-header-row">
                    <h3>1. <a href="https://github.com/thedotmack/claude-mem" target="_blank" rel="noopener noreferrer">thedotmack/claude-mem</a> <span class="seen-badge">Seen before</span></h3>
                    <button type="button" class="repo-toggle" aria-expanded="true">Hide details</button>
                </div>
                <div class="repo-body">
                    <p class="description">A Claude Code plugin that automatically captures everything Claude does during your coding sessions, compresses it with AI (using Claude&#x27;s agent-sdk), and injects relevant context back into future sessions.</p>
                    <p class="meta">
                        <span class="language">TypeScript</span> |
                        <span class="stars">20,818 stars</span>
                        | <span class="today">1,730 stars today</span>
                    </p>
                    <p class="history">First seen: February 02, 2026 | Consecutive daily streak: 3 days</p>
                    <div class="ai-summary">
                        <h4>Analysis</h4>
                        <p>Claude-Mem is a Claude Code plugin that automatically captures Claude’s tool usage and observations during coding sessions, compresses them with the Claude agent-sdk, stores them in a local database, and injects relevant context into future sessions. Technically it uses lifecycle hook scripts to intercept session events, a Bun-managed worker HTTP service (default port 37777) with a web viewer, SQLite (with FTS5) for persistence, and a Chroma vector DB for hybrid semantic search. Retrieval is token-efficient via MCP tools (search → timeline → get_observations) implementing progressive disclosure, and the system includes configuration, privacy exclusion tags, citationable observation IDs, and a beta channel for experimental features like Endless Mode.</p>
<p>This tool benefits individual developers and teams who need continuity across ephemeral AI sessions by reducing repetitive context copying, accelerating debugging and onboarding, and preserving project knowledge and decision history. Its token-aware progressive retrieval and hybrid search make it cost-efficient for large histories, while local storage and privacy controls suit sensitive codebases. Because it integrates directly into Claude Code and leverages the agent SDK for automated summarization and injection, it’s attractive to users looking for persistent, reproducible AI-assisted coding workflows.</p>
                    </div>
                </div>
            </section>

            <section class="repo" data-seen-before="0">
                <div class="repo-header-row">
                    <h3>2. <a href="https://github.com/masoncl/review-prompts" target="_blank" rel="noopener noreferrer">masoncl/review-prompts</a> </h3>
                    <button type="button" class="repo-toggle" aria-expanded="true">Hide details</button>
                </div>
                <div class="repo-body">
                    <p class="description">AI review prompts</p>
                    <p class="meta">
                        <span class="language">Python</span> |
                        <span class="stars">421 stars</span>
                        | <span class="today">54 stars today</span>
                    </p>
                    <p class="history">First seen: February 04, 2026 | Consecutive daily streak: 1 day</p>
                    <div class="ai-summary">
                        <h4>Analysis</h4>
                        <p>This repository provides curated AI-assisted code review prompts and workflows tailored for Linux kernel and systemd development, designed to work with Claude Code and other AI tools. Its main features include project-specific skill files that auto-load context based on your working directory, slash commands for quick review/debug/verify actions (e.g., /kreview, /kdebug, /systemd-review), subsystem pattern documentation, and setup scripts to install prompts. Technically it packages skill templates, slash-command handlers, and subsystem files that supply domain-specific knowledge on demand, while detecting project roots to load relevant context. It also documents bug patterns and recommends pairing with semcode for fast semantic code navigation and search.</p>
<p>The project is valuable to kernel and systemd developers, maintainers, and reviewers who want consistent, repeatable AI-assisted review workflows to speed triage, debugging, and verification. Typical use cases include pre-commit or patchset reviews, focused debugging prompts for subsystem-specific issues, onboarding contributors to project conventions, and automated checks for common bug patterns. Its relevance is growing due to broader adoption of LLM-powered developer tooling and semantic search, which together improve review throughput and reduce manual effort in complex system codebases.</p>
                    </div>
                </div>
            </section>

            <section class="repo" data-seen-before="0">
                <div class="repo-header-row">
                    <h3>3. <a href="https://github.com/openai/skills" target="_blank" rel="noopener noreferrer">openai/skills</a> </h3>
                    <button type="button" class="repo-toggle" aria-expanded="true">Hide details</button>
                </div>
                <div class="repo-body">
                    <p class="description">Skills Catalog for Codex</p>
                    <p class="meta">
                        <span class="language">Python</span> |
                        <span class="stars">3,201 stars</span>
                        | <span class="today">377 stars today</span>
                    </p>
                    <p class="history">First seen: February 04, 2026 | Consecutive daily streak: 1 day</p>
                    <div class="ai-summary">
                        <h4>Analysis</h4>
                        <p>This repository catalogs &quot;Agent Skills&quot;—self-contained folders of instructions, scripts, and resources that Codex-based AI agents can discover and use to perform specific tasks. Its main features include a curated and experimental skills layout, automatic installation of .system skills, and a $skill-installer command that adds skills by name, folder, or GitHub directory URL; each skill carries its own LICENSE.txt and follows the Agent Skills open standard. Technically, skills are packaged as directory-based modules that Codex imports at runtime (requiring a restart to pick up new skills), enabling repeatable, shareable capabilities composed of prompts, scripts, and ancillary assets. The repository therefore acts as both a registry and distribution mechanism for modular agent behaviors.</p>
<p>The project is valuable to developers, product teams, and organizations building agent-driven automation because it centralizes reusable, testable components that speed development and reduce duplication of effort. Use cases include domain-specific automation, standardized prompt and workflow sharing, rapid prototyping of agent capabilities, and collaboration across teams that need consistent agent behavior. It’s gaining traction because modular, discoverable skills align with the broader trend toward agentization and platformization of AI tooling, making it easier to scale and govern autonomous workflows.</p>
                    </div>
                </div>
            </section>

            <section class="repo" data-seen-before="0">
                <div class="repo-header-row">
                    <h3>4. <a href="https://github.com/automazeio/ccpm" target="_blank" rel="noopener noreferrer">automazeio/ccpm</a> </h3>
                    <button type="button" class="repo-toggle" aria-expanded="true">Hide details</button>
                </div>
                <div class="repo-body">
                    <p class="description">Project management system for Claude Code using GitHub Issues and Git worktrees for parallel agent execution.</p>
                    <p class="meta">
                        <span class="language">Shell</span> |
                        <span class="stars">6,784 stars</span>
                        | <span class="today">123 stars today</span>
                    </p>
                    <p class="history">First seen: February 04, 2026 | Consecutive daily streak: 1 day</p>
                    <div class="ai-summary">
                        <h4>Analysis</h4>
                        <p>Claude Code PM is a spec-driven project management system that converts PRDs into epics, decomposes epics into actionable tasks, and synchronizes those tasks with GitHub Issues while running multiple AI agents in parallel using Git worktrees. Its main features include a five‑phase discipline (brainstorm, document, plan, execute, track), a command set for creating and syncing PRDs/epics/issues (/pm:prd-new, /pm:epic-sync, /pm:issue-start, etc.), and a .claude workspace containing agents, commands, epics, and PRD files. Technically it treats GitHub Issues as the single source of truth and audit trail, uses worktrees to give independent agent branches and avoid conflicts, and updates progress via issue comments, labels, and PRs for full traceability.</p>
<p>This system is valuable for engineering teams, AI-augmented developers, and managers who need scalable, auditable collaboration between humans and LLM agents without losing context or creating merge conflicts. It speeds delivery by enabling parallel task execution, enforcing specification-first development to reduce rework and bugs, and providing transparent progress tracking in the GitHub ecosystem teams already use. Its relevance is rising with increased adoption of autonomous AI agents and the need for reproducible, Git-native workflows that integrate AI workstreams into existing team processes.</p>
                    </div>
                </div>
            </section>

            <section class="repo" data-seen-before="0">
                <div class="repo-header-row">
                    <h3>5. <a href="https://github.com/obra/superpowers" target="_blank" rel="noopener noreferrer">obra/superpowers</a> </h3>
                    <button type="button" class="repo-toggle" aria-expanded="true">Hide details</button>
                </div>
                <div class="repo-body">
                    <p class="description">An agentic skills framework &amp; software development methodology that works.</p>
                    <p class="meta">
                        <span class="language">Shell</span> |
                        <span class="stars">43,756 stars</span>
                        | <span class="today">817 stars today</span>
                    </p>
                    <p class="history">First seen: February 04, 2026 | Consecutive daily streak: 1 day</p>
                    <div class="ai-summary">
                        <h4>Analysis</h4>
                        <p>Superpowers is a complete software development workflow for coding agents built around composable &quot;skills&quot; and starter instructions that make agents follow a consistent process. Key features include a modular skills library (TDD, systematic debugging, brainstorming, plan-writing, subagent-driven development, code review, and git worktree management), an enforced workflow that moves from spec elicitation to plan to task-level subagents, and a two-stage review system (spec compliance then code quality). Technically it ships as a plugin (Claude Code marketplace) with manual install paths for Codex and OpenCode, stores skills directly in the repository, and has agents fetch and execute skills, spawn subagents per task, and run automated verification and test-driven cycles. The repo also contains contribution guidance and update mechanics so skills can be extended and updated easily.</p>
<p>This project benefits developers, teams, and tool builders who want repeatable, evidence-driven AI-assisted engineering—especially those who need automated testing, clear task plans, and controlled parallel work via subagents. It’s gaining traction because it addresses practical pain points in agentic coding (avoiding ad-hoc behavior, enforcing RED‑GREEN‑REFACTOR, and integrating with git workflows) while offering an easy integration path for Claude Code and extensibility via an open-source MIT-licensed skill library.</p>
                    </div>
                </div>
            </section>

            <section class="repo" data-seen-before="0">
                <div class="repo-header-row">
                    <h3>6. <a href="https://github.com/virattt/dexter" target="_blank" rel="noopener noreferrer">virattt/dexter</a> </h3>
                    <button type="button" class="repo-toggle" aria-expanded="true">Hide details</button>
                </div>
                <div class="repo-body">
                    <p class="description">An autonomous agent for deep financial research</p>
                    <p class="meta">
                        <span class="language">TypeScript</span> |
                        <span class="stars">10,185 stars</span>
                        | <span class="today">222 stars today</span>
                    </p>
                    <p class="history">First seen: February 04, 2026 | Consecutive daily streak: 1 day</p>
                    <div class="ai-summary">
                        <h4>Analysis</h4>
                        <p>Dexter is an autonomous financial research agent that converts complex finance questions into structured, step-by-step research plans, executes those steps using live market and financial datasets, and iterates via self-validation until it produces a confident, data-backed answer. Key features include intelligent task planning, autonomous tool selection and execution, self-reflection and error checking, and safety mechanisms such as loop detection and step limits. Technically it runs on the Bun runtime, integrates with LLM providers (OpenAI, Anthropic, XAI, OpenRouter, or local Ollama), pulls institutional-grade data via a Financial Datasets API and optional web search (Exa/Tavily), and logs every action to newline-delimited JSON scratchpads for traceability. An evaluation suite using LangSmith and an LLM-as-judge enables benchmarking on a dataset of financial questions.</p>
<p>This project is useful for sell-side and buy-side analysts, quantitative researchers, fintech engineers, and anyone needing reproducible, fast financial due diligence or idea vetting. It automates tedious data collection and initial analysis, provides auditable scratchpad logs and an eval pipeline for continuous improvement, and is extensible to different LLMs and data sources. Its traction comes from combining domain-specific tooling, live data access, and autonomous agent patterns that accelerate high-stakes research workflows while maintaining safety and traceability.</p>
                    </div>
                </div>
            </section>

            <section class="repo" data-seen-before="1">
                <div class="repo-header-row">
                    <h3>7. <a href="https://github.com/karpathy/nanochat" target="_blank" rel="noopener noreferrer">karpathy/nanochat</a> <span class="seen-badge">Seen before</span></h3>
                    <button type="button" class="repo-toggle" aria-expanded="true">Hide details</button>
                </div>
                <div class="repo-body">
                    <p class="description">The best ChatGPT that $100 can buy.</p>
                    <p class="meta">
                        <span class="language">Python</span> |
                        <span class="stars">42,095 stars</span>
                        | <span class="today">447 stars today</span>
                    </p>
                    <p class="history">First seen: February 03, 2026 | Consecutive daily streak: 2 days</p>
                    <div class="ai-summary">
                        <h4>Analysis</h4>
                        <p>nanochat is a compact, hackable PyTorch harness for training, evaluating, and interacting with transformer LLMs on a single GPU node or small multi‑GPU setups. It bundles tokenization (BPE), data loading, the GPT model implementation, optimizer (AdamW + Muon), distributed training utilities, evaluation (CORE metric, bits‑per‑byte), KV‑cache inference engine, checkpoint management, and a ChatGPT‑like web UI; the repo includes runnable scripts (e.g., runs/speedrun.sh) that automate end‑to‑end pretraining and finetuning workflows using torchrun, gradient accumulation, and optional wandb logging. The code is intentionally minimal so users can reproduce GPT‑2‑grade models quickly (targeting an 8×H100 node in ~3 hours) and experiment with scaling laws, synthetic data, or custom SFT stages. It also provides support for CPU/MPS, small‑model quick experiments, and a leaderboard/metrics to compare runs.</p>
<p>This project is valuable for researchers, engineers, educators, and hobbyists who want an accessible, reproducible platform to prototype and study LLM training and finetuning without a large infrastructure investment. It enables rapid iteration on model depth, data budgets, and training recipes, making it useful for scaling‑law research, model development, and building custom chatbots or fine‑tuned assistants. nanochat is trending because it dramatically lowers the barrier to entry for training capable LLMs (cost/time reductions), offers a concise codebase that’s easy to modify, and provides community hooks (Discussions/Discord/leaderboard) for collaborative experimentation.</p>
                    </div>
                </div>
            </section>

            <section class="repo" data-seen-before="1">
                <div class="repo-header-row">
                    <h3>8. <a href="https://github.com/kovidgoyal/calibre" target="_blank" rel="noopener noreferrer">kovidgoyal/calibre</a> <span class="seen-badge">Seen before</span></h3>
                    <button type="button" class="repo-toggle" aria-expanded="true">Hide details</button>
                </div>
                <div class="repo-body">
                    <p class="description">The official source code repository for the calibre ebook manager</p>
                    <p class="meta">
                        <span class="language">Python</span> |
                        <span class="stars">23,949 stars</span>
                        | <span class="today">141 stars today</span>
                    </p>
                    <p class="history">First seen: February 02, 2026 | Consecutive daily streak: 1 day</p>
                    <div class="ai-summary">
                        <h4>Analysis</h4>
                        <p>Calibre is an open-source e-book manager that lets users view, convert, edit, and catalog e-books in all major formats, sync with e-reader devices, fetch metadata from the internet, and convert news into e-book form. The repository contains the application source and build instructions for producing platform-specific binaries for Linux, Windows and macOS. Technically it combines a desktop GUI with a conversion/processing engine, device communication drivers, internet metadata/news fetchers and a content server, and exposes extensibility via plugins. The core application is implemented to be portable across platforms (commonly using Python and cross-platform GUI libraries) and includes tooling for building installers.</p>
<p>The project is valuable to individual readers, e-reader owners, librarians, archivists and developers who need a single, flexible tool to manage large digital book collections and to interoperate across devices and formats. It centralizes library organization, automated metadata enrichment, format conversion and device synchronization, reducing friction when moving books between ecosystems. Its open-source nature, extensibility and active maintenance make it a practical choice for users who want control, automation and long-term access to their e-book collections, which helps explain its ongoing popularity.</p>
                    </div>
                </div>
            </section>

            <section class="repo" data-seen-before="1">
                <div class="repo-header-row">
                    <h3>9. <a href="https://github.com/OpenBMB/ChatDev" target="_blank" rel="noopener noreferrer">OpenBMB/ChatDev</a> <span class="seen-badge">Seen before</span></h3>
                    <button type="button" class="repo-toggle" aria-expanded="true">Hide details</button>
                </div>
                <div class="repo-body">
                    <p class="description">ChatDev 2.0: Dev All through LLM-powered Multi-Agent Collaboration</p>
                    <p class="meta">
                        <span class="language">Python</span> |
                        <span class="stars">29,823 stars</span>
                        | <span class="today">476 stars today</span>
                    </p>
                    <p class="history">First seen: February 03, 2026 | Consecutive daily streak: 2 days</p>
                    <div class="ai-summary">
                        <h4>Analysis</h4>
                        <p>ChatDev 2.0 (DevAll) is a zero-code multi-agent orchestration platform that enables users to design, configure, and execute customized multi-agent systems via a visual workflow canvas or YAML-based instances. It exposes a web console (Vite + Vue frontend) and a Python SDK for programmatic runs, supports environment/model key configuration, real-time logs, human-in-the-loop feedback, and safe execution options (Docker/Git modes). Technically, it composes LLM-powered agents into node-based workflows and supports advanced orchestration paradigms—chain-topologies from ChatDev 1.0, MacNet DAGs for scalable collaboration, and a puppeteer-style central orchestrator optimized with reinforcement learning—to sequence, activate, and manage agents efficiently. The repository also bundles research implementations and papers (e.g., MacNet, IER, NeurIPS 2025 puppeteer branch) illustrating evolving orchestration and experience refinement techniques.</p>
<p>This project benefits developers, AI researchers, startups, and enterprises seeking to prototype or automate complex tasks (software development, data analysis, visualization, 3D generation) without writing orchestration code. By offering a zero-code interface, reusable YAML workflows, and programmatic SDK access, it reduces engineering overhead and speeds iteration while scaling to many agents and optimizing compute via learned orchestration. Researchers gain a practical testbed and reference implementations for multi-agent collaboration methods, and product teams can integrate with LLM providers through simple API key configuration. Given the surge in interest in multi-agent LLM systems, ChatDev is timely: it operationalizes collaborative agent patterns, supports human-in-the-loop and CI-friendly workflows, and bridges research ideas to deployable tooling.</p>
                    </div>
                </div>
            </section>

            <section class="repo" data-seen-before="1">
                <div class="repo-header-row">
                    <h3>10. <a href="https://github.com/pedramamini/Maestro" target="_blank" rel="noopener noreferrer">pedramamini/Maestro</a> <span class="seen-badge">Seen before</span></h3>
                    <button type="button" class="repo-toggle" aria-expanded="true">Hide details</button>
                </div>
                <div class="repo-body">
                    <p class="description">Agent Orchestration Command Center</p>
                    <p class="meta">
                        <span class="language">TypeScript</span> |
                        <span class="stars">1,537 stars</span>
                        | <span class="today">269 stars today</span>
                    </p>
                    <p class="history">First seen: February 02, 2026 | Consecutive daily streak: 3 days</p>
                    <div class="ai-summary">
                        <h4>Analysis</h4>
                        <p>Maestro is a cross-platform desktop app for orchestrating fleets of AI coding agents and projects, offering a keyboard-first interface to run, monitor, and automate parallel agent workflows. Key features include Git worktrees for isolated branch-based agents, Auto Run and filesystem playbooks for batch task execution, multi-agent group chat, mobile remote control via a built-in web server, and a CLI for headless operation. Technically it functions as a pass-through to supported AI providers (Claude Code, OpenAI Codex, OpenCode, Factory Droid), spawning fresh or resumed sessions per task, managing conversation context and workspaces, queuing messages, and integrating with git and optional remote tunneling. It also provides session discovery, token/cost tracking, analytics, and a document graph for visualizing project knowledge.</p>
<p>Maestro is aimed at developers, AI researchers, and power users who need to run multiple agentic workflows in parallel, automate repeatable playbooks, or maintain long-running unattended sessions tied to codebases. Teams benefit from isolated worktree agents that produce PR-ready outputs and from embedding agent runs into cron/CI via the CLI, while solo practitioners gain speed from keyboard-driven controls and rapid context switching. Its analytics, cost tracking, and knowledge-graph features support governance and project visibility, and the pass-through model preserves existing provider permissions and tooling—attributes that make it relevant in the growing trend of agent orchestration tools.</p>
                    </div>
                </div>
            </section>

            </div>
        </article>
    </main>
    <footer>
        <p>Generated automatically. Data from <a href="https://github.com/trending">GitHub Trending</a>.</p>
    </footer>

<script>
(() => {
    const readKey = "gtd:read_days:gh:v1";
    const dayStr = "2026-02-04";

    let stored = [];
    try {
        stored = JSON.parse(localStorage.getItem(readKey) || "[]");
        if (!Array.isArray(stored)) {
            stored = [];
        }
    } catch (_err) {
        stored = [];
    }

    if (!stored.includes(dayStr)) {
        stored.push(dayStr);
        stored.sort();
        localStorage.setItem(readKey, JSON.stringify(stored));
    }

    const collapseParam = new URLSearchParams(window.location.search).get("collapse_seen");
    const collapseSeen = collapseParam === "0" ? false : true;

    function setCollapsed(repoEl, collapsed) {
        repoEl.classList.toggle("collapsed", collapsed);
        const button = repoEl.querySelector(".repo-toggle");
        if (button) {
            button.textContent = collapsed ? "Show details" : "Hide details";
            button.setAttribute("aria-expanded", String(!collapsed));
        }
    }

    const repos = Array.from(document.querySelectorAll("section.repo[data-seen-before]"));

    repos.forEach((repoEl) => {
        const toggle = repoEl.querySelector(".repo-toggle");
        if (!toggle) {
            return;
        }

        toggle.addEventListener("click", () => {
            setCollapsed(repoEl, !repoEl.classList.contains("collapsed"));
        });
    });

    const collapseBtn = document.getElementById("collapse-seen-btn");
    const expandBtn = document.getElementById("expand-all-btn");

    if (collapseBtn) {
        collapseBtn.addEventListener("click", () => {
            repos.forEach((repoEl) => {
                if (repoEl.dataset.seenBefore === "1") {
                    setCollapsed(repoEl, true);
                }
            });
        });
    }

    if (expandBtn) {
        expandBtn.addEventListener("click", () => {
            repos.forEach((repoEl) => setCollapsed(repoEl, false));
        });
    }

    if (collapseSeen) {
        repos.forEach((repoEl) => {
            if (repoEl.dataset.seenBefore === "1") {
                setCollapsed(repoEl, true);
            }
        });
    }
})();
</script>

</body>
</html>
