<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>GitHub Trending - February 02, 2026</title>
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <header>
        <h1>GitHub Trending Digest - February 02, 2026</h1>
        <nav>
            <a href="../">&larr; Back to Calendar</a>
        </nav>
    </header>
    <main>
        <article>
            <div class="repos">

            <section class="repo">
                <h3>1. <a href="https://github.com/openclaw/openclaw" target="_blank">openclaw/openclaw</a></h3>
                <p class="description">Your own personal AI assistant. Any OS. Any Platform. The lobster way. ü¶û</p>
                <p class="meta">
                    <span class="language">TypeScript</span> |
                    <span class="stars">144,907 stars</span>
                    | <span class="today">10,794 stars today</span>
                </p>
                <div class="ai-summary">
                    <h4>Analysis</h4>
                    <p>OpenClaw is a self-hosted personal AI assistant that runs on your devices and responds over the messaging channels you already use (WhatsApp, Telegram, Slack, Discord, Google Chat, Signal, iMessage, WebChat, and more) while also supporting voice (macOS/iOS/Android), a live Canvas UI, and extension nodes. Its core is a Gateway WebSocket control plane that coordinates sessions, multi-agent routing, tools (browser, canvas, cron, nodes), and presence; clients include a CLI, web UI, macOS/iOS/Android nodes, and Pi agents. The project emphasizes model flexibility and failover (Anthropic/OpenAI supported), runs on Node ‚â•22 with an onboarding wizard and daemon for continuous operation, and ships sensible security defaults like DM pairing and local allowlists. Packaging and remote access options (Tailscale, Docker, Nix) plus first-class media/transcription and streaming tooling make it a full-stack assistant platform.</p>
<p>OpenClaw is valuable to privacy-conscious users, power users, and developers who want a single-user, always-on assistant that integrates into existing communication flows and device capabilities without relying solely on cloud-hosted control planes. It‚Äôs well suited for automation, multi-device voice workflows, browser-driven tasks, and team or personal productivity scenarios that need per-agent isolation and tool access. Its trending appeal comes from being local‚Äëfirst and extensible (broad channel support, model failover, Canvas/voice features), combined with strong onboarding and security primitives that lower the barrier to running a capable personal AI on your own infrastructure.</p>

                </div>
            </section>

            <section class="repo">
                <h3>2. <a href="https://github.com/ThePrimeagen/99" target="_blank">ThePrimeagen/99</a></h3>
                <p class="description">Neovim AI agent done right</p>
                <p class="meta">
                    <span class="language">Lua</span> |
                    <span class="stars">2,812 stars</span>
                    | <span class="today">781 stars today</span>
                </p>
                <div class="ai-summary">
                    <h4>Analysis</h4>
                    <p>99 is a Neovim plugin that provides an integrated AI agent workflow for in-editor code transformations, fill-in function generation, and rule-driven edits. Built in Lua and designed to work with the opencode backend, it uses customizable SKILL/AGENT markdown rules, cmp-based completion for rule inclusion (triggered with @), and a set of keymaps (e.g., fillin_function, visual) to trigger scoped AI actions. The plugin also includes file- and project-level rule discovery, logging and debug facilities, and planned integration with Treesitter/LSP and FIM models to improve context-aware requests; it is currently alpha and focused on TypeScript and Lua support. Users should expect the agent to send contextual file contents for edits and apply replacements, with known quirks around long functions, visual selection behavior, and prompt quality.</p>
<p>This tool benefits experienced Neovim users and developers who want fast, constrained AI-driven edits without leaving their editor‚Äîparticularly those working in TypeScript and Lua or who already use cmp and Lazy. It‚Äôs useful for automated refactors, inserting debugging helpers, filling out function bodies, and enforcing project-specific rules via custom SKILL.md files. The project‚Äôs momentum comes from ThePrimeagen‚Äôs community visibility, live deep-dive discussions, and its approach of combining editor ergonomics, completion, and rule-based AI prompts to produce repeatable, auditable code changes.</p>

                </div>
            </section>

            <section class="repo">
                <h3>3. <a href="https://github.com/pedramamini/Maestro" target="_blank">pedramamini/Maestro</a></h3>
                <p class="description">Agent Orchestration Command Center</p>
                <p class="meta">
                    <span class="language">TypeScript</span> |
                    <span class="stars">1,061 stars</span>
                    | <span class="today">49 stars today</span>
                </p>
                <div class="ai-summary">
                    <h4>Analysis</h4>
                    <p>Maestro is a cross-platform desktop app that orchestrates fleets of AI agents and automates project workflows through a keyboard-first interface. It combines Git worktrees for isolated parallel branches, Auto Run &amp; Playbooks (a file-system-based task runner that processes markdown checklists with fresh AI sessions), group chats mediated by moderator agents, per-agent AI and shell terminals, message queueing, session discovery, and cost/usage analytics. Technically it spawns isolated sessions per task, integrates with agent providers like Claude Code, OpenAI Codex, and OpenCode, and exposes a CLI plus a built-in web server for mobile/remote control; it ties into the filesystem and git to provide context-aware prompts and reproducible workspaces. The app also includes a document graph, output filtering, theming, and extensible slash commands to support automation and integration into scripts or CI pipelines.</p>
<p>Maestro is useful for developers, technical leads, and power users who juggle multiple projects, need reproducible automation, or want to run long-running unattended agent workflows. Typical use cases include spec-to-code pipelines, parallel refactoring or feature branches via worktrees, cron/CI-driven playbooks, multi-agent architecture discussions, and mobile monitoring of running agents. Its keyboard-first UX, CLI/JSONL outputs, and git-aware context make it easy to integrate into development workflows while token tracking and session isolation reduce noise and cost risk. With rising interest in agentic tooling and multi-agent orchestration, Maestro addresses a trending need for developer-centric automation and scalable AI-assisted workflows.</p>

                </div>
            </section>

            <section class="repo">
                <h3>4. <a href="https://github.com/kovidgoyal/calibre" target="_blank">kovidgoyal/calibre</a></h3>
                <p class="description">The official source code repository for the calibre ebook manager</p>
                <p class="meta">
                    <span class="language">Python</span> |
                    <span class="stars">23,680 stars</span>
                    | <span class="today">37 stars today</span>
                </p>
                <div class="ai-summary">
                    <h4>Analysis</h4>
                    <p>Calibre is an e-book manager that lets users view, convert, edit, and catalog e-books across all major formats, and it can communicate with e-reader devices, fetch metadata from the internet, and download newspapers for conversion into e-books. It is cross-platform (Linux, Windows, macOS) and exposes both a Qt-based GUI and command-line tools for library management and batch operations. Technically, calibre is implemented in Python with a Qt UI, an internal conversion engine, a plugin architecture, and a library structure combining on-disk book files with a metadata database and a built-in content server for network/OPDS access.</p>
<p>This project is valuable for individual readers, librarians, archivists, publishers, and power users who need robust tools for organizing, converting, and syncing large ebook collections or automating content ingestion. Features like batch conversion, device integration, metadata enrichment, and scheduled downloads simplify workflows from acquisition to reading on devices. As an open-source, extensible, cross-platform tool with an active community and frequent updates, calibre remains a popular solution for resolving format and interoperability challenges in the ebook ecosystem.</p>

                </div>
            </section>

            <section class="repo">
                <h3>5. <a href="https://github.com/badlogic/pi-mono" target="_blank">badlogic/pi-mono</a></h3>
                <p class="description">AI agent toolkit: coding agent CLI, unified LLM API, TUI &amp; web UI libraries, Slack bot, vLLM pods</p>
                <p class="meta">
                    <span class="language">TypeScript</span> |
                    <span class="stars">5,135 stars</span>
                    | <span class="today">613 stars today</span>
                </p>
                <div class="ai-summary">
                    <h4>Analysis</h4>
                    <p>Pi Monorepo is a TypeScript monorepo that provides a toolkit for building AI agents and managing LLM deployments. Its main features include a unified multi-provider LLM API (@mariozechner/pi-ai) that wraps OpenAI, Anthropic, Google, etc.; an agent runtime with tool calling and state management (@mariozechner/pi-agent-core); an interactive coding-agent CLI; a Slack bot that delegates to the coding agent; terminal UI components with differential rendering; web UI chat components; and a CLI for managing vLLM GPU pod deployments. It is organized as multiple npm packages with standard build/test/check scripts (npm install, npm run build/check, test.sh), uses TypeScript (tsc and .d.ts artifacts), and includes CONTRIBUTING.md and AGENTS.md for contribution and agent-specific rules.</p>
<p>This project is valuable to developers and ops teams who want to prototype or productionize LLM-driven agents, integrate multi-vendor LLMs, or run vLLM on GPU pods‚Äîparticularly coding-assistant builders, SREs managing GPU deployments, and teams embedding chat/UIs or Slack automation. It‚Äôs timely because unified provider APIs, agent runtimes, and on-prem vLLM orchestration address common pain points (vendor lock-in, tool integration, deployment on GPUs), and the modular packages (CLI, TUI, web UI, bot) make it easy to reuse components across different products and workflows.</p>

                </div>
            </section>

            <section class="repo">
                <h3>6. <a href="https://github.com/thedotmack/claude-mem" target="_blank">thedotmack/claude-mem</a></h3>
                <p class="description">A Claude Code plugin that automatically captures everything Claude does during your coding sessions, compresses it with AI (using Claude&#x27;s agent-sdk), and injects relevant context back into future sessions.</p>
                <p class="meta">
                    <span class="language">TypeScript</span> |
                    <span class="stars">16,759 stars</span>
                    | <span class="today">196 stars today</span>
                </p>
                <div class="ai-summary">
                    <h4>Analysis</h4>
                    <p>Claude-Mem is a Claude Code plugin that automatically captures observations and tool usage during coding sessions, compresses them with Claude‚Äôs agent-sdk, and injects relevant semantic summaries back into future sessions to preserve context. Its main features include persistent memory across sessions, a progressive-disclosure MCP search workflow (search ‚Üí timeline ‚Üí get_observations) for token-efficient retrieval, a mem-search skill, a web viewer and HTTP worker service on port 37777, privacy exclusion tags, and a beta channel with experimental modes like Endless Mode. Technically it uses lifecycle hook scripts (SessionStart, UserPromptSubmit, PostToolUse, Stop, SessionEnd), a Bun-managed worker, SQLite (FTS5) for storage, and a Chroma vector database for hybrid semantic + keyword search, exposing search endpoints and configuration under ~/.claude-mem. The system is designed to run automatically with fine-grained context configuration and citationable observation IDs for traceability.</p>
<p>This project is valuable to developers and teams who use Claude Code and need continuity across ephemeral conversations‚Äîhelpful for long-running projects, debugging history, onboarding, and preserving decisions or tool outputs. It reduces token costs via its layered retrieval workflow, provides traceable citations and privacy controls for sensitive content, and surfaces relevant past context automatically so sessions can pick up where they left off. Claude-Mem is timely because agent-enabled tooling and persistent memory for LLM workflows are trending priorities, and its plugin architecture and practical integrations make it immediately useful for practitioners adopting Claude Code.</p>

                </div>
            </section>

            <section class="repo">
                <h3>7. <a href="https://github.com/microsoft/agent-lightning" target="_blank">microsoft/agent-lightning</a></h3>
                <p class="description">The absolute trainer to light up AI agents.</p>
                <p class="meta">
                    <span class="language">Python</span> |
                    <span class="stars">13,066 stars</span>
                    | <span class="today">406 stars today</span>
                </p>
                <div class="ai-summary">
                    <h4>Analysis</h4>
                    <p>Agent Lightning is a lightweight training framework that enables optimization of AI agents with minimal or no code changes across any agent framework (LangChain, OpenAI Agent SDK, AutoGen, CrewAI, Microsoft Agent Framework) or plain Python OpenAI clients. It supports selective optimization in multi-agent systems and integrates algorithms like reinforcement learning, automatic prompt optimization, and supervised fine‚Äëtuning. The system instruments agents via an agl.emit_xxx() helper or an automated tracer to capture prompts, tool calls, and rewards as structured spans that feed into a central LightningStore. Algorithms consume those spans and publish updated resources (refined prompt templates, policy weights) while a Trainer streams datasets to runners, synchronizes resources, and updates the inference engine to close the learning loop.</p>
<p>This project is useful for researchers, ML engineers, and product teams who need to iterate on agent behavior, tune tool-using or multi-agent systems, and run scalable RL experiments without rewriting existing agent code. Typical use cases include prompt and policy optimization for production agents, multi-agent coordination and verification workflows, and large-scale RL training demonstrated in community case studies. Its framework-agnostic architecture, trace-driven data model, and Microsoft research backing make it attractive for rapid experimentation, reproducible pipelines, and teams aiming for stable, scalable agent improvement.</p>

                </div>
            </section>

            <section class="repo">
                <h3>8. <a href="https://github.com/amantus-ai/vibetunnel" target="_blank">amantus-ai/vibetunnel</a></h3>
                <p class="description">Turn any browser into your terminal &amp; command your agents on the go.</p>
                <p class="meta">
                    <span class="language">TypeScript</span> |
                    <span class="stars">3,627 stars</span>
                    | <span class="today">43 stars today</span>
                </p>
                <div class="ai-summary">
                    <h4>Analysis</h4>
                    <p>VibeTunnel is a tool that proxies local terminal sessions into a browser so you can run, monitor, and share shells from any device. It ships as a macOS menu bar app plus a standalone Node.js/TypeScript server and a Lit-based web frontend, with a smart vt wrapper script that forwards terminals (including shell aliases and interactive shells) to the server; sessions can be recorded in asciinema format. Key features include zero-configuration access, Git ‚Äúfollow‚Äù mode, smart keyboard handling, session activity indicators, and multiple remote access options (localhost-only, Tailscale/ngrok/Funnel). The project is optimized for Apple Silicon but also distributes an npm package for Linux and headless systems.</p>
<p>VibeTunnel is useful for developers, SREs, and AI-agent operators who need to check long-running jobs, control terminal-based agents, or share live shells without SSH and complex networking. Its combination of mobile/browser access, session recording, and secure tunnelling (via Tailscale or local-only modes) makes it easy to monitor infrastructure or collaborate from anywhere. The trendiness comes from growing remote work, increased use of AI agents that run in terminals, and demand for low-friction, secure remote access tools that avoid traditional port-forwarding and key management.</p>

                </div>
            </section>

            <section class="repo">
                <h3>9. <a href="https://github.com/steipete/CodexBar" target="_blank">steipete/CodexBar</a></h3>
                <p class="description">Show usage stats for OpenAI Codex and Claude Code, without having to login.</p>
                <p class="meta">
                    <span class="language">Swift</span> |
                    <span class="stars">4,035 stars</span>
                    | <span class="today">99 stars today</span>
                </p>
                <div class="ai-summary">
                    <h4>Analysis</h4>
                    <p>CodexBar is a compact macOS 14+ menu bar app that displays usage and quota statistics for many code‚Äëassistant providers (OpenAI Codex, Anthropic Claude, Gemini, Copilot, and others), showing session and weekly meters with reset countdowns and dynamic icons (or a merged-icons mode). Key features include per‚Äëprovider toggles, refresh cadence presets, optional Codex dashboard enrichments, a WidgetKit snapshot, and a bundled CLI for scripting and Linux CLI builds. Technically, it collects data from multiple local sources‚Äîprovider CLIs, OAuth flows, browser cookies, local JSONL/Claude logs, and Keychain‚Äëstored API tokens‚Äîparses usage on device, and communicates with local probes or CLIs as needed while avoiding unnecessary permissions and not storing passwords. Providers are modular so users enable just the sources they use, with fallbacks (PTY/CLI or browser cookies) to maximize compatibility.</p>
<p>CodexBar is useful for developers, ML engineers, and teams who use multiple AI coding assistants and need a lightweight, always‚Äëvisible way to avoid service interruptions or unexpected costs by tracking consumption in real time. Privacy‚Äëminded users benefit from on‚Äëdevice parsing, Keychain integration, and opt‚Äëin cookie use, while power users can integrate the bundled CLI into scripts and CI. Because AI coding tools and token costs are proliferating, centralizing quota visibility across heterogeneous providers saves time and money, making a compact macOS menu‚Äëbar monitor like CodexBar relevant and increasingly popular.</p>

                </div>
            </section>

            <section class="repo">
                <h3>10. <a href="https://github.com/j178/prek" target="_blank">j178/prek</a></h3>
                <p class="description">‚ö° Better `pre-commit`, re-engineered in Rust</p>
                <p class="meta">
                    <span class="language">Rust</span> |
                    <span class="stars">4,319 stars</span>
                    | <span class="today">61 stars today</span>
                </p>
                <div class="ai-summary">
                    <h4>Analysis</h4>
                    <p>prek is a Rust reimplementation of the pre-commit framework that runs multi-language hooks while managing toolchains and dependencies. It ships as a single, dependency-free binary, aims for drop-in compatibility with pre-commit configs, and includes several Rust-native hooks for speed. Technically, prek reduces disk usage and latency by sharing toolchains between hooks, cloning repositories and installing dependencies in parallel, and delegating Python virtualenv work to uv for fast environment management; it also supports workspace/monorepo mode. The tool adds UX improvements such as directory- and last-commit-scoped runs, hook listing, and safer auto-update options.</p>
<p>prek is valuable to repository maintainers, CI authors, and large monorepo teams that want faster, more efficient pre-commit checks without requiring a local Python runtime. It particularly benefits projects aiming to reduce install time and disk footprint, run hooks concurrently, or adopt a drop-in, higher-performance alternative to pre-commit. Early adoption by projects like CPython, Apache Airflow, and FastAPI demonstrates momentum and real-world suitability, while teams should note some languages may not yet have full parity with the original pre-commit. Overall, prek is well suited for teams seeking quicker developer feedback, leaner tooling, and more efficient CI runs.</p>

                </div>
            </section>

            </div>
        </article>
    </main>
    <footer>
        <p>Generated automatically. Data from <a href="https://github.com/trending">GitHub Trending</a>.</p>
    </footer>
</body>
</html>
