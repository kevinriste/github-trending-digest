<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>GitHub Trending - February 02, 2026</title>
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <header>
        <h1>GitHub Trending Digest - February 02, 2026</h1>
        <nav>
            <a href="../">&larr; GitHub Calendar</a>
            <a href="../hn/">Hacker News</a>
        </nav>
    </header>
    <main>
        <div class="repo-controls">
            <button id="collapse-seen-btn" type="button">Collapse Seen Repos</button>
            <button id="expand-all-btn" type="button">Expand All</button>
        </div>
        <article>
            <div class="repos">

            <section class="repo" data-seen-before="0">
                <div class="repo-header-row">
                    <h3>1. <a href="https://github.com/openclaw/openclaw" target="_blank" rel="noopener noreferrer">openclaw/openclaw</a> </h3>
                    <button type="button" class="repo-toggle" aria-expanded="true">Hide details</button>
                </div>
                <div class="repo-body">
                    <p class="description">Your own personal AI assistant. Any OS. Any Platform. The lobster way. ü¶û</p>
                    <p class="meta">
                        <span class="language">TypeScript</span> |
                        <span class="stars">144,907 stars</span>
                        | <span class="today">10,794 stars today</span>
                    </p>
                    <p class="history">First seen: February 02, 2026 | Consecutive daily streak: 1 day</p>
                    <div class="ai-summary">
                        <h4>Analysis</h4>
                        <p>OpenClaw is a local-first personal AI assistant platform that you run on your own devices to answer and act across the messaging surfaces you already use (WhatsApp, Telegram, Slack, Discord, Google Chat, Signal, iMessage/BlueBubbles, Teams, Matrix, Zalo, WebChat, etc.). Technically it centers on a Gateway control plane (WS API) that coordinates per-agent sessions, a CLI onboarding wizard and daemon, Pi-style agent runtimes, and companion nodes/apps for macOS, iOS and Android. It integrates model auth/failover (Anthropic/OpenAI and others), a media pipeline, browser control, Canvas/A2UI for visual workspaces, and first-class tools (cron, sessions, nodes, actions) with security defaults for DM handling. Installation and development target Node ‚â•22 with npm/pnpm/bun, plus optional Tailscale exposure and declarative Nix/Docker deployment paths.</p>
<p>This project is valuable for privacy-conscious individuals, power users, and teams who want a single always-on assistant that integrates messaging, voice, browser automation, and custom skills while keeping control of data and models. Use cases include personal productivity across channels, automated ops and alerts, multi-agent routing for complex workflows, and local voice/Canvas-driven interactions on desktop and mobile. It‚Äôs trending because it combines open-source extensibility with multi-channel reach and the rising demand for private, long-context AI assistants that can orchestrate tools and devices rather than just chat.</p>
                    </div>
                </div>
            </section>

            <section class="repo" data-seen-before="0">
                <div class="repo-header-row">
                    <h3>2. <a href="https://github.com/ThePrimeagen/99" target="_blank" rel="noopener noreferrer">ThePrimeagen/99</a> </h3>
                    <button type="button" class="repo-toggle" aria-expanded="true">Hide details</button>
                </div>
                <div class="repo-body">
                    <p class="description">Neovim AI agent done right</p>
                    <p class="meta">
                        <span class="language">Lua</span> |
                        <span class="stars">2,812 stars</span>
                        | <span class="today">781 stars today</span>
                    </p>
                    <p class="history">First seen: February 02, 2026 | Consecutive daily streak: 1 day</p>
                    <div class="ai-summary">
                        <h4>Analysis</h4>
                        <p>This repository implements a Neovim-native AI agent designed to provide constrained, rule-driven code assistance inside the editor. It‚Äôs a Lua plugin that integrates with the opencode backend and uses Neovim APIs (keymaps, virtual text, cwd heuristics) plus cmp for completion (skills prefixed with @) to offer actions like fill-in-function, visual-selection edits, and custom rule inclusion via SKILL.md/AGENT.md files. The plugin exposes a small API, configurable logging, and project-level custom rules while currently focusing on TypeScript and Lua support and planning tighter Treesitter/LSP context integration to reduce sent payloads and improve replacements.</p>
<p>This tool is valuable for power users who want AI assistance tightly integrated into a Neovim workflow‚Äîespecially developers working in Lua/TypeScript who prefer in-editor, repeatable transformations, quick debugging scaffolds, and project-specific behavior via rule files. It benefits maintainers and advanced editors who want constrained, auditable AI outputs rather than open-ended chat, and it&#x27;s well-suited for streamers or teams discussing live workflows (hence the Twitch deep-dive). Because it packages model calls, completion, and keybindings into a single extensible plugin and is promoted by a prominent Neovim influencer, it has traction despite being alpha and having known usability caveats.</p>
                    </div>
                </div>
            </section>

            <section class="repo" data-seen-before="0">
                <div class="repo-header-row">
                    <h3>3. <a href="https://github.com/pedramamini/Maestro" target="_blank" rel="noopener noreferrer">pedramamini/Maestro</a> </h3>
                    <button type="button" class="repo-toggle" aria-expanded="true">Hide details</button>
                </div>
                <div class="repo-body">
                    <p class="description">Agent Orchestration Command Center</p>
                    <p class="meta">
                        <span class="language">TypeScript</span> |
                        <span class="stars">1,061 stars</span>
                        | <span class="today">49 stars today</span>
                    </p>
                    <p class="history">First seen: February 02, 2026 | Consecutive daily streak: 1 day</p>
                    <div class="ai-summary">
                        <h4>Analysis</h4>
                        <p>Maestro is a cross-platform desktop app for orchestrating fleets of AI coding agents and projects, offering a keyboard-first interface to run, monitor, and automate parallel agent workflows. Key features include Git worktrees for isolated branch-based agents, Auto Run and filesystem playbooks for batch task execution, multi-agent group chat, mobile remote control via a built-in web server, and a CLI for headless operation. Technically it functions as a pass-through to supported AI providers (Claude Code, OpenAI Codex, OpenCode, Factory Droid), spawning fresh or resumed sessions per task, managing conversation context and workspaces, queuing messages, and integrating with git and optional remote tunneling. It also provides session discovery, token/cost tracking, analytics, and a document graph for visualizing project knowledge.</p>
<p>Maestro is aimed at developers, AI researchers, and power users who need to run multiple agentic workflows in parallel, automate repeatable playbooks, or maintain long-running unattended sessions tied to codebases. Teams benefit from isolated worktree agents that produce PR-ready outputs and from embedding agent runs into cron/CI via the CLI, while solo practitioners gain speed from keyboard-driven controls and rapid context switching. Its analytics, cost tracking, and knowledge-graph features support governance and project visibility, and the pass-through model preserves existing provider permissions and tooling‚Äîattributes that make it relevant in the growing trend of agent orchestration tools.</p>
                    </div>
                </div>
            </section>

            <section class="repo" data-seen-before="0">
                <div class="repo-header-row">
                    <h3>4. <a href="https://github.com/kovidgoyal/calibre" target="_blank" rel="noopener noreferrer">kovidgoyal/calibre</a> </h3>
                    <button type="button" class="repo-toggle" aria-expanded="true">Hide details</button>
                </div>
                <div class="repo-body">
                    <p class="description">The official source code repository for the calibre ebook manager</p>
                    <p class="meta">
                        <span class="language">Python</span> |
                        <span class="stars">23,680 stars</span>
                        | <span class="today">37 stars today</span>
                    </p>
                    <p class="history">First seen: February 02, 2026 | Consecutive daily streak: 1 day</p>
                    <div class="ai-summary">
                        <h4>Analysis</h4>
                        <p>Calibre is an open-source e-book manager that lets users view, convert, edit, and catalog e-books in all major formats, sync with e-reader devices, fetch metadata from the internet, and convert news into e-book form. The repository contains the application source and build instructions for producing platform-specific binaries for Linux, Windows and macOS. Technically it combines a desktop GUI with a conversion/processing engine, device communication drivers, internet metadata/news fetchers and a content server, and exposes extensibility via plugins. The core application is implemented to be portable across platforms (commonly using Python and cross-platform GUI libraries) and includes tooling for building installers.</p>
<p>The project is valuable to individual readers, e-reader owners, librarians, archivists and developers who need a single, flexible tool to manage large digital book collections and to interoperate across devices and formats. It centralizes library organization, automated metadata enrichment, format conversion and device synchronization, reducing friction when moving books between ecosystems. Its open-source nature, extensibility and active maintenance make it a practical choice for users who want control, automation and long-term access to their e-book collections, which helps explain its ongoing popularity.</p>
                    </div>
                </div>
            </section>

            <section class="repo" data-seen-before="0">
                <div class="repo-header-row">
                    <h3>5. <a href="https://github.com/badlogic/pi-mono" target="_blank" rel="noopener noreferrer">badlogic/pi-mono</a> </h3>
                    <button type="button" class="repo-toggle" aria-expanded="true">Hide details</button>
                </div>
                <div class="repo-body">
                    <p class="description">AI agent toolkit: coding agent CLI, unified LLM API, TUI &amp; web UI libraries, Slack bot, vLLM pods</p>
                    <p class="meta">
                        <span class="language">TypeScript</span> |
                        <span class="stars">5,135 stars</span>
                        | <span class="today">613 stars today</span>
                    </p>
                    <p class="history">First seen: February 02, 2026 | Consecutive daily streak: 1 day</p>
                    <div class="ai-summary">
                        <h4>Analysis</h4>
                        <p>This monorepo packages a suite of TypeScript/Node tools for building AI agents and managing LLM deployments. It includes a unified multi-provider LLM API (@mariozechner/pi-ai), an agent runtime with tool-calling and state management (@mariozechner/pi-agent-core), an interactive coding agent CLI, a Slack bot that delegates to the coding agent, terminal and web UI libraries (with differential rendering and web components), and a CLI to manage vLLM deployments on GPU pods. Technically the repo is organized as multiple npm packages with adapters for providers like OpenAI, Anthropic, and Google, an orchestrator core that handles tool invocations and state, and separate UI packages for different frontends; standard npm scripts handle build, checks, and tests (LLM tests are skipped without API keys).</p>
<p>This project is useful for developers, platform engineers, and research teams who want to prototype, deploy, and integrate LLM-powered agents‚Äîespecially coding assistants‚Äîacross CLI, Slack, web, and terminal interfaces. It lowers friction for multi-provider experimentation and for running private or on-prem vLLM deployments by providing unified APIs and pod management tooling. The modular agent runtime and reusable UI components accelerate development and experimentation, and the toolkit is timely given strong demand for composable LLM tooling, multi-vendor support, and easier production deployment paths.</p>
                    </div>
                </div>
            </section>

            <section class="repo" data-seen-before="0">
                <div class="repo-header-row">
                    <h3>6. <a href="https://github.com/thedotmack/claude-mem" target="_blank" rel="noopener noreferrer">thedotmack/claude-mem</a> </h3>
                    <button type="button" class="repo-toggle" aria-expanded="true">Hide details</button>
                </div>
                <div class="repo-body">
                    <p class="description">A Claude Code plugin that automatically captures everything Claude does during your coding sessions, compresses it with AI (using Claude&#x27;s agent-sdk), and injects relevant context back into future sessions.</p>
                    <p class="meta">
                        <span class="language">TypeScript</span> |
                        <span class="stars">16,759 stars</span>
                        | <span class="today">196 stars today</span>
                    </p>
                    <p class="history">First seen: February 02, 2026 | Consecutive daily streak: 1 day</p>
                    <div class="ai-summary">
                        <h4>Analysis</h4>
                        <p>Claude-Mem is a Claude Code plugin that automatically captures Claude‚Äôs tool usage and observations during coding sessions, compresses them with the Claude agent-sdk, stores them in a local database, and injects relevant context into future sessions. Technically it uses lifecycle hook scripts to intercept session events, a Bun-managed worker HTTP service (default port 37777) with a web viewer, SQLite (with FTS5) for persistence, and a Chroma vector DB for hybrid semantic search. Retrieval is token-efficient via MCP tools (search ‚Üí timeline ‚Üí get_observations) implementing progressive disclosure, and the system includes configuration, privacy exclusion tags, citationable observation IDs, and a beta channel for experimental features like Endless Mode.</p>
<p>This tool benefits individual developers and teams who need continuity across ephemeral AI sessions by reducing repetitive context copying, accelerating debugging and onboarding, and preserving project knowledge and decision history. Its token-aware progressive retrieval and hybrid search make it cost-efficient for large histories, while local storage and privacy controls suit sensitive codebases. Because it integrates directly into Claude Code and leverages the agent SDK for automated summarization and injection, it‚Äôs attractive to users looking for persistent, reproducible AI-assisted coding workflows.</p>
                    </div>
                </div>
            </section>

            <section class="repo" data-seen-before="0">
                <div class="repo-header-row">
                    <h3>7. <a href="https://github.com/microsoft/agent-lightning" target="_blank" rel="noopener noreferrer">microsoft/agent-lightning</a> </h3>
                    <button type="button" class="repo-toggle" aria-expanded="true">Hide details</button>
                </div>
                <div class="repo-body">
                    <p class="description">The absolute trainer to light up AI agents.</p>
                    <p class="meta">
                        <span class="language">Python</span> |
                        <span class="stars">13,066 stars</span>
                        | <span class="today">406 stars today</span>
                    </p>
                    <p class="history">First seen: February 02, 2026 | Consecutive daily streak: 1 day</p>
                    <div class="ai-summary">
                        <h4>Analysis</h4>
                        <p>Agent Lightning is a lightweight training framework that enables optimization of AI agents with minimal or no code changes across any agent framework (LangChain, OpenAI Agent SDK, AutoGen, CrewAI, Microsoft Agent Framework) or plain Python OpenAI clients. It supports selective optimization in multi-agent systems and integrates algorithms like reinforcement learning, automatic prompt optimization, and supervised fine‚Äëtuning. The system instruments agents via an agl.emit_xxx() helper or an automated tracer to capture prompts, tool calls, and rewards as structured spans that feed into a central LightningStore. Algorithms consume those spans and publish updated resources (refined prompt templates, policy weights) while a Trainer streams datasets to runners, synchronizes resources, and updates the inference engine to close the learning loop.</p>
<p>This project is useful for researchers, ML engineers, and product teams who need to iterate on agent behavior, tune tool-using or multi-agent systems, and run scalable RL experiments without rewriting existing agent code. Typical use cases include prompt and policy optimization for production agents, multi-agent coordination and verification workflows, and large-scale RL training demonstrated in community case studies. Its framework-agnostic architecture, trace-driven data model, and Microsoft research backing make it attractive for rapid experimentation, reproducible pipelines, and teams aiming for stable, scalable agent improvement.</p>
                    </div>
                </div>
            </section>

            <section class="repo" data-seen-before="0">
                <div class="repo-header-row">
                    <h3>8. <a href="https://github.com/amantus-ai/vibetunnel" target="_blank" rel="noopener noreferrer">amantus-ai/vibetunnel</a> </h3>
                    <button type="button" class="repo-toggle" aria-expanded="true">Hide details</button>
                </div>
                <div class="repo-body">
                    <p class="description">Turn any browser into your terminal &amp; command your agents on the go.</p>
                    <p class="meta">
                        <span class="language">TypeScript</span> |
                        <span class="stars">3,627 stars</span>
                        | <span class="today">43 stars today</span>
                    </p>
                    <p class="history">First seen: February 02, 2026 | Consecutive daily streak: 1 day</p>
                    <div class="ai-summary">
                        <h4>Analysis</h4>
                        <p>VibeTunnel is a tool that proxies local terminal sessions into a browser so you can run, monitor, and share shells from any device. It ships as a macOS menu bar app plus a standalone Node.js/TypeScript server and a Lit-based web frontend, with a smart vt wrapper script that forwards terminals (including shell aliases and interactive shells) to the server; sessions can be recorded in asciinema format. Key features include zero-configuration access, Git ‚Äúfollow‚Äù mode, smart keyboard handling, session activity indicators, and multiple remote access options (localhost-only, Tailscale/ngrok/Funnel). The project is optimized for Apple Silicon but also distributes an npm package for Linux and headless systems.</p>
<p>VibeTunnel is useful for developers, SREs, and AI-agent operators who need to check long-running jobs, control terminal-based agents, or share live shells without SSH and complex networking. Its combination of mobile/browser access, session recording, and secure tunnelling (via Tailscale or local-only modes) makes it easy to monitor infrastructure or collaborate from anywhere. The trendiness comes from growing remote work, increased use of AI agents that run in terminals, and demand for low-friction, secure remote access tools that avoid traditional port-forwarding and key management.</p>
                    </div>
                </div>
            </section>

            <section class="repo" data-seen-before="0">
                <div class="repo-header-row">
                    <h3>9. <a href="https://github.com/steipete/CodexBar" target="_blank" rel="noopener noreferrer">steipete/CodexBar</a> </h3>
                    <button type="button" class="repo-toggle" aria-expanded="true">Hide details</button>
                </div>
                <div class="repo-body">
                    <p class="description">Show usage stats for OpenAI Codex and Claude Code, without having to login.</p>
                    <p class="meta">
                        <span class="language">Swift</span> |
                        <span class="stars">4,035 stars</span>
                        | <span class="today">99 stars today</span>
                    </p>
                    <p class="history">First seen: February 02, 2026 | Consecutive daily streak: 1 day</p>
                    <div class="ai-summary">
                        <h4>Analysis</h4>
                        <p>CodexBar is a compact macOS 14+ menu bar app that displays usage and quota statistics for many code‚Äëassistant providers (OpenAI Codex, Anthropic Claude, Gemini, Copilot, and others), showing session and weekly meters with reset countdowns and dynamic icons (or a merged-icons mode). Key features include per‚Äëprovider toggles, refresh cadence presets, optional Codex dashboard enrichments, a WidgetKit snapshot, and a bundled CLI for scripting and Linux CLI builds. Technically, it collects data from multiple local sources‚Äîprovider CLIs, OAuth flows, browser cookies, local JSONL/Claude logs, and Keychain‚Äëstored API tokens‚Äîparses usage on device, and communicates with local probes or CLIs as needed while avoiding unnecessary permissions and not storing passwords. Providers are modular so users enable just the sources they use, with fallbacks (PTY/CLI or browser cookies) to maximize compatibility.</p>
<p>CodexBar is useful for developers, ML engineers, and teams who use multiple AI coding assistants and need a lightweight, always‚Äëvisible way to avoid service interruptions or unexpected costs by tracking consumption in real time. Privacy‚Äëminded users benefit from on‚Äëdevice parsing, Keychain integration, and opt‚Äëin cookie use, while power users can integrate the bundled CLI into scripts and CI. Because AI coding tools and token costs are proliferating, centralizing quota visibility across heterogeneous providers saves time and money, making a compact macOS menu‚Äëbar monitor like CodexBar relevant and increasingly popular.</p>
                    </div>
                </div>
            </section>

            <section class="repo" data-seen-before="0">
                <div class="repo-header-row">
                    <h3>10. <a href="https://github.com/j178/prek" target="_blank" rel="noopener noreferrer">j178/prek</a> </h3>
                    <button type="button" class="repo-toggle" aria-expanded="true">Hide details</button>
                </div>
                <div class="repo-body">
                    <p class="description">‚ö° Better `pre-commit`, re-engineered in Rust</p>
                    <p class="meta">
                        <span class="language">Rust</span> |
                        <span class="stars">4,319 stars</span>
                        | <span class="today">61 stars today</span>
                    </p>
                    <p class="history">First seen: February 02, 2026 | Consecutive daily streak: 1 day</p>
                    <div class="ai-summary">
                        <h4>Analysis</h4>
                        <p>prek is a Rust reimplementation of the pre-commit framework that runs and manages multi-language hooks as a single standalone binary without requiring Python or other runtimes. Its main features include faster execution, reduced disk usage, full compatibility with pre-commit configurations, workspace/monorepo support, built-in Rust implementations of common hooks, and shared toolchain installation for Python, Node.js, Bun, Go, Rust, and Ruby. Technically, prek centrally manages toolchains and dependencies (leveraging uv for Python virtualenvs), clones repositories and installs dependencies in parallel, shares environments between hooks, and executes hooks concurrently by priority to minimize end-to-end runtime.</p>
<p>prek is valuable to developers, CI/CD pipelines, and maintainers who want a drop-in, faster alternative to pre-commit with minimal runtime friction. Large codebases and monorepos, or projects that run many heterogeneous hooks, benefit most from shared toolchains, parallel installs, and Rust-native hooks that reduce CI time and disk use. Its adoption by projects such as CPython, Apache Airflow, and FastAPI highlights practical gains in performance and ease of integration, while the single-binary distribution and multiple install options simplify onboarding and automated workflows.</p>
                    </div>
                </div>
            </section>

            </div>
        </article>
    </main>
    <footer>
        <p>Generated automatically. Data from <a href="https://github.com/trending">GitHub Trending</a>.</p>
    </footer>

<script>
(() => {
    const readKey = "gtd:read_days:gh:v1";
    const dayStr = "2026-02-02";

    let stored = [];
    try {
        stored = JSON.parse(localStorage.getItem(readKey) || "[]");
        if (!Array.isArray(stored)) {
            stored = [];
        }
    } catch (_err) {
        stored = [];
    }

    if (!stored.includes(dayStr)) {
        stored.push(dayStr);
        stored.sort();
        localStorage.setItem(readKey, JSON.stringify(stored));
    }

    const collapseParam = new URLSearchParams(window.location.search).get("collapse_seen");
    const collapseSeen = collapseParam === "0" ? false : true;

    function setCollapsed(repoEl, collapsed) {
        repoEl.classList.toggle("collapsed", collapsed);
        const button = repoEl.querySelector(".repo-toggle");
        if (button) {
            button.textContent = collapsed ? "Show details" : "Hide details";
            button.setAttribute("aria-expanded", String(!collapsed));
        }
    }

    const repos = Array.from(document.querySelectorAll("section.repo[data-seen-before]"));

    repos.forEach((repoEl) => {
        const toggle = repoEl.querySelector(".repo-toggle");
        if (!toggle) {
            return;
        }

        toggle.addEventListener("click", () => {
            setCollapsed(repoEl, !repoEl.classList.contains("collapsed"));
        });
    });

    const collapseBtn = document.getElementById("collapse-seen-btn");
    const expandBtn = document.getElementById("expand-all-btn");

    if (collapseBtn) {
        collapseBtn.addEventListener("click", () => {
            repos.forEach((repoEl) => {
                if (repoEl.dataset.seenBefore === "1") {
                    setCollapsed(repoEl, true);
                }
            });
        });
    }

    if (expandBtn) {
        expandBtn.addEventListener("click", () => {
            repos.forEach((repoEl) => setCollapsed(repoEl, false));
        });
    }

    if (collapseSeen) {
        repos.forEach((repoEl) => {
            if (repoEl.dataset.seenBefore === "1") {
                setCollapsed(repoEl, true);
            }
        });
    }
})();
</script>

</body>
</html>
