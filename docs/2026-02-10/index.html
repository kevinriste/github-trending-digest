<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>GitHub Trending - February 10, 2026</title>
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <header>
        <h1>GitHub Trending Digest - February 10, 2026</h1>
        <nav>
            <a href="../">&larr; Back to Calendar</a>
        </nav>
    </header>
    <main>
        <article>
            <div class="repos">

            <section class="repo">
                <h3>1. <a href="https://github.com/KeygraphHQ/shannon" target="_blank">KeygraphHQ/shannon</a></h3>
                <p class="description">Fully autonomous AI hacker to find actual exploits in your web apps. Shannon has achieved a 96.15% success rate on the hint-free, source-aware XBOW Benchmark.</p>
                <p class="meta">
                    <span class="language">TypeScript</span> |
                    <span class="stars">17,948 stars</span>
                    | <span class="today">4,144 stars today</span>
                </p>
                <div class="ai-summary">
                    <h4>Analysis</h4>
                    <p>Shannon is a fully autonomous AI pentester that ingests white‚Äëbox source code, hunts for attack vectors, and executes live exploits (browser and CLI) to produce reproducible proofs‚Äëof‚Äëconcept. Its main features include automated authentication handling (2FA/TOTP, OAuth), coverage of critical OWASP classes (injection, XSS, SSRF, broken auth/authorization), parallelized workflows, and integrated reconnaissance tools like Nmap, Subfinder, WhatWeb, and Schemathesis. Technically it runs in Docker, leverages LLM providers (Anthropic/Claude) to analyze code and guide attacks, exposes workflow monitoring (Temporal Web UI), and produces pentester‚Äëgrade reports with copy‚Äëpaste PoCs. This repository contains Shannon Lite (AGPL‚Äë3.0) for white‚Äëbox testing; Shannon Pro extends the core with an enterprise LLM dataflow analysis engine and CI/CD integrations.</p>
<p>Shannon benefits security teams, DevOps, and independent researchers who need continuous, repeatable, on‚Äëdemand pentesting to close the gap between rapid shipping and infrequent manual tests. By delivering verified exploits and detailed, reproducible reports it reduces false positives, speeds triage and remediation, and can be integrated into compliance and CI/CD workflows for audit readiness. Its traction is driven by practical LLM‚Äëguided automation, demonstrated benchmark performance (96.15% on an XBOW benchmark), and the industry shift toward automated, continuous application security validation.</p>

                </div>
            </section>

            <section class="repo">
                <h3>2. <a href="https://github.com/virattt/dexter" target="_blank">virattt/dexter</a></h3>
                <p class="description">An autonomous agent for deep financial research</p>
                <p class="meta">
                    <span class="language">TypeScript</span> |
                    <span class="stars">13,697 stars</span>
                    | <span class="today">1,115 stars today</span>
                </p>
                <div class="ai-summary">
                    <h4>Analysis</h4>
                    <p>Dexter is an autonomous financial research agent that converts complex finance questions into structured, step-by-step research plans, executes those steps using live market and financial datasets, and iterates via self-validation until it produces a confident, data-backed answer. Key features include intelligent task planning, autonomous tool selection and execution, self-reflection and error checking, and safety mechanisms such as loop detection and step limits. Technically it runs on the Bun runtime, integrates with LLM providers (OpenAI, Anthropic, XAI, OpenRouter, or local Ollama), pulls institutional-grade data via a Financial Datasets API and optional web search (Exa/Tavily), and logs every action to newline-delimited JSON scratchpads for traceability. An evaluation suite using LangSmith and an LLM-as-judge enables benchmarking on a dataset of financial questions.</p>
<p>This project is useful for sell-side and buy-side analysts, quantitative researchers, fintech engineers, and anyone needing reproducible, fast financial due diligence or idea vetting. It automates tedious data collection and initial analysis, provides auditable scratchpad logs and an eval pipeline for continuous improvement, and is extensible to different LLMs and data sources. Its traction comes from combining domain-specific tooling, live data access, and autonomous agent patterns that accelerate high-stakes research workflows while maintaining safety and traceability.</p>

                </div>
            </section>

            <section class="repo">
                <h3>3. <a href="https://github.com/pydantic/monty" target="_blank">pydantic/monty</a></h3>
                <p class="description">A minimal, secure Python interpreter written in Rust for use by AI</p>
                <p class="meta">
                    <span class="language">Rust</span> |
                    <span class="stars">4,096 stars</span>
                    | <span class="today">1,291 stars today</span>
                </p>
                <div class="ai-summary">
                    <h4>Analysis</h4>
                    <p>Monty is a minimal, secure Python interpreter implemented in Rust designed for running LLM-generated code inside agents. It executes a restricted but useful subset of Python, enforces complete isolation of host resources (filesystem, env, network) unless you expose specific external functions, supports async/sync calls, type checking, stdout/stderr capture, and resource limits (memory, stack, time). Technically it embeds a custom Python runtime with no CPython dependency, is serializable (can snapshot parsed code and in-flight execution to bytes), and can be called from Rust, Python, or JavaScript with microsecond-scale startup and comparable runtime performance to CPython. It intentionally omits most of the stdlib and third-party libraries and currently lacks some language features (classes, match) while under active development.</p>
<p>Monty is valuable to teams building AI agents, tool-call frameworks, and code-mode workflows that need low-latency, safe execution of model-written code without the overhead and risk of container sandboxes. Developers who want deterministic, auditable interactions between LLMs and host functions‚Äîplus the ability to cache and resume interpreter state across processes‚Äîwill benefit most. Its momentum stems from practical demand for faster, simpler, and safer in-process code execution for agents (e.g., Pydantic AI, Anthropic-style tool calling, and Cloudflare codemode), filling a gap between unsafe direct execution and heavyweight sandboxing.</p>

                </div>
            </section>

            <section class="repo">
                <h3>4. <a href="https://github.com/hsliuping/TradingAgents-CN" target="_blank">hsliuping/TradingAgents-CN</a></h3>
                <p class="description">Âü∫‰∫éÂ§öÊô∫ËÉΩ‰ΩìLLMÁöÑ‰∏≠ÊñáÈáëËûç‰∫§ÊòìÊ°ÜÊû∂ - TradingAgents‰∏≠ÊñáÂ¢ûÂº∫Áâà</p>
                <p class="meta">
                    <span class="language">Python</span> |
                    <span class="stars">16,390 stars</span>
                    | <span class="today">149 stars today</span>
                </p>
                <div class="ai-summary">
                    <h4>Analysis</h4>
                    <p>This repository is a Chinese-enhanced fork of the TradingAgents multi‚Äëagent LLM framework for financial stock analysis and learning. It bundles multi‚ÄëLLM orchestration (OpenAI, Google, domestic models, etc.), intelligent news analysis, stock screening, simulated trading, professional report export, and real‚Äëtime progress via SSE/WebSocket. Technically it is built as a FastAPI backend with a Vue 3 frontend, uses MongoDB + Redis for persistence and caching, supports multiple data sources (Tushare, AkShare, BaoStock), and is distributed with Docker multi‚Äëarchitecture images so agents and model selection components can be deployed and coordinated via RESTful APIs and streams.</p>
<p>The project is valuable for researchers, educators, quant analysts, developers and students who want a Chinese‚Äëlocalized, hands‚Äëon platform to prototype LLM‚Äëdriven stock research, backtest strategies in a simulated environment, and generate reproducible analysis reports. Its support for multiple LLM providers, modular agent design, and containerized deployment make it well suited for experimentation, teaching, and rapid prototyping in Chinese markets. Note that while most of the repo is Apache‚Äë2.0 open source, the app/ and frontend/ directories are marked proprietary and require commercial authorization for commercial use, so it is best for non‚Äëcommercial research and internal evaluation unless licensed.</p>

                </div>
            </section>

            <section class="repo">
                <h3>5. <a href="https://github.com/iOfficeAI/AionUi" target="_blank">iOfficeAI/AionUi</a></h3>
                <p class="description">Free, local, open-source 24/7 Cowork and OpenClaw for Gemini CLI, Claude Code, Codex, OpenCode, Qwen Code, Goose CLI, Auggie, and more | üåü Star if you like it!</p>
                <p class="meta">
                    <span class="language">TypeScript</span> |
                    <span class="stars">13,985 stars</span>
                    | <span class="today">673 stars today</span>
                </p>
                <div class="ai-summary">
                    <h4>Analysis</h4>
                    <p>AionUi is an open-source multi-agent GUI and WebUI that wraps and unifies command-line AI tools (built‚Äëin Gemini CLI plus autodetected CLIs like Claude Code, Codex, Qwen Code, Goose, etc.) into a single desktop/Web interface. Its main features include automatic detection of local CLIs, multi-session local conversation storage, multi-model support (cloud and local models such as Gemini, OpenAI, Claude, Ollama, LM Studio), scheduled tasks for unattended automation, smart file management (batch rename, auto-classify, merge), real-time preview for 9+ file formats, and an extensible assistants/skills system (defined in assistant/ and skills/). Technically it works by wrapping CLI tools and exposing them through a unified GUI/WebUI and chat-channel integrations (WebUI, Telegram, Feishu/Lark, with Slack planned), persisting context and files locally for privacy, and allowing customization via CSS and skill definition files.</p>
<p>This project is valuable to developers, data analysts, automation engineers, and power users who want a cross‚Äëplatform, privacy‚Äëpreserving alternative to paid, model‚Äëlocked cowork platforms‚Äîespecially those who run multiple local or cloud models and command‚Äëline tools. Use cases include 24/7 autonomous agents for scheduled reports or maintenance, batch document and code processing, PPT/Word/XLSX generation, file system automation and previews, and multi‚Äëtool research workflows. Its trendiness stems from being a free, cross‚Äëplatform, extensible Cowork replacement that consolidates many CLIs and models into one workflow while keeping data local and enabling remote access.</p>

                </div>
            </section>

            <section class="repo">
                <h3>6. <a href="https://github.com/public-apis/public-apis" target="_blank">public-apis/public-apis</a></h3>
                <p class="description">A collective list of free APIs</p>
                <p class="meta">
                    <span class="language">Python</span> |
                    <span class="stars">397,364 stars</span>
                    | <span class="today">410 stars today</span>
                </p>
                <div class="ai-summary">
                    <h4>Analysis</h4>
                    <p>This repository is a community-maintained, manually curated directory of public APIs, organized into categories and presented primarily as Markdown tables in the README. Its main features include extensive categorized listings (Animals, Anime, Anti‚ÄëMalware, etc.), metadata for each API such as authentication type, HTTPS and CORS support, short descriptions, and links; the project also provides a contributing guide and an API endpoint for programmatic access. Technically it is hosted on GitHub and maintained via issues and pull requests, with human-readable tables that are often mirrored or consumed in machine-readable forms so developers can search, filter, or integrate the catalogue into tools and workflows.</p>
<p>The project is valuable to developers, product teams, educators, and hobbyists who need a fast way to discover and compare free APIs for prototyping, production features, or learning. Because it is community-curated and regularly updated, it reduces time spent searching for services and surfaces alternatives across many domains while showing key integration details up front. Its breadth, ease of contribution, and utility as a single reference point explain its popularity and why it‚Äôs commonly used in hackathons, startups, and other API-driven projects.</p>

                </div>
            </section>

            <section class="repo">
                <h3>7. <a href="https://github.com/github/gh-aw" target="_blank">github/gh-aw</a></h3>
                <p class="description">GitHub Agentic Workflows</p>
                <p class="meta">
                    <span class="language">Go</span> |
                    <span class="stars">1,015 stars</span>
                    | <span class="today">260 stars today</span>
                </p>
                <div class="ai-summary">
                    <h4>Analysis</h4>
                    <p>GitHub Agentic Workflows lets developers write agentic workflows in natural-language Markdown and run them inside GitHub Actions. It provides features such as read-only-by-default execution, sanitized safe-outputs for write operations, sandboxed execution, network isolation, SHA-pinned dependencies, tool allow-listing, and compile-time validation to enforce safety. Technically, workflows are interpreted and executed within the GitHub Actions runtime with layered guardrails, optional human approval gates, and companion services (Agent Workflow Firewall for egress control and MCP Gateway for model calls) to centralize and control agent behavior.</p>
<p>The project is valuable for repository maintainers, automation and DevOps teams, and security-conscious organizations that want to safely apply AI to routine tasks like dependency updates, issue triage, release notes, test generation, and CI/CD automation. By combining agentic natural-language workflows with strict security controls and auditing, it reduces manual work while keeping supply-chain and network exposure limited. Its integration with GitHub Actions and emphasis on guardrails explains its appeal and why it is trending as teams seek practical, governed ways to scale AI-driven developer workflows.</p>

                </div>
            </section>

            <section class="repo">
                <h3>8. <a href="https://github.com/Shubhamsaboo/awesome-llm-apps" target="_blank">Shubhamsaboo/awesome-llm-apps</a></h3>
                <p class="description">Collection of awesome LLM apps with AI Agents and RAG using OpenAI, Anthropic, Gemini and opensource models.</p>
                <p class="meta">
                    <span class="language">Python</span> |
                    <span class="stars">93,264 stars</span>
                    | <span class="today">227 stars today</span>
                </p>
                <div class="ai-summary">
                    <h4>Analysis</h4>
                    <p>This repository is a curated collection of runnable LLM applications and example projects that demonstrate Retrieval-Augmented Generation (RAG), AI agents, multi-agent teams, MCP (multi-context processing), voice agents, memory, fine-tuning, and cost/response optimization techniques. It aggregates starter and advanced agents, autonomous game-playing agents, voice and MCP agents, RAG patterns, memory-enabled apps, &quot;chat with X&quot; integrations, and tools for fine-tuning and token/context optimization. Technically, the projects show how to combine embeddings, vector stores, agent orchestration, tool/function calling, multimodal inputs, and model-agnostic stacks (OpenAI, Anthropic, Gemini, xAI, Qwen, Llama) for both cloud and local deployment, with structured outputs and developer-focused tutorials. The repo includes setup instructions, sample code, and recommendations for reducing API cost and improving context handling (e.g., Toonify, Headroom).</p>
<p>This collection is valuable to developers, researchers, AI product teams, and educators who want practical, hands-on examples to prototype, learn, and deploy LLM-driven solutions across domains like search, customer support, analytics, and automation. It benefits anyone building agentic or RAG-enabled systems by providing vetted patterns, integration examples with major LLM providers and open-source models, and guides for scaling, memory, and fine-tuning. The repo is trending because it consolidates modern agent and RAG best practices into runnable projects, demonstrates multi-vendor interoperability, and lowers the barrier to experimentation with emergent agentic and multimodal LLM capabilities.</p>

                </div>
            </section>

            <section class="repo">
                <h3>9. <a href="https://github.com/gitbutlerapp/gitbutler" target="_blank">gitbutlerapp/gitbutler</a></h3>
                <p class="description">The GitButler version control client, backed by Git, powered by Tauri/Rust/Svelte</p>
                <p class="meta">
                    <span class="language">Rust</span> |
                    <span class="stars">18,697 stars</span>
                    | <span class="today">376 stars today</span>
                </p>
                <div class="ai-summary">
                    <h4>Analysis</h4>
                    <p>GitButler is a Git-based version control client that provides a friendlier, more powerful interface for working with existing Git repositories. Its core features include stacked and parallel branches, easy commit mutations (uncommit, reword, amend, move, split, squash), an undo timeline that logs and reverts operations, first-class conflict handling, forge integrations (GitHub/GitLab), and built-in AI tooling for commit messages and PR content. The desktop app is built with Tauri (Svelte + TypeScript UI) and a Rust backend; the same Rust engine powers the &quot;but&quot; CLI so the tool can be used as a drop-in UI replacement or headless automation layer.</p>
<p>GitButler is valuable for individual developers, teams, maintainers, and automation/agent workflows that need safer, faster Git operations without complex rebases or frequent branch switching. It reduces friction for experiments and parallel workstreams, makes resolving and undoing changes straightforward, and streamlines pull request and CI interactions through forge integration and AI helpers. The project‚Äôs modern tech stack and agent-friendly features help explain its traction, while its Fair Source license (with an eventual MIT reversion) balances open contribution and competitive protection.</p>

                </div>
            </section>

            <section class="repo">
                <h3>10. <a href="https://github.com/microsoft/litebox" target="_blank">microsoft/litebox</a></h3>
                <p class="description">A security-focused library OS supporting kernel- and user-mode execution</p>
                <p class="meta">
                    <span class="language">Rust</span> |
                    <span class="stars">1,765 stars</span>
                    | <span class="today">354 stars today</span>
                </p>
                <div class="ai-summary">
                    <h4>Analysis</h4>
                    <p>LiteBox is a sandboxing library OS that minimizes the host interface to reduce attack surface while enabling both kernel- and user-mode execution. It provides a Rust-y, nix/rustix-inspired &quot;North&quot; API that is paired with pluggable &quot;South&quot; Platform backends, allowing a wide variety of North‚ÄìSouth combinations and easy interoperation of shims and platforms. Technically, the project is modular: the North surface exposes POSIX-like primitives to runtimes and applications, and the South adapters implement those primitives against different hosts (Windows, Linux, SEV SNP, OP-TEE, LVBS, etc.), enabling unmodified binaries or constrained workloads to run across environments.</p>
<p>The project is valuable for security-focused system and platform developers, cloud and virtualization engineers, and anyone needing portable sandboxing or compatibility layers (for example, running Linux programs on Windows or isolating Linux workloads). Its pluggable architecture and focus on minimizing host interfaces make it attractive for mitigating attack surface in sensitive deployments, integrating with enclave technologies, and experimenting with alternative runtime models. Growing interest in Rust-based systems, supply-chain security, and hardware-backed isolation helps explain why a flexible library OS like LiteBox is gaining attention.</p>

                </div>
            </section>

            </div>
        </article>
    </main>
    <footer>
        <p>Generated automatically. Data from <a href="https://github.com/trending">GitHub Trending</a>.</p>
    </footer>
</body>
</html>
