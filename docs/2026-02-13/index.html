<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>GitHub Trending - February 13, 2026</title>
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <header>
        <h1>GitHub Trending Digest - February 13, 2026</h1>
        <nav>
            <a href="../">&larr; GitHub Calendar</a>
            <a href="../hn/">Hacker News</a>
        </nav>
    </header>
    <main>
        <div class="repo-controls">
            <button id="collapse-seen-btn" type="button">Collapse Repos Not New Today</button>
            <button id="expand-all-btn" type="button">Expand All</button>
        </div>
        <p class="seen-help">Repos marked "Not new today" appeared on one or more previous daily pages.</p>
        <article>
            <div class="repos">

            <section class="repo" data-seen-before="0">
                <div class="repo-header-row">
                    <h3>1. <a href="https://github.com/tambo-ai/tambo" target="_blank" rel="noopener noreferrer">tambo-ai/tambo</a> </h3>
                    <button type="button" class="repo-toggle" aria-expanded="true">Hide details</button>
                </div>
                <div class="repo-body">
                    <p class="description">Generative UI SDK for React</p>
                    <p class="meta">
                        <span class="language">TypeScript</span> |
                        <span class="stars">9,166 stars</span>
                        | <span class="today">300 stars today</span>
                    </p>
                    <p class="history">First seen: February 13, 2026 | Consecutive daily streak: 1 day</p>
                    <div class="ai-summary">
                        <h4>Analysis</h4>
                        <p>Tambo is a React toolkit for building generative UIs where an LLM-driven agent decides which registered components to render and streams props to them in real time. Developers register components with Zod schemas that become LLM tool definitions; the included agent runs the conversation loop, selects components, and streams updates with built-in cancellation, error recovery, and reconnection. The project ships as a fullstack solution (Tambo Cloud or self-hosted via Docker) with provider configuration, hooks (useTambo, useTamboThreadInput), local browser-executable tools, and MCP integrations, and it supports multiple LLM providers or agent frameworks.</p>
<p>Tambo is useful for frontend teams and product engineers who want adaptive, conversational interfaces‚Äîexamples include AI-driven analytics dashboards, interactive charts, chat interfaces that spawn components, shopping carts, and task boards‚Äîbecause it automates component selection and incremental prop streaming. It accelerates prototyping and production by providing persistent, stateful interactable components, schema-driven safety for LLM calls, and flexible hosting options for enterprise needs. Its traction reflects growing demand for richer, real-time AI frontends and the convenience of removing manual tool-to-component wiring while supporting multiple LLM ecosystems.</p>
                    </div>
                </div>
            </section>

            <section class="repo" data-seen-before="0">
                <div class="repo-header-row">
                    <h3>2. <a href="https://github.com/danielmiessler/Personal_AI_Infrastructure" target="_blank" rel="noopener noreferrer">danielmiessler/Personal_AI_Infrastructure</a> </h3>
                    <button type="button" class="repo-toggle" aria-expanded="true">Hide details</button>
                </div>
                <div class="repo-body">
                    <p class="description">Agentic AI Infrastructure for magnifying HUMAN capabilities.</p>
                    <p class="meta">
                        <span class="language">TypeScript</span> |
                        <span class="stars">7,642 stars</span>
                        | <span class="today">351 stars today</span>
                    </p>
                    <p class="history">First seen: February 13, 2026 | Consecutive daily streak: 1 day</p>
                    <div class="ai-summary">
                        <h4>Analysis</h4>
                        <p>Summary not available.</p>
                    </div>
                </div>
            </section>

            <section class="repo" data-seen-before="1">
                <div class="repo-header-row">
                    <h3>3. <a href="https://github.com/google/langextract" target="_blank" rel="noopener noreferrer">google/langextract</a> <span class="seen-badge">Not new today</span></h3>
                    <button type="button" class="repo-toggle" aria-expanded="true">Hide details</button>
                </div>
                <div class="repo-body">
                    <p class="description">A Python library for extracting structured information from unstructured text using LLMs with precise source grounding and interactive visualization.</p>
                    <p class="meta">
                        <span class="language">Python</span> |
                        <span class="stars">31,570 stars</span>
                        | <span class="today">1,122 stars today</span>
                    </p>
                    <p class="history">First seen: February 09, 2026 | Consecutive daily streak: 3 days</p>
                    <div class="ai-summary">
                        <h4>Analysis</h4>
                        <p>LangExtract is a Python library that uses large language models to extract structured information from unstructured text while precisely grounding each extraction to its original source location. Key features include few‚Äëshot, prompt-driven extraction schemas, controlled generation for robust structured outputs (with support for models like Google Gemini and local LLMs via Ollama), optimized handling of long documents through chunking, parallel/multi‚Äëpass extraction to boost recall, and self‚Äëcontained interactive HTML visualizations alongside JSONL outputs for review. Technically, users provide a prompt and example extractions, the library divides texts into context windows, runs model calls (optionally in parallel or batch), aligns results to source offsets, and assembles validated, schema‚Äëconstrained outputs for downstream use.</p>
<p>This tooling is valuable for teams that need reliable, auditable structure from noisy text‚Äîclinical documentation (e.g., radiology/medication extraction), legal/contract review, research literature mining, content moderation, and annotation pipelines‚Äîbecause it combines scale with traceability and reviewer-friendly visualization. Organizations benefit from faster, consistent data extraction without model fine‚Äëtuning, and the project‚Äôs support for cloud and local providers, batch modes, and emphasis on explainability aligns with current demands for practical, controllable LLM applications, making it a timely and widely applicable solution.</p>
                    </div>
                </div>
            </section>

            <section class="repo" data-seen-before="1">
                <div class="repo-header-row">
                    <h3>4. <a href="https://github.com/ChromeDevTools/chrome-devtools-mcp" target="_blank" rel="noopener noreferrer">ChromeDevTools/chrome-devtools-mcp</a> <span class="seen-badge">Not new today</span></h3>
                    <button type="button" class="repo-toggle" aria-expanded="true">Hide details</button>
                </div>
                <div class="repo-body">
                    <p class="description">Chrome DevTools for coding agents</p>
                    <p class="meta">
                        <span class="language">TypeScript</span> |
                        <span class="stars">24,471 stars</span>
                        | <span class="today">436 stars today</span>
                    </p>
                    <p class="history">First seen: February 12, 2026 | Consecutive daily streak: 2 days</p>
                    <div class="ai-summary">
                        <h4>Analysis</h4>
                        <p>Chrome DevTools MCP is an MCP (Model‚ÄëContext‚ÄëProtocol) server that lets AI coding agents (e.g., Gemini, Claude, Cursor, Copilot) control and inspect a live Chrome browser by exposing Chrome DevTools functionality. Its main features include recording and analyzing performance traces, network request inspection, screenshots, console messages with source‚Äëmapped stack traces, and reliable automation driven by puppeteer with automatic waits. Technically it runs as a local Node.js process (distributed via npx), attaches to or launches a Chrome instance over the remote debugging protocol, and exposes DevTools APIs and optional ‚Äúskills‚Äù to MCP clients; it can also augment lab traces with CrUX field data and collects usage statistics by default (opt‚Äëout available).</p>
<p>This project is valuable for engineers and platform builders who want to integrate deep browser automation, debugging, and performance analysis into LLM-driven workflows‚Äîuse cases include automated bug reproduction, performance audits, QA test automation, and assistive developer tooling. Teams building AI assistants, SRE/performance teams, QA engineers, and tool integrators benefit because MCP enables standardized, programmatic access to the full power of DevTools from agents, improving reliability and observability of automated tasks. Its relevance is rising as more developer tooling adopts agent-based workflows and MCP-style integrations to let models perform actionable, context-aware operations in real browsers.</p>
                    </div>
                </div>
            </section>

            <section class="repo" data-seen-before="1">
                <div class="repo-header-row">
                    <h3>5. <a href="https://github.com/microsoft/PowerToys" target="_blank" rel="noopener noreferrer">microsoft/PowerToys</a> <span class="seen-badge">Not new today</span></h3>
                    <button type="button" class="repo-toggle" aria-expanded="true">Hide details</button>
                </div>
                <div class="repo-body">
                    <p class="description">Microsoft PowerToys is a collection of utilities that supercharge productivity and customization on Windows</p>
                    <p class="meta">
                        <span class="language">C#</span> |
                        <span class="stars">129,695 stars</span>
                        | <span class="today">316 stars today</span>
                    </p>
                    <p class="history">First seen: February 12, 2026 | Consecutive daily streak: 2 days</p>
                    <div class="ai-summary">
                        <h4>Analysis</h4>
                        <p>Microsoft PowerToys is an open-source suite of over 25 small utilities that extend and customize the Windows experience, including tools like FancyZones (window layouts), PowerToys Run (quick launcher), Color Picker, PowerRename, Image Resizer, Shortcut Guide, Text Extractor and the Command Palette. The project is modular: a central Settings UI and background/tray process manages individual utility modules that integrate with the OS using native Windows APIs and .NET components, while builds are packaged for installers/MSIX and distributed via GitHub releases, winget and the Microsoft Store. The repo and releases show active maintenance with frequent bug fixes, new features, CI-driven SDK work for extensions, and an extensible plugin model for things like Command Palette extensions.</p>
<p>PowerToys is valuable for power users, developers, designers and IT administrators who want to boost productivity, streamline workflows and apply enterprise policies without third‚Äëparty paid tools. Typical use cases include rapid app/file launching, advanced clipboard and paste transforms, automated image/file operations, advanced window management for multi-monitor setups, accessibility aids and quick system utilities that replace repetitive manual steps. Its popularity is driven by active Microsoft and community contributions, extensibility (SDKs and extensions), and easy deployment/updates via package managers and enterprise-friendly installers and policy templates.</p>
                    </div>
                </div>
            </section>

            <section class="repo" data-seen-before="1">
                <div class="repo-header-row">
                    <h3>6. <a href="https://github.com/iOfficeAI/AionUi" target="_blank" rel="noopener noreferrer">iOfficeAI/AionUi</a> <span class="seen-badge">Not new today</span></h3>
                    <button type="button" class="repo-toggle" aria-expanded="true">Hide details</button>
                </div>
                <div class="repo-body">
                    <p class="description">Free, local, open-source 24/7 Cowork and OpenClaw for Gemini CLI, Claude Code, Codex, OpenCode, Qwen Code, Goose CLI, Auggie, and more | üåü Star if you like it!</p>
                    <p class="meta">
                        <span class="language">TypeScript</span> |
                        <span class="stars">15,494 stars</span>
                        | <span class="today">271 stars today</span>
                    </p>
                    <p class="history">First seen: February 09, 2026 | Consecutive daily streak: 1 day</p>
                    <div class="ai-summary">
                        <h4>Analysis</h4>
                        <p>AionUi is a free, open-source multi-agent desktop and WebUI that provides a unified graphical interface for command-line AI tools (with a built-in Gemini CLI out of the box) and auto-detects and integrates local CLIs such as Claude Code, Codex, Qwen Code, Goose AI, OpenClaw and others. Its main features include multi-session/local storage with independent conversation contexts, scheduled task automation, smart file management (batch rename, classification, merging), real-time previews for 9+ file formats, AI image generation/editing, and an extensible assistants/skills ecosystem (pptx, pdf, mermaid, etc.). Technically it runs as a cross-platform (macOS/Windows/Linux) WebUI and desktop app that orchestrates local and remote models via direct CLI integration or model gateways (NewAPI), supports local model runtimes like Ollama/LM Studio, and exposes remote access through WebUI and chat platforms (Telegram, Feishu/Lark) while keeping data stored locally.</p>
<p>AionUi is valuable for developers, data analysts, knowledge workers, and teams who rely on multiple command-line AI tools but need persistent sessions, file-centric automation, and an easier GUI-driven workflow; it streamlines office automation tasks like scheduled reports, batch file processing, and document generation. Its cross-platform, multi-model support and local-data focus make it a compelling, cost-free alternative to vendor-locked tools (e.g., Claude Cowork), and its extensible assistants and remote access options explain its traction among users looking to run 24/7 AI assistants and integrate AI into everyday workflows.</p>
                    </div>
                </div>
            </section>

            <section class="repo" data-seen-before="1">
                <div class="repo-header-row">
                    <h3>7. <a href="https://github.com/Shubhamsaboo/awesome-llm-apps" target="_blank" rel="noopener noreferrer">Shubhamsaboo/awesome-llm-apps</a> <span class="seen-badge">Not new today</span></h3>
                    <button type="button" class="repo-toggle" aria-expanded="true">Hide details</button>
                </div>
                <div class="repo-body">
                    <p class="description">Collection of awesome LLM apps with AI Agents and RAG using OpenAI, Anthropic, Gemini and opensource models.</p>
                    <p class="meta">
                        <span class="language">Python</span> |
                        <span class="stars">94,457 stars</span>
                        | <span class="today">287 stars today</span>
                    </p>
                    <p class="history">First seen: February 10, 2026 | Consecutive daily streak: 1 day</p>
                    <div class="ai-summary">
                        <h4>Analysis</h4>
                        <p>This repository is a curated collection of LLM-powered applications and templates showcasing Retrieval-Augmented Generation (RAG), AI agents, multi-agent teams, MCP, voice agents, and related tooling across OpenAI, Anthropic, Google Gemini, xAI and open-source models like Qwen and Llama. It organizes starter and advanced projects‚Äîagentic RAG, memory-enabled chat, voice/RAG agents, MCP integrations, multi-agent orchestration, and optimization/fine-tuning tutorials‚Äîeach with project-specific READMEs and run instructions. Technically the examples demonstrate embedding-based retrieval, agent orchestration patterns (function calling, tools, Pydantic structured outputs), hybrid local/cloud deployments, and workflows for fine-tuning and cost/context optimization, with reproducible setup via pip requirements and per-project code.</p>
<p>The collection provides practical blueprints for developers, researchers, and product teams to prototype, compare, and deploy LLM applications or learn agent design patterns and RAG pipelines. It‚Äôs useful for anyone building conversational apps, knowledge assistants, voice interfaces, autonomous agents, and domain-specific retrieval systems because it consolidates interoperable examples across major and open-source models. Its popularity stems from the rapid rise of agentic and multimodal LLM use cases, broad model support, and curated, well-documented reference implementations that accelerate adoption and experimentation.</p>
                    </div>
                </div>
            </section>

            <section class="repo" data-seen-before="0">
                <div class="repo-header-row">
                    <h3>8. <a href="https://github.com/rowboatlabs/rowboat" target="_blank" rel="noopener noreferrer">rowboatlabs/rowboat</a> </h3>
                    <button type="button" class="repo-toggle" aria-expanded="true">Hide details</button>
                </div>
                <div class="repo-body">
                    <p class="description">Open-source AI coworker, with memory</p>
                    <p class="meta">
                        <span class="language">TypeScript</span> |
                        <span class="stars">5,271 stars</span>
                        | <span class="today">191 stars today</span>
                    </p>
                    <p class="history">First seen: February 13, 2026 | Consecutive daily streak: 1 day</p>
                    <div class="ai-summary">
                        <h4>Analysis</h4>
                        <p>Rowboat is an open-source, local-first AI coworker that builds and maintains a long-lived knowledge graph from your email, meeting notes, and other work artifacts to help you summarize, draft, plan, and produce deliverables. It stores memory as an Obsidian-compatible vault of plain Markdown with backlinks so context is transparent, editable, and kept on your machine. Rowboat connects to services like Gmail, Calendar, Drive, meeting transcription tools, and can record voice notes, draft emails, generate PDFs and slides, and run background agents to automate recurring tasks. Technically it supports local and hosted LLMs (Ollama, LM Studio, or API-based providers) and exposes a Model Context Protocol (MCP) to integrate search, databases, and other tools.</p>
<p>The project benefits knowledge workers, product managers, executives, and small teams who need contextual continuity, private data storage, and automated task assistance. By compounding memory rather than reconstructing context each time, Rowboat reduces repetitive work, speeds meeting prep, improves email drafting, and helps capture decisions and action items reliably. Its local-first design, Obsidian compatibility, background agents, and support for local models make it attractive to privacy-conscious users, while MCP-driven extensibility aligns with the current trend toward customizable, interoperable AI assistants.</p>
                    </div>
                </div>
            </section>

            <section class="repo" data-seen-before="1">
                <div class="repo-header-row">
                    <h3>9. <a href="https://github.com/github/gh-aw" target="_blank" rel="noopener noreferrer">github/gh-aw</a> <span class="seen-badge">Not new today</span></h3>
                    <button type="button" class="repo-toggle" aria-expanded="true">Hide details</button>
                </div>
                <div class="repo-body">
                    <p class="description">GitHub Agentic Workflows</p>
                    <p class="meta">
                        <span class="language">Go</span> |
                        <span class="stars">2,056 stars</span>
                        | <span class="today">405 stars today</span>
                    </p>
                    <p class="history">First seen: February 10, 2026 | Consecutive daily streak: 4 days</p>
                    <div class="ai-summary">
                        <h4>Analysis</h4>
                        <p>GitHub Agentic Workflows lets you describe agentic workflows in natural-language Markdown and execute them inside GitHub Actions, effectively combining Actions, autonomous agents, and built-in safety. Its main features include a Quick Start, workflow types and examples, Peli‚Äôs Agent Factory guided library, and extensive guardrails such as read-only-by-default run contexts, sanitized safe-outputs for writes, sandboxed execution, network isolation, SHA-pinned dependencies, tool allow-listing, compile-time validation, and human approval gates. Technically, the system parses Markdown workflows into actionable steps that invoke models and tools through controlled interfaces, with companion components like the AWF for egress control and the MCP Gateway for routed model calls to enforce policy and observability. The design emphasizes multilayered security and supply-chain protections so autonomous actions can be audited and constrained within repository CI/CD.</p>
<p>This project is valuable for developer teams, platform engineers, and security/DevOps groups who want to automate repository tasks‚Äîsuch as issue triage, code generation, dependency updates, and release orchestration‚Äîwhile maintaining rigorous safety and auditability. Organizations that need reproducible, policy-driven automation with human-in-the-loop approvals will benefit from the guardrails and enterprise-friendly controls. It‚Äôs trending because of the rapid adoption of LLM-driven agents to augment developer workflows and the demand for tightly integrated, secure automation within GitHub‚Äôs ecosystem. The combination of agentic convenience and explicit safety mechanisms makes it attractive for teams exploring trustworthy AI-assisted CI/CD.</p>
                    </div>
                </div>
            </section>

            <section class="repo" data-seen-before="0">
                <div class="repo-header-row">
                    <h3>10. <a href="https://github.com/unslothai/unsloth" target="_blank" rel="noopener noreferrer">unslothai/unsloth</a> </h3>
                    <button type="button" class="repo-toggle" aria-expanded="true">Hide details</button>
                </div>
                <div class="repo-body">
                    <p class="description">Fine-tuning &amp; Reinforcement Learning for LLMs. ü¶• Train OpenAI gpt-oss, DeepSeek, Qwen, Llama, Gemma, TTS 2x faster with 70% less VRAM.</p>
                    <p class="meta">
                        <span class="language">Python</span> |
                        <span class="stars">52,087 stars</span>
                        | <span class="today">81 stars today</span>
                    </p>
                    <p class="history">First seen: February 13, 2026 | Consecutive daily streak: 1 day</p>
                    <div class="ai-summary">
                        <h4>Analysis</h4>
                        <p>Unsloth is a tooling and training library for fine-tuning and reinforcement learning (RL) of large language and multimodal models that promises much faster throughput and dramatically lower VRAM use. It provides end-to-end support for full-finetuning, pretraining, RL algorithms (GRPO, GSPO, DrGRPO, DAPO), low-bit and FP8 training, TTS, vision and embedding models, and export to formats like GGUF/llama.cpp/vLLM. Technically it achieves these gains via custom Triton kernels (RoPE &amp; MLP kernels), padding-free packing, a manual backprop engine, quantization-aware training, and optimized batching/packing algorithms to reduce memory and extend context length. The repo also includes beginner-friendly notebooks, Docker images and guides for multi-GPU and cross-platform use to simplify hands-on experimentation and deployment.</p>
<p>This project is valuable to researchers, ML engineers, startups and hobbyists who need to fine-tune or run RL on large models but are constrained by GPU memory or budget, enabling training of much larger-context and larger-parameter models on modest hardware. Typical use cases include instruction tuning, RL-based preference training, vision-language model RL, TTS fine-tuning, and efficient embedding model training with fast notebooks for quick iteration. Unsloth is trending because it materially lowers compute and memory barriers (e.g., enabling 20B on ~14GB, 120B on 65GB), introduces practical techniques like FP8 and dynamic 4-bit quantization, and packages these advances with accessible tooling, examples and deployment paths.</p>
                    </div>
                </div>
            </section>

            </div>
        </article>
    </main>
    <footer>
        <p>Generated automatically. Data from <a href="https://github.com/trending">GitHub Trending</a>.</p>
    </footer>

<script>
(() => {
    const readKey = "gtd:read_days:gh:v1";
    const dayStr = "2026-02-13";

    let stored = [];
    try {
        stored = JSON.parse(localStorage.getItem(readKey) || "[]");
        if (!Array.isArray(stored)) {
            stored = [];
        }
    } catch (_err) {
        stored = [];
    }

    if (!stored.includes(dayStr)) {
        stored.push(dayStr);
        stored.sort();
        localStorage.setItem(readKey, JSON.stringify(stored));
    }

    const collapseParam = new URLSearchParams(window.location.search).get("collapse_seen");
    const collapseSeen = collapseParam === "0" ? false : true;

    function setCollapsed(repoEl, collapsed) {
        repoEl.classList.toggle("collapsed", collapsed);
        const button = repoEl.querySelector(".repo-toggle");
        if (button) {
            button.textContent = collapsed ? "Show details" : "Hide details";
            button.setAttribute("aria-expanded", String(!collapsed));
        }
    }

    const repos = Array.from(document.querySelectorAll("section.repo[data-seen-before]"));

    repos.forEach((repoEl) => {
        const toggle = repoEl.querySelector(".repo-toggle");
        if (!toggle) {
            return;
        }

        toggle.addEventListener("click", () => {
            setCollapsed(repoEl, !repoEl.classList.contains("collapsed"));
        });
    });

    const collapseBtn = document.getElementById("collapse-seen-btn");
    const expandBtn = document.getElementById("expand-all-btn");

    if (collapseBtn) {
        collapseBtn.addEventListener("click", () => {
            repos.forEach((repoEl) => {
                if (repoEl.dataset.seenBefore === "1") {
                    setCollapsed(repoEl, true);
                }
            });
        });
    }

    if (expandBtn) {
        expandBtn.addEventListener("click", () => {
            repos.forEach((repoEl) => setCollapsed(repoEl, false));
        });
    }

    if (collapseSeen) {
        repos.forEach((repoEl) => {
            if (repoEl.dataset.seenBefore === "1") {
                setCollapsed(repoEl, true);
            }
        });
    }
})();
</script>

</body>
</html>
