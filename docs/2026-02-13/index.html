<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>GitHub Trending - February 13, 2026</title>
    <link rel="stylesheet" href="../style.css">
</head>
<body>
    <header>
        <h1>GitHub Trending Digest - February 13, 2026</h1>
        <nav>
            <a href="../">&larr; Back to Calendar</a>
        </nav>
    </header>
    <main>
        <article>
            <div class="repos">

            <section class="repo">
                <h3>1. <a href="https://github.com/tambo-ai/tambo" target="_blank">tambo-ai/tambo</a></h3>
                <p class="description">Generative UI SDK for React</p>
                <p class="meta">
                    <span class="language">TypeScript</span> |
                    <span class="stars">9,166 stars</span>
                    | <span class="today">300 stars today</span>
                </p>
                <div class="ai-summary">
                    <h4>Analysis</h4>
                    <p>Tambo is a React toolkit for building generative UIs where an LLM-driven agent selects and renders components in response to user input. Developers register React components with Zod prop schemas that become tool-like definitions the agent can call; the backend runs the conversation loop, streams props to components (with cancellation, reconnection, and error recovery), and supports both a hosted Tambo Cloud and a self-hosted Docker backend. It supports interactable (stateful) components as well as one-off generative components, local browser tools, and MCP integrations, and exposes a TamboProvider and hooks (useTambo, useTamboThreadInput, useTamboSuggestions) to manage threads, streaming state, and suggestions. The SDK is provider-agnostic, working with OpenAI, Anthropic, Gemini, Mistral and other OpenAI-compatible LLMs, and can interoperate with agent frameworks like LangChain.</p>
<p>Tambo is useful for frontend engineers and product teams who want to add AI-driven, adaptive interfaces‚Äîexamples include analytics dashboards, chat interfaces that generate visualizations, task boards, and interactive forms‚Äîwithout wiring component selection and streaming plumbing themselves. It reduces implementation friction by making component props declarative, enabling client-side tool execution, and handling persistent UI state, which accelerates prototyping and production deployment of AI-enhanced experiences. Its support for multiple LLMs, self-hosting or cloud options, and integrations with existing agent and MCP ecosystems makes it attractive to startups and enterprises alike, and it‚Äôs trending because generative UI and agent orchestration are becoming central patterns for building intelligent, interactive applications.</p>

                </div>
            </section>

            <section class="repo">
                <h3>2. <a href="https://github.com/danielmiessler/Personal_AI_Infrastructure" target="_blank">danielmiessler/Personal_AI_Infrastructure</a></h3>
                <p class="description">Agentic AI Infrastructure for magnifying HUMAN capabilities.</p>
                <p class="meta">
                    <span class="language">TypeScript</span> |
                    <span class="stars">7,642 stars</span>
                    | <span class="today">351 stars today</span>
                </p>
                <div class="ai-summary">
                    <h4>Analysis</h4>
                    <p>PAI is a Personalized AI Infrastructure that turns agentic systems into persistent, goal-oriented assistants that learn and improve from every interaction. It composes core primitives‚Äîmemory, skill routing, TELOS goal files (MISSION, GOALS, PROJECTS, etc.), agent personalities, CLI tooling, deterministic templates, and spec/test-driven evals‚Äîaround the Foundational Algorithm (Observe ‚Üí Think ‚Üí Plan ‚Üí Execute ‚Üí Verify ‚Üí Learn). Architecturally it favors scaffolding over model choice, emphasizing modular skills, CLI-first interfaces, SRE principles, and a feedback pipeline that captures ratings, sentiment, and verification outcomes so the system can self-upgrade; recent releases add Two-Pass Capability Selection, Justify-Exclusion thinking tools, and parallel-by-default execution for faster reliability.</p>
<p>PAI is useful for nontechnical users, small businesses, managers, creatives, developers, and teams who want AI that retains context, pursues long-term goals, automates repeatable workflows, and produces verifiable outputs rather than one-off answers. Its open-source, infrastructure-first approach democratizes high-quality agentic AI‚Äîmaking persistent personal assistants and reproducible automation accessible beyond the technical elite‚Äîand aligns with current trends toward agentization, continuous learning, and production-grade AI tooling. The emphasis on testing, determinism, and modularity also makes it attractive for organizations that need auditable, versionable, and scalable AI systems.</p>

                </div>
            </section>

            <section class="repo">
                <h3>3. <a href="https://github.com/google/langextract" target="_blank">google/langextract</a></h3>
                <p class="description">A Python library for extracting structured information from unstructured text using LLMs with precise source grounding and interactive visualization.</p>
                <p class="meta">
                    <span class="language">Python</span> |
                    <span class="stars">31,570 stars</span>
                    | <span class="today">1,122 stars today</span>
                </p>
                <div class="ai-summary">
                    <h4>Analysis</h4>
                    <p>LangExtract is a Python library that uses large language models to extract structured information from unstructured text while precisely grounding each extraction to its original source location. Key features include few‚Äëshot, prompt-driven extraction schemas, controlled generation for robust structured outputs (with support for models like Google Gemini and local LLMs via Ollama), optimized handling of long documents through chunking, parallel/multi‚Äëpass extraction to boost recall, and self‚Äëcontained interactive HTML visualizations alongside JSONL outputs for review. Technically, users provide a prompt and example extractions, the library divides texts into context windows, runs model calls (optionally in parallel or batch), aligns results to source offsets, and assembles validated, schema‚Äëconstrained outputs for downstream use.</p>
<p>This tooling is valuable for teams that need reliable, auditable structure from noisy text‚Äîclinical documentation (e.g., radiology/medication extraction), legal/contract review, research literature mining, content moderation, and annotation pipelines‚Äîbecause it combines scale with traceability and reviewer-friendly visualization. Organizations benefit from faster, consistent data extraction without model fine‚Äëtuning, and the project‚Äôs support for cloud and local providers, batch modes, and emphasis on explainability aligns with current demands for practical, controllable LLM applications, making it a timely and widely applicable solution.</p>

                </div>
            </section>

            <section class="repo">
                <h3>4. <a href="https://github.com/ChromeDevTools/chrome-devtools-mcp" target="_blank">ChromeDevTools/chrome-devtools-mcp</a></h3>
                <p class="description">Chrome DevTools for coding agents</p>
                <p class="meta">
                    <span class="language">TypeScript</span> |
                    <span class="stars">24,471 stars</span>
                    | <span class="today">436 stars today</span>
                </p>
                <div class="ai-summary">
                    <h4>Analysis</h4>
                    <p>Chrome DevTools MCP is a local Model‚ÄëContext‚ÄëProtocol (MCP) server that lets AI coding agents (e.g., Gemini, Claude, Cursor, Copilot) control and inspect a live Chrome browser and the full Chrome DevTools surface. It provides features for recording performance traces and extracting actionable insights, inspecting network requests, capturing screenshots, reading console messages with source‚Äëmapped stack traces, and automating interactions via puppeteer with automatic waiting. Technically it runs as an MCP server (installable via npx) that connects to or launches a Chrome instance, exposes DevTools APIs to MCP clients, and can optionally enrich traces with CrUX field data while offering toggles to disable telemetry and usage statistics. The tool is designed to integrate with many MCP clients and developer CLIs so agents can reliably drive and inspect browsers programmatically.</p>
<p>This project is useful for AI-assisted developers, performance engineers, QA and automation teams, and platform/tooling integrators who need programmatic, reproducible access to browser internals for debugging, performance analysis, and scripted workflows. By bridging DevTools with MCP-enabled assistants it enables agents to run traces, diagnose regressions, reproduce complex browser behaviors, and automate testing without manual intervention. Its simple npx installation and broad MCP client support lower adoption friction, making it attractive as AI-driven development and observability tooling gain traction. Organizations focused on reducing time-to-fix and improving web performance will particularly benefit from its capabilities.</p>

                </div>
            </section>

            <section class="repo">
                <h3>5. <a href="https://github.com/microsoft/PowerToys" target="_blank">microsoft/PowerToys</a></h3>
                <p class="description">Microsoft PowerToys is a collection of utilities that supercharge productivity and customization on Windows</p>
                <p class="meta">
                    <span class="language">C#</span> |
                    <span class="stars">129,695 stars</span>
                    | <span class="today">316 stars today</span>
                </p>
                <div class="ai-summary">
                    <h4>Analysis</h4>
                    <p>Microsoft PowerToys is an open-source suite of over 25 small utilities that extend and customize the Windows experience, including tools like FancyZones (window layouts), PowerToys Run (quick launcher), Color Picker, PowerRename, Image Resizer, Shortcut Guide, Text Extractor and the Command Palette. The project is modular: a central Settings UI and background/tray process manages individual utility modules that integrate with the OS using native Windows APIs and .NET components, while builds are packaged for installers/MSIX and distributed via GitHub releases, winget and the Microsoft Store. The repo and releases show active maintenance with frequent bug fixes, new features, CI-driven SDK work for extensions, and an extensible plugin model for things like Command Palette extensions.</p>
<p>PowerToys is valuable for power users, developers, designers and IT administrators who want to boost productivity, streamline workflows and apply enterprise policies without third‚Äëparty paid tools. Typical use cases include rapid app/file launching, advanced clipboard and paste transforms, automated image/file operations, advanced window management for multi-monitor setups, accessibility aids and quick system utilities that replace repetitive manual steps. Its popularity is driven by active Microsoft and community contributions, extensibility (SDKs and extensions), and easy deployment/updates via package managers and enterprise-friendly installers and policy templates.</p>

                </div>
            </section>

            <section class="repo">
                <h3>6. <a href="https://github.com/iOfficeAI/AionUi" target="_blank">iOfficeAI/AionUi</a></h3>
                <p class="description">Free, local, open-source 24/7 Cowork and OpenClaw for Gemini CLI, Claude Code, Codex, OpenCode, Qwen Code, Goose CLI, Auggie, and more | üåü Star if you like it!</p>
                <p class="meta">
                    <span class="language">TypeScript</span> |
                    <span class="stars">15,494 stars</span>
                    | <span class="today">271 stars today</span>
                </p>
                <div class="ai-summary">
                    <h4>Analysis</h4>
                    <p>AionUi is a free, open-source multi-agent desktop and WebUI that provides a unified graphical interface for command-line AI tools (with a built-in Gemini CLI out of the box) and auto-detects and integrates local CLIs such as Claude Code, Codex, Qwen Code, Goose AI, OpenClaw and others. Its main features include multi-session/local storage with independent conversation contexts, scheduled task automation, smart file management (batch rename, classification, merging), real-time previews for 9+ file formats, AI image generation/editing, and an extensible assistants/skills ecosystem (pptx, pdf, mermaid, etc.). Technically it runs as a cross-platform (macOS/Windows/Linux) WebUI and desktop app that orchestrates local and remote models via direct CLI integration or model gateways (NewAPI), supports local model runtimes like Ollama/LM Studio, and exposes remote access through WebUI and chat platforms (Telegram, Feishu/Lark) while keeping data stored locally.</p>
<p>AionUi is valuable for developers, data analysts, knowledge workers, and teams who rely on multiple command-line AI tools but need persistent sessions, file-centric automation, and an easier GUI-driven workflow; it streamlines office automation tasks like scheduled reports, batch file processing, and document generation. Its cross-platform, multi-model support and local-data focus make it a compelling, cost-free alternative to vendor-locked tools (e.g., Claude Cowork), and its extensible assistants and remote access options explain its traction among users looking to run 24/7 AI assistants and integrate AI into everyday workflows.</p>

                </div>
            </section>

            <section class="repo">
                <h3>7. <a href="https://github.com/Shubhamsaboo/awesome-llm-apps" target="_blank">Shubhamsaboo/awesome-llm-apps</a></h3>
                <p class="description">Collection of awesome LLM apps with AI Agents and RAG using OpenAI, Anthropic, Gemini and opensource models.</p>
                <p class="meta">
                    <span class="language">Python</span> |
                    <span class="stars">94,457 stars</span>
                    | <span class="today">287 stars today</span>
                </p>
                <div class="ai-summary">
                    <h4>Analysis</h4>
                    <p>This repository is a curated collection of LLM-powered applications and templates showcasing Retrieval-Augmented Generation (RAG), AI agents, multi-agent teams, MCP, voice agents, and related tooling across OpenAI, Anthropic, Google Gemini, xAI and open-source models like Qwen and Llama. It organizes starter and advanced projects‚Äîagentic RAG, memory-enabled chat, voice/RAG agents, MCP integrations, multi-agent orchestration, and optimization/fine-tuning tutorials‚Äîeach with project-specific READMEs and run instructions. Technically the examples demonstrate embedding-based retrieval, agent orchestration patterns (function calling, tools, Pydantic structured outputs), hybrid local/cloud deployments, and workflows for fine-tuning and cost/context optimization, with reproducible setup via pip requirements and per-project code.</p>
<p>The collection provides practical blueprints for developers, researchers, and product teams to prototype, compare, and deploy LLM applications or learn agent design patterns and RAG pipelines. It‚Äôs useful for anyone building conversational apps, knowledge assistants, voice interfaces, autonomous agents, and domain-specific retrieval systems because it consolidates interoperable examples across major and open-source models. Its popularity stems from the rapid rise of agentic and multimodal LLM use cases, broad model support, and curated, well-documented reference implementations that accelerate adoption and experimentation.</p>

                </div>
            </section>

            <section class="repo">
                <h3>8. <a href="https://github.com/rowboatlabs/rowboat" target="_blank">rowboatlabs/rowboat</a></h3>
                <p class="description">Open-source AI coworker, with memory</p>
                <p class="meta">
                    <span class="language">TypeScript</span> |
                    <span class="stars">5,271 stars</span>
                    | <span class="today">191 stars today</span>
                </p>
                <div class="ai-summary">
                    <h4>Analysis</h4>
                    <p>Rowboat is an open-source, local-first ‚ÄúAI coworker‚Äù that ingests email, meeting notes, and other work artifacts to build a long-lived, Obsidian-compatible Markdown knowledge graph with backlinks. It uses that persistent context to draft emails, prepare meeting briefs, generate docs and PDF slides, capture voice memos (Deepgram optional), and spin up background agents for repeatable workflows. Technically it stores everything as plain Markdown on your machine, plugs into local or hosted LLMs (Ollama, LM Studio, or API providers), and extends functionality via the Model Context Protocol (MCP) to connect external tools and services. You control what runs and what gets written back into the vault, keeping the memory transparent and editable.</p>
<p>Rowboat is valuable for knowledge workers who need contextualized, automated assistance‚Äîproduct managers, executives, consultants, engineers, or any privacy-conscious teams‚Äîbecause it reduces context-switching, preserves institutional memory, and automates routine follow-ups and content generation. Its local-first design, editable knowledge graph, background agents, and BYO-model flexibility make it attractive where privacy, reproducibility, and workflow integration matter. Being open-source and extensible via MCP also encourages adoption and customization for internal tooling and team-specific automations, which helps explain its current traction.</p>

                </div>
            </section>

            <section class="repo">
                <h3>9. <a href="https://github.com/github/gh-aw" target="_blank">github/gh-aw</a></h3>
                <p class="description">GitHub Agentic Workflows</p>
                <p class="meta">
                    <span class="language">Go</span> |
                    <span class="stars">2,056 stars</span>
                    | <span class="today">405 stars today</span>
                </p>
                <div class="ai-summary">
                    <h4>Analysis</h4>
                    <p>GitHub Agentic Workflows lets developers write agentic automation in natural‚Äëlanguage Markdown and run those agents directly as GitHub Actions. It integrates into the Actions runtime and enforces layered guardrails‚Äîread‚Äëonly defaults, sanitized safe‚Äëoutputs for deliberate writes, sandboxed execution, input sanitization, network isolation, SHA‚Äëpinned dependencies, tool allow‚Äëlisting, and compile‚Äëtime validation‚Äîwhile companion projects (Agent Workflow Firewall and MCP Gateway) provide egress control and model gatewaying. Users install an extension, add agentic workflow files to repositories, and can gate access to teams or require human approvals for critical operations to reduce risk. The design focuses on embedding AI agents into existing CI/CD flows while minimizing attack surface and supply‚Äëchain exposure.</p>
<p>This project benefits repository maintainers, DevOps/SRE teams, automation engineers, and security teams who want to delegate routine tasks (triage, testing, code generation, workflow orchestration) to AI without sacrificing governance. Its strict security and auditing features make it suitable for organizations with compliance requirements that still want to experiment with agentic automation. It‚Äôs trending because it lowers the friction of adopting AI agents by packaging them into familiar GitHub workflows and addressing safety concerns that have hampered broader operational use. As a result, teams can prototype autonomous workflows rapidly while maintaining operational oversight.</p>

                </div>
            </section>

            <section class="repo">
                <h3>10. <a href="https://github.com/unslothai/unsloth" target="_blank">unslothai/unsloth</a></h3>
                <p class="description">Fine-tuning &amp; Reinforcement Learning for LLMs. ü¶• Train OpenAI gpt-oss, DeepSeek, Qwen, Llama, Gemma, TTS 2x faster with 70% less VRAM.</p>
                <p class="meta">
                    <span class="language">Python</span> |
                    <span class="stars">52,087 stars</span>
                    | <span class="today">81 stars today</span>
                </p>
                <div class="ai-summary">
                    <h4>Analysis</h4>
                    <p>Unsloth is a tooling and training library for fine-tuning and reinforcement learning (RL) of large language and multimodal models that promises much faster throughput and dramatically lower VRAM use. It provides end-to-end support for full-finetuning, pretraining, RL algorithms (GRPO, GSPO, DrGRPO, DAPO), low-bit and FP8 training, TTS, vision and embedding models, and export to formats like GGUF/llama.cpp/vLLM. Technically it achieves these gains via custom Triton kernels (RoPE &amp; MLP kernels), padding-free packing, a manual backprop engine, quantization-aware training, and optimized batching/packing algorithms to reduce memory and extend context length. The repo also includes beginner-friendly notebooks, Docker images and guides for multi-GPU and cross-platform use to simplify hands-on experimentation and deployment.</p>
<p>This project is valuable to researchers, ML engineers, startups and hobbyists who need to fine-tune or run RL on large models but are constrained by GPU memory or budget, enabling training of much larger-context and larger-parameter models on modest hardware. Typical use cases include instruction tuning, RL-based preference training, vision-language model RL, TTS fine-tuning, and efficient embedding model training with fast notebooks for quick iteration. Unsloth is trending because it materially lowers compute and memory barriers (e.g., enabling 20B on ~14GB, 120B on 65GB), introduces practical techniques like FP8 and dynamic 4-bit quantization, and packages these advances with accessible tooling, examples and deployment paths.</p>

                </div>
            </section>

            </div>
        </article>
    </main>
    <footer>
        <p>Generated automatically. Data from <a href="https://github.com/trending">GitHub Trending</a>.</p>
    </footer>
</body>
</html>
